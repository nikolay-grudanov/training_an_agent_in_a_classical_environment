# Experiment Configuration Schema
# This file defines the structure and default values for controlled RL experiments
# Used by ExperimentRunner and related components

# Experiment metadata
experiment:
  name: "default_experiment"
  description: "Controlled experiment comparing RL algorithms"
  hypothesis: "Algorithm A will outperform Algorithm B in the given environment"
  
  # Experiment execution settings
  execution:
    mode: "sequential"  # sequential, parallel, validation
    timeout_minutes: 60  # Maximum time per configuration
    max_retries: 3
    enable_monitoring: true
    save_checkpoints: true
    
  # Output settings
  output:
    base_directory: "results/experiments"
    save_models: true
    save_videos: true
    save_plots: true
    export_format: "json"  # json, csv, both

# Baseline configuration (reference point)
baseline:
  name: "baseline_ppo"
  algorithm: "PPO"
  environment: "LunarLander-v2"
  seed: 42
  training_steps: 100000
  evaluation_frequency: 10000
  
  # Algorithm-specific hyperparameters
  hyperparameters:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5

# Variant configuration (experimental change)
variant:
  name: "variant_a2c"
  algorithm: "A2C"
  environment: "LunarLander-v2"  # Should match baseline
  seed: 42  # Should match baseline for fair comparison
  training_steps: 100000  # Should match baseline
  evaluation_frequency: 10000
  
  # Algorithm-specific hyperparameters
  hyperparameters:
    learning_rate: 0.0007
    n_steps: 5
    gamma: 0.99
    gae_lambda: 1.0
    ent_coef: 0.01
    vf_coef: 0.25
    max_grad_norm: 0.5

# Evaluation settings
evaluation:
  # Number of episodes for final evaluation
  num_episodes: 20
  
  # Metrics to track and compare
  metrics:
    - "mean_reward"
    - "std_reward"
    - "episode_length"
    - "convergence_steps"
    - "final_performance"
    - "peak_performance"
    - "stability_score"
  
  # Statistical analysis settings
  statistical_tests:
    significance_level: 0.05
    use_bonferroni_correction: true
    bootstrap_samples: 1000
    confidence_interval: 0.95

# Comparison settings
comparison:
  # Performance thresholds
  thresholds:
    convergence_reward: 200.0  # Environment-specific
    minimum_improvement: 0.05  # 5% minimum improvement to be significant
    stability_threshold: 0.1   # Maximum acceptable standard deviation
  
  # Visualization settings
  plots:
    learning_curves: true
    reward_distributions: true
    statistical_comparison: true
    convergence_analysis: true
    save_format: "png"  # png, svg, pdf
    dpi: 300
    
  # Report generation
  report:
    generate_html: true
    generate_markdown: true
    include_raw_data: false
    include_statistical_details: true

# Environment-specific configurations
environments:
  LunarLander-v2:
    convergence_threshold: 200.0
    max_episode_steps: 1000
    success_threshold: 200.0
    
  MountainCarContinuous-v0:
    convergence_threshold: 90.0
    max_episode_steps: 999
    success_threshold: 90.0
    
  Pendulum-v1:
    convergence_threshold: -200.0
    max_episode_steps: 200
    success_threshold: -200.0
    
  Acrobot-v1:
    convergence_threshold: -100.0
    max_episode_steps: 500
    success_threshold: -100.0

# Algorithm-specific default hyperparameters
algorithm_defaults:
  PPO:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5
    
  A2C:
    learning_rate: 0.0007
    n_steps: 5
    gamma: 0.99
    gae_lambda: 1.0
    ent_coef: 0.01
    vf_coef: 0.25
    max_grad_norm: 0.5
    
  SAC:
    learning_rate: 0.0003
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    ent_coef: "auto"
    target_update_interval: 1
    
  TD3:
    learning_rate: 0.001
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 100
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    policy_delay: 2
    target_policy_noise: 0.2
    target_noise_clip: 0.5

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_to_file: true
  log_to_console: true
  structured_logging: true
  include_timestamps: true
  
# Resource management
resources:
  # Memory limits
  max_memory_gb: 8
  memory_check_interval: 60  # seconds
  
  # CPU settings
  n_cpu_cores: -1  # -1 for all available cores
  cpu_check_interval: 30  # seconds
  
  # Storage settings
  min_free_space_gb: 2
  cleanup_intermediate_files: false

# Reproducibility settings
reproducibility:
  enforce_deterministic: true
  save_random_states: true
  track_dependencies: true
  save_environment_info: true
  git_commit_tracking: true

# Validation settings (for dry-run mode)
validation:
  quick_test_steps: 1000
  skip_training: false
  mock_environment: false
  validate_configs_only: false

# Error handling
error_handling:
  strategy: "abort"  # abort, retry, skip, continue
  max_retries: 3
  retry_delay_seconds: 30
  save_on_failure: true
  detailed_error_logging: true

# Templates for common experiment types
templates:
  algorithm_comparison:
    description: "Compare two different algorithms on the same environment"
    baseline:
      algorithm: "PPO"
    variant:
      algorithm: "A2C"
      
  hyperparameter_tuning:
    description: "Test different hyperparameter values"
    baseline:
      hyperparameters:
        learning_rate: 0.0003
    variant:
      hyperparameters:
        learning_rate: 0.001
        
  environment_generalization:
    description: "Test same algorithm on different environments"
    baseline:
      environment: "LunarLander-v2"
    variant:
      environment: "Pendulum-v1"

# CLI defaults
cli:
  default_config: "configs/experiment_schema.yaml"
  output_directory: "results/experiments"
  verbose: true
  progress_bar: true
  save_plots: true
  open_report: false