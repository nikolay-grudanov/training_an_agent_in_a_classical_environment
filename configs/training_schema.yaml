# Training Configuration Schema for RL Agent Training System
# This file defines the complete configuration structure for training RL agents
# Supports PPO, A2C, SAC, TD3 algorithms with LunarLander-v3 and other environments

# ==============================================================================
# DEFAULT TRAINING CONFIGURATION
# ==============================================================================

defaults:
  - algorithm: ppo  # Default algorithm
  - environment: lunar_lander  # Default environment
  - experiment: baseline  # Default experiment setup

# ==============================================================================
# TRAINING SETTINGS
# ==============================================================================

training:
  # Basic training parameters
  total_timesteps: 100000  # Total training steps
  seed: 42  # Random seed for reproducibility
  
  # Training mode and strategy
  mode: "train"  # Options: train, resume, evaluate, finetune
  strategy: "timestep_based"  # Options: episodic, timestep_based, mixed, adaptive
  
  # Evaluation settings
  eval_freq: 10000  # Evaluate every N timesteps
  eval_episodes: 10  # Number of episodes for evaluation
  eval_deterministic: true  # Use deterministic policy for evaluation
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 50000  # Stop if no improvement for N timesteps
    min_delta: 1.0  # Minimum improvement threshold
    target_reward: 200.0  # Target reward for LunarLander-v3
  
  # Progress reporting
  progress:
    log_interval: 1000  # Log progress every N timesteps
    save_interval: 25000  # Save model every N timesteps
    tensorboard: true  # Enable TensorBoard logging
    verbose: 1  # Verbosity level (0, 1, 2)

# ==============================================================================
# AGENT CONFIGURATION
# ==============================================================================

agent:
  # Algorithm selection (overridden by defaults)
  algorithm: "PPO"  # Options: PPO, A2C, SAC, TD3
  
  # Common hyperparameters
  learning_rate: 3.0e-4
  batch_size: 64
  gamma: 0.99  # Discount factor
  
  # Network architecture
  policy_kwargs:
    net_arch: [64, 64]  # Hidden layers for policy and value networks
    activation_fn: "tanh"  # Activation function: tanh, relu
  
  # Algorithm-specific settings (examples for each)
  ppo:
    n_steps: 2048
    n_epochs: 10
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: false
    sde_sample_freq: -1
    target_kl: null
    
  a2c:
    n_steps: 5
    vf_coef: 0.25
    ent_coef: 0.01
    max_grad_norm: 0.5
    rms_prop_eps: 1.0e-5
    use_rms_prop: true
    use_sde: false
    sde_sample_freq: -1
    normalize_advantage: false
    
  sac:
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    ent_coef: "auto"
    target_update_interval: 1
    target_entropy: "auto"
    use_sde: false
    sde_sample_freq: -1
    use_sde_at_warmup: false
    
  td3:
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 100
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    policy_delay: 2
    target_policy_noise: 0.2
    target_noise_clip: 0.5

# ==============================================================================
# ENVIRONMENT CONFIGURATION
# ==============================================================================

environment:
  # Environment selection (overridden by defaults)
  name: "LunarLander-v3"
  
  # Environment wrappers
  wrappers:
    normalize_observations: true
    normalize_rewards: true
    clip_rewards: false
    frame_stack: 1
    
  # Environment-specific settings
  lunar_lander:
    continuous: false  # Use continuous action space
    enable_wind: false  # Add wind disturbance
    wind_power: 15.0  # Wind power
    turbulence_power: 1.5  # Turbulence power
    
  # Vectorized environment
  vec_env:
    n_envs: 1  # Number of parallel environments
    vec_env_class: "DummyVecEnv"  # Options: DummyVecEnv, SubprocVecEnv

# ==============================================================================
# EXPERIMENT CONFIGURATION
# ==============================================================================

experiment:
  # Experiment identification
  name: "lunar_lander_training"
  description: "Training RL agent on LunarLander-v3 environment"
  tags: ["lunarlander", "ppo", "baseline"]
  
  # Output directories
  output_dir: "results"
  log_dir: "${experiment.output_dir}/logs"
  model_dir: "${experiment.output_dir}/models"
  tensorboard_dir: "${experiment.output_dir}/tensorboard"
  
  # Checkpointing
  checkpointing:
    enabled: true
    save_freq: 25000  # Save checkpoint every N timesteps
    max_checkpoints: 5  # Keep only N latest checkpoints
    save_best: true  # Save best model based on evaluation
    
  # Reproducibility
  reproducibility:
    deterministic: true
    benchmark: false  # Set torch.backends.cudnn.benchmark
    
  # Resource monitoring
  monitoring:
    track_memory: true
    track_gpu: false  # Set to true if using GPU
    log_system_info: true

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================

logging:
  # Logging levels
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  
  # Log outputs
  console: true
  file: true
  structured: true  # Use structured logging (JSON)
  
  # Log file settings
  log_file: "${experiment.log_dir}/training.log"
  max_file_size: "100MB"
  backup_count: 5
  
  # TensorBoard settings
  tensorboard:
    enabled: true
    log_dir: "${experiment.tensorboard_dir}"
    update_freq: 1000
    
  # Weights & Biases (optional)
  wandb:
    enabled: false
    project: "rl-agent-training"
    entity: null
    tags: ["${agent.algorithm}", "${environment.name}"]

# ==============================================================================
# HYDRA CONFIGURATION
# ==============================================================================

hydra:
  run:
    dir: "${experiment.output_dir}/${now:%Y-%m-%d_%H-%M-%S}"
  sweep:
    dir: "${experiment.output_dir}/multirun/${now:%Y-%m-%d_%H-%M-%S}"
    subdir: "${hydra:job.num}"
  job:
    name: "train_${agent.algorithm}_${environment.name}"
    chdir: false

# ==============================================================================
# ALGORITHM-SPECIFIC PRESETS
# ==============================================================================

# PPO optimized for LunarLander-v3
ppo_lunar_lander:
  agent:
    algorithm: "PPO"
    learning_rate: 3.0e-4
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    policy_kwargs:
      net_arch: [64, 64]
      activation_fn: "tanh"
  environment:
    name: "LunarLander-v3"
    wrappers:
      normalize_observations: true
      normalize_rewards: true
  training:
    total_timesteps: 100000
    target_reward: 200.0

# A2C optimized for LunarLander-v3
a2c_lunar_lander:
  agent:
    algorithm: "A2C"
    learning_rate: 7.0e-4
    n_steps: 5
    gamma: 0.99
    vf_coef: 0.25
    ent_coef: 0.01
    policy_kwargs:
      net_arch: [64, 64]
      activation_fn: "tanh"
  environment:
    name: "LunarLander-v3"
    wrappers:
      normalize_observations: true
      normalize_rewards: true
  training:
    total_timesteps: 100000
    target_reward: 200.0

# SAC optimized for continuous environments
sac_continuous:
  agent:
    algorithm: "SAC"
    learning_rate: 3.0e-4
    buffer_size: 1000000
    batch_size: 256
    gamma: 0.99
    tau: 0.005
    ent_coef: "auto"
  environment:
    name: "LunarLanderContinuous-v2"
    wrappers:
      normalize_observations: true
      normalize_rewards: false
  training:
    total_timesteps: 100000
    target_reward: 200.0

# TD3 optimized for continuous environments
td3_continuous:
  agent:
    algorithm: "TD3"
    learning_rate: 1.0e-3
    buffer_size: 1000000
    batch_size: 100
    gamma: 0.99
    tau: 0.005
    policy_delay: 2
    target_policy_noise: 0.2
    target_noise_clip: 0.5
  environment:
    name: "LunarLanderContinuous-v2"
    wrappers:
      normalize_observations: true
      normalize_rewards: false
  training:
    total_timesteps: 100000
    target_reward: 200.0

# ==============================================================================
# ENVIRONMENT-SPECIFIC PRESETS
# ==============================================================================

# CartPole environment
cartpole:
  environment:
    name: "CartPole-v1"
    wrappers:
      normalize_observations: false
      normalize_rewards: false
  training:
    total_timesteps: 50000
    target_reward: 475.0
    eval_freq: 5000

# Pendulum environment (continuous)
pendulum:
  environment:
    name: "Pendulum-v1"
    wrappers:
      normalize_observations: true
      normalize_rewards: true
  training:
    total_timesteps: 50000
    target_reward: -150.0
    eval_freq: 5000

# MountainCar environment
mountain_car:
  environment:
    name: "MountainCar-v0"
    wrappers:
      normalize_observations: true
      normalize_rewards: false
  training:
    total_timesteps: 100000
    target_reward: -110.0
    eval_freq: 10000

# ==============================================================================
# EXPERIMENTAL CONFIGURATIONS
# ==============================================================================

# Hyperparameter sweep example
sweep_example:
  agent:
    learning_rate: 
      - 1.0e-4
      - 3.0e-4
      - 1.0e-3
    batch_size:
      - 32
      - 64
      - 128
  training:
    total_timesteps: 50000  # Shorter for sweep

# Multi-environment training
multi_env:
  environment:
    names:
      - "LunarLander-v3"
      - "CartPole-v1"
      - "MountainCar-v0"
  training:
    total_timesteps: 100000
    eval_freq: 10000

# ==============================================================================
# USAGE EXAMPLES
# ==============================================================================

# Example 1: Train PPO on LunarLander-v3
# python -m src.training.cli train --config-name=ppo_lunar_lander

# Example 2: Train SAC on continuous LunarLander
# python -m src.training.cli train --config-name=sac_continuous

# Example 3: Custom configuration override
# python -m src.training.cli train agent.learning_rate=1e-3 training.total_timesteps=200000

# Example 4: Resume training from checkpoint
# python -m src.training.cli resume --checkpoint-path=results/models/checkpoint_50000.zip

# Example 5: Evaluate trained model
# python -m src.training.cli evaluate --model-path=results/models/final_model.zip