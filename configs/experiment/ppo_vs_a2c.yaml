# Experiment Configuration: PPO vs A2C Comparison
# @package experiment

name: "PPO_vs_A2C_LunarLander"
description: "Compare PPO and A2C performance on LunarLander-v3 environment"
hypothesis: "PPO should achieve higher sample efficiency and more stable learning compared to A2C"

# Experiment settings
runs_per_algorithm: 3
seeds: [42, 123, 456]

# Algorithms to compare
algorithms:
  - ppo
  - a2c

# Environment settings
environment: lunar_lander

# Training settings
training:
  total_timesteps: 200000
  eval_freq: 5000
  n_eval_episodes: 20

# Comparison metrics
metrics:
  - mean_reward
  - episode_length
  - training_time
  - sample_efficiency

# Output settings
save_results: true
generate_plots: true
create_videos: true