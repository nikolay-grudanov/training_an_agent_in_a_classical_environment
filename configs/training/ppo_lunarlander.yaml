# Конфигурация для обучения PPO на LunarLander-v3
experiment_name: "ppo_lunarlander_baseline"
output_dir: "results/training"
seed: 42

algorithm:
  name: "PPO"
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  device: "auto"
  verbose: 1
  policy_kwargs:
    net_arch:
      - pi: [64, 64]
        vf: [64, 64]
    activation_fn: "tanh"

environment:
  name: "LunarLander-v3"
  render_mode: null
  wrappers: []

training:
  total_timesteps: 200000
  eval_freq: 10000
  n_eval_episodes: 10
  save_freq: 25000
  early_stopping: true
  patience: 5
  min_delta: 5.0
  log_interval: 1000

logging:
  level: "INFO"
  log_to_file: true
  console_output: true

reproducibility:
  seed: 42
  deterministic: true
  benchmark: false