#!/usr/bin/env python3
"""–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ —Ç—Ä–µ–Ω–µ—Ä–∞."""

import sys
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional
import tempfile
import yaml

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent / "src"))


class TrainingMode(Enum):
    """–†–µ–∂–∏–º—ã –æ–±—É—á–µ–Ω–∏—è."""
    
    TRAIN = "train"           # –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
    RESUME = "resume"         # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
    EVALUATE = "evaluate"     # –¢–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∞
    FINETUNE = "finetune"     # –î–æ–æ–±—É—á–µ–Ω–∏–µ


@dataclass
class AgentConfig:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞."""
    algorithm: str = "PPO"
    env_name: str = "LunarLander-v3"
    total_timesteps: int = 100_000
    seed: int = 42
    learning_rate: float = 3e-4


@dataclass
class TrainerConfig:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞."""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    experiment_name: str = "default_experiment"
    algorithm: str = "PPO"
    environment_name: str = "LunarLander-v3"
    mode: TrainingMode = TrainingMode.TRAIN
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    total_timesteps: int = 100_000
    seed: int = 42
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ü–µ–Ω–∫–∞
    eval_freq: int = 10_000
    n_eval_episodes: int = 10
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_freq: int = 50_000
    output_dir: str = "results"
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent_config: Optional[AgentConfig] = None
    
    def __post_init__(self) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        supported_algorithms = ["PPO", "A2C", "SAC", "TD3"]
        if self.algorithm.upper() not in supported_algorithms:
            raise ValueError(
                f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {self.algorithm}. "
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ: {supported_algorithms}"
            )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        self.algorithm = self.algorithm.upper()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if self.total_timesteps <= 0:
            raise ValueError(f"total_timesteps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.total_timesteps}")
        
        if self.eval_freq <= 0:
            raise ValueError(f"eval_freq –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.eval_freq}")
        
        if self.n_eval_episodes <= 0:
            raise ValueError(f"n_eval_episodes –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.n_eval_episodes}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Ç–µ–π
        self._setup_paths()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞
        if self.agent_config is None:
            self.agent_config = AgentConfig(
                algorithm=self.algorithm,
                env_name=self.environment_name,
                total_timesteps=self.total_timesteps,
                seed=self.seed,
            )
    
    def _setup_paths(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        output_path = Path(self.output_dir)
        experiment_path = output_path / self.experiment_name
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        experiment_path.mkdir(parents=True, exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
        self.model_save_path = str(experiment_path / "models" / f"{self.algorithm.lower()}_model")
        self.logs_dir = str(experiment_path / "logs")
        self.tensorboard_log = str(experiment_path / "tensorboard")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        Path(self.model_save_path).parent.mkdir(parents=True, exist_ok=True)
        Path(self.logs_dir).mkdir(parents=True, exist_ok=True)
        Path(self.tensorboard_log).mkdir(parents=True, exist_ok=True)


@dataclass
class TrainingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è."""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    success: bool
    total_timesteps: int
    training_time: float
    final_mean_reward: float
    final_std_reward: float
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    training_history: Dict[str, List[float]] = field(default_factory=dict)
    evaluation_history: Dict[str, List[float]] = field(default_factory=dict)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    model_path: Optional[str] = None
    checkpoint_paths: List[str] = field(default_factory=list)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    experiment_name: str = ""
    algorithm: str = ""
    environment_name: str = ""
    seed: int = 42
    
    # –û—à–∏–±–∫–∏
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    best_mean_reward: float = float("-inf")
    convergence_timestep: Optional[int] = None
    early_stopped: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return {
            "success": self.success,
            "total_timesteps": self.total_timesteps,
            "training_time": self.training_time,
            "final_mean_reward": self.final_mean_reward,
            "final_std_reward": self.final_std_reward,
            "best_mean_reward": self.best_mean_reward,
            "convergence_timestep": self.convergence_timestep,
            "early_stopped": self.early_stopped,
            "experiment_name": self.experiment_name,
            "algorithm": self.algorithm,
            "environment_name": self.environment_name,
            "seed": self.seed,
            "model_path": self.model_path,
            "checkpoint_paths": self.checkpoint_paths,
            "error_message": self.error_message,
            "warnings": self.warnings,
            "training_history": self.training_history,
            "evaluation_history": self.evaluation_history,
        }
    
    def save(self, path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª."""
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w', encoding='utf-8') as f:
            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)


def test_trainer_config_basic():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ TrainerConfig."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TrainerConfig...")
    
    # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    config = TrainerConfig()
    assert config.experiment_name == "default_experiment"
    assert config.algorithm == "PPO"
    assert config.environment_name == "LunarLander-v3"
    assert config.mode == TrainingMode.TRAIN
    assert config.total_timesteps == 100_000
    assert config.seed == 42
    assert config.agent_config is not None
    assert config.agent_config.algorithm == "PPO"
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ–∑–¥–∞–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    # –¢–µ—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    custom_config = TrainerConfig(
        experiment_name="test_experiment",
        algorithm="A2C",
        environment_name="CartPole-v1",
        total_timesteps=50_000,
        seed=123,
    )
    assert custom_config.experiment_name == "test_experiment"
    assert custom_config.algorithm == "A2C"
    assert custom_config.environment_name == "CartPole-v1"
    assert custom_config.total_timesteps == 50_000
    assert custom_config.seed == 123
    assert custom_config.agent_config.algorithm == "A2C"
    print("‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    # –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    try:
        TrainerConfig(algorithm="INVALID")
        assert False, "–î–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –±—ã—Ç—å –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"
    except ValueError as e:
        assert "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º" in str(e)
        print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    try:
        TrainerConfig(total_timesteps=0)
        assert False, "–î–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –±—ã—Ç—å –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"
    except ValueError as e:
        assert "total_timesteps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0" in str(e)
        print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è timesteps —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    # –¢–µ—Å—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π
    with tempfile.TemporaryDirectory() as temp_dir:
        path_config = TrainerConfig(
            experiment_name="test_paths",
            output_dir=temp_dir,
        )
        assert hasattr(path_config, 'model_save_path')
        assert hasattr(path_config, 'logs_dir')
        assert hasattr(path_config, 'tensorboard_log')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã
        assert Path(path_config.logs_dir).exists()
        assert Path(path_config.tensorboard_log).exists()
        print("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_training_mode():
    """–¢–µ—Å—Ç enum TrainingMode."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TrainingMode...")
    
    assert TrainingMode.TRAIN.value == "train"
    assert TrainingMode.RESUME.value == "resume"
    assert TrainingMode.EVALUATE.value == "evaluate"
    assert TrainingMode.FINETUNE.value == "finetune"
    print("‚úÖ –ó–Ω–∞—á–µ–Ω–∏—è TrainingMode –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
    
    # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑ —Å—Ç—Ä–æ–∫–∏
    assert TrainingMode("train") == TrainingMode.TRAIN
    assert TrainingMode("resume") == TrainingMode.RESUME
    print("‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ TrainingMode –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_training_result():
    """–¢–µ—Å—Ç TrainingResult."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TrainingResult...")
    
    result = TrainingResult(
        success=True,
        total_timesteps=100_000,
        training_time=300.5,
        final_mean_reward=250.0,
        final_std_reward=50.0,
        experiment_name="test_exp",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        seed=42,
    )
    
    assert result.success is True
    assert result.total_timesteps == 100_000
    assert result.training_time == 300.5
    assert result.final_mean_reward == 250.0
    assert result.experiment_name == "test_exp"
    print("‚úÖ TrainingResult —Å–æ–∑–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    # –¢–µ—Å—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å
    result_dict = result.to_dict()
    assert isinstance(result_dict, dict)
    assert result_dict["success"] is True
    assert result_dict["total_timesteps"] == 100_000
    assert result_dict["experiment_name"] == "test_exp"
    print("‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    # –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    with tempfile.TemporaryDirectory() as temp_dir:
        result_path = Path(temp_dir) / "test_result.yaml"
        result.save(result_path)
        assert result_path.exists()
        print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_supported_algorithms():
    """–¢–µ—Å—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤...")
    
    supported = ["PPO", "A2C", "SAC", "TD3"]
    
    for algorithm in supported:
        config = TrainerConfig(algorithm=algorithm)
        assert config.algorithm == algorithm
        print(f"‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º {algorithm} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
    
    # –¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞
    config = TrainerConfig(algorithm="ppo")
    assert config.algorithm == "PPO"
    print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥–∏—Å—Ç—Ä–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_agent_config():
    """–¢–µ—Å—Ç AgentConfig."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AgentConfig...")
    
    config = AgentConfig()
    assert config.algorithm == "PPO"
    assert config.env_name == "LunarLander-v3"
    assert config.total_timesteps == 100_000
    assert config.seed == 42
    assert config.learning_rate == 3e-4
    print("‚úÖ AgentConfig —Å–æ–∑–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    custom_config = AgentConfig(
        algorithm="SAC",
        env_name="Pendulum-v1",
        total_timesteps=50_000,
        seed=999,
        learning_rate=1e-3,
    )
    assert custom_config.algorithm == "SAC"
    assert custom_config.env_name == "Pendulum-v1"
    assert custom_config.total_timesteps == 50_000
    assert custom_config.seed == 999
    assert custom_config.learning_rate == 1e-3
    print("‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π AgentConfig —Å–æ–∑–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("üéÆ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ—Å—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 50)
    
    try:
        test_agent_config()
        test_trainer_config_basic()
        test_training_mode()
        test_training_result()
        test_supported_algorithms()
        
        print("\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        print("‚úÖ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        print("\nüìã –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
        print("  - TrainerConfig: —Å–æ–∑–¥–∞–Ω–∏–µ, –≤–∞–ª–∏–¥–∞—Ü–∏—è, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π")
        print("  - TrainingMode: enum –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫")
        print("  - TrainingResult: —Å–æ–∑–¥–∞–Ω–∏–µ, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
        print("  - AgentConfig: –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞")
        print("  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤: PPO, A2C, SAC, TD3")
        
        print("\nüöÄ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–∞—Ö: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()