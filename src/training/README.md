# üéÆ –ú–æ–¥—É–ª—å –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ—Å—Å–∏–π.

## üöÄ –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### ‚ú® –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- **PPO** (Proximal Policy Optimization) - –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
- **A2C** (Advantage Actor-Critic) - –±—ã—Å—Ç—Ä—ã–π on-policy –∞–ª–≥–æ—Ä–∏—Ç–º  
- **SAC** (Soft Actor-Critic) - –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
- **TD3** (Twin Delayed Deep Deterministic Policy Gradient) - –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π

### üõ†Ô∏è –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã
- **TRAIN** - –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
- **RESUME** - –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- **EVALUATE** - —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
- **FINETUNE** - –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏

### üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —Ç—Ä–µ–∫–∏–Ω–≥
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å TensorBoard
- –°–∏—Å—Ç–µ–º–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
- –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
- –î–µ—Ç–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è

### ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Hydra
- –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª—è

```
src/training/
‚îú‚îÄ‚îÄ __init__.py          # –≠–∫—Å–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª—è
‚îú‚îÄ‚îÄ trainer.py           # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å Trainer
‚îú‚îÄ‚îÄ cli.py              # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îî‚îÄ‚îÄ README.md           # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

## üéØ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from src.training import Trainer, TrainerConfig

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = TrainerConfig(
    experiment_name="my_experiment",
    algorithm="PPO",
    environment_name="LunarLander-v3",
    total_timesteps=100_000,
    seed=42,
)

# –û–±—É—á–µ–Ω–∏–µ
with Trainer(config) as trainer:
    result = trainer.train()
    
    if result.success:
        print(f"–ù–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f}")
        print(f"–ú–æ–¥–µ–ª—å: {result.model_path}")
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CLI

```bash
# –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
python -m src.training.cli train --algorithm PPO --env LunarLander-v3 --timesteps 100000

# –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
python -m src.training.cli train --config configs/ppo_lunarlander.yaml

# –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
python -m src.training.cli resume checkpoints/checkpoint_50000.pkl

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
python -m src.training.cli evaluate models/ppo_final.zip --episodes 10 --render

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
python -m src.training.cli compare --algorithm PPO A2C --timesteps 50000 --runs 3
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª

```yaml
# configs/my_config.yaml
experiment_name: "advanced_ppo"
output_dir: "results"
seed: 42

algorithm:
  name: "PPO"
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95

environment:
  name: "LunarLander-v3"

training:
  total_timesteps: 200000
  eval_freq: 10000
  n_eval_episodes: 10
  save_freq: 25000
  early_stopping: true
  patience: 5
```

## üìö –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### TrainerConfig

–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ø–æ–ª–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –æ–±—É—á–µ–Ω–∏—è:

```python
@dataclass
class TrainerConfig:
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    experiment_name: str = "default_experiment"
    algorithm: str = "PPO"  # PPO, A2C, SAC, TD3
    environment_name: str = "LunarLander-v3"
    mode: TrainingMode = TrainingMode.TRAIN
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    total_timesteps: int = 100_000
    seed: int = 42
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    eval_freq: int = 10_000
    n_eval_episodes: int = 10
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_freq: int = 50_000
    checkpoint_freq: int = 25_000
    max_checkpoints: int = 5
    
    # –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    early_stopping: bool = False
    patience: int = 5
    min_improvement: float = 0.01
```

### Trainer

–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è:

```python
class Trainer:
    def __init__(self, config: TrainerConfig) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
    
    def setup(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
    
    def train(self) -> TrainingResult:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è."""
    
    def evaluate(self, n_episodes: int = 10) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞."""
    
    def save_checkpoint(self, timestep: int) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
    
    def load_checkpoint(self, path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
```

### TrainingResult

–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π:

```python
@dataclass
class TrainingResult:
    success: bool
    total_timesteps: int
    training_time: float
    final_mean_reward: float
    final_std_reward: float
    
    # –ò—Å—Ç–æ—Ä–∏—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    training_history: Dict[str, List[float]]
    evaluation_history: Dict[str, List[float]]
    model_path: Optional[str]
    checkpoint_paths: List[str]
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    best_mean_reward: float
    convergence_timestep: Optional[int]
    early_stopped: bool
    error_message: Optional[str]
```

## üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞

```python
from src.agents.base import AgentConfig

# –î–µ—Ç–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
agent_config = AgentConfig(
    algorithm="PPO",
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    policy_kwargs={
        "net_arch": [dict(pi=[64, 64], vf=[64, 64])],
        "activation_fn": "tanh",
    },
)

config = TrainerConfig(
    experiment_name="custom_ppo",
    agent_config=agent_config,
    # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
)
```

### –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
config = TrainerConfig(
    mode=TrainingMode.RESUME,
    resume_from_checkpoint="checkpoints/checkpoint_50000.pkl",
    total_timesteps=100_000,  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
)

with Trainer(config) as trainer:
    result = trainer.train()  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç —Å 50000 —à–∞–≥–æ–≤
```

### –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞

```python
config = TrainerConfig(
    early_stopping=True,
    patience=5,  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Å–ª–µ 5 –æ—Ü–µ–Ω–æ–∫ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    min_improvement=10.0,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
    eval_freq=5000,  # –ß–∞—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
)
```

### –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ RLConfig

```python
from src.utils.config import load_config

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ YAML
rl_config = load_config(config_name="my_config")

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ TrainerConfig
trainer_config = TrainerConfig.from_rl_config(rl_config)

trainer = Trainer(trainer_config)
```

## üéÆ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

```python
algorithms = ["PPO", "A2C", "SAC"]
results = {}

for algorithm in algorithms:
    config = TrainerConfig(
        experiment_name=f"comparison_{algorithm}",
        algorithm=algorithm,
        total_timesteps=50_000,
        seed=42,
    )
    
    with Trainer(config) as trainer:
        result = trainer.train()
        results[algorithm] = result

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
for alg, result in results.items():
    if result.success:
        print(f"{alg}: {result.final_mean_reward:.2f}")
```

### –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

```python
learning_rates = [1e-4, 3e-4, 1e-3]
best_result = None
best_reward = float("-inf")

for lr in learning_rates:
    config = TrainerConfig(
        experiment_name=f"hypersearch_lr_{lr}",
        algorithm="PPO",
        total_timesteps=30_000,
    )
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ learning_rate
    config.agent_config.learning_rate = lr
    
    with Trainer(config) as trainer:
        result = trainer.train()
        
        if result.success and result.final_mean_reward > best_reward:
            best_reward = result.final_mean_reward
            best_result = result

print(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_reward:.2f}")
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

```python
class CustomCallback(BaseCallback):
    def __init__(self):
        super().__init__()
        self.rewards = []
    
    def _on_step(self) -> bool:
        # –ö–∞—Å—Ç–æ–º–Ω–∞—è –ª–æ–≥–∏–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        if self.n_calls % 1000 == 0:
            print(f"–®–∞–≥ {self.n_calls}")
        return True

config = TrainerConfig(
    experiment_name="monitored_training",
    total_timesteps=100_000,
)

with Trainer(config) as trainer:
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ callback
    callback = CustomCallback()
    result = trainer.train()
```

## üîç –û—Ç–ª–∞–¥–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import logging
from src.utils.logging import setup_logging

# –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
setup_logging(level=logging.DEBUG)

config = TrainerConfig(
    verbose=2,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
    log_interval=100,  # –ß–∞—Å—Ç–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
)
```

### –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
# –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
result = trainer.train()

if result.success:
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    rewards = result.evaluation_history.get("mean_rewards", [])
    timesteps = result.evaluation_history.get("timesteps", [])
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    import matplotlib.pyplot as plt
    plt.plot(timesteps, rewards)
    plt.xlabel("Timesteps")
    plt.ylabel("Mean Reward")
    plt.title("Training Progress")
    plt.show()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    result.save("results/training_result.yaml")
```

## üö® –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
try:
    with Trainer(config) as trainer:
        result = trainer.train()
        
        if not result.success:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {result.error_message}")
            
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
            for warning in result.warnings:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {warning}")
                
except KeyboardInterrupt:
    print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    
except Exception as e:
    print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
```

## üìà –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏

```python
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
config = TrainerConfig(
    track_experiment=True,
    experiment_tags=["baseline", "ppo", "lunarlander"],
)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∏—Å—Ç–µ–º—É —Ç—Ä–µ–∫–∏–Ω–≥–∞
with Trainer(config) as trainer:
    result = trainer.train()
    # –ú–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
```

## üéØ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

1. **–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ seed** –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
2. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ eval_freq** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
3. **–í–∫–ª—é—á–∏—Ç–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã** –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
4. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞** –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
5. **–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏** –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
6. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ —Ä–µ—Å—É—Ä—Å—ã** –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏

- `src.agents` - –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ RL –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- `src.environments` - –û–±–µ—Ä—Ç–∫–∏ –¥–ª—è —Å—Ä–µ–¥
- `src.utils` - –£—Ç–∏–ª–∏—Ç—ã (–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
- `src.experiments` - –°–∏—Å—Ç–µ–º–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

## üìù Changelog

### v1.0.0
- –ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π PPO, A2C, SAC, TD3
- CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –°–∏—Å—Ç–µ–º–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏