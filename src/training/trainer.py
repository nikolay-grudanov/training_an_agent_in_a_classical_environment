"""–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π
–∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã: –∞–≥–µ–Ω—Ç–æ–≤, —Å—Ä–µ–¥—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é,
–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:
- PPO (Proximal Policy Optimization)
- A2C (Advantage Actor-Critic) 
- SAC (Soft Actor-Critic)
- TD3 (Twin Delayed Deep Deterministic Policy Gradient)

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ YAML/Hydra
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
- –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º —Ç—Ä–µ–∫–∏–Ω–≥–æ–º
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è
"""

import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Type, Union

import gymnasium as gym
import numpy as np
import yaml
from stable_baselines3.common.callbacks import BaseCallback, CallbackList

from src.agents import (
    Agent,
    AgentConfig,
    TrainingResult as AgentTrainingResult,
    PPOAgent,
    A2CAgent,
    SACAgent,
    TD3Agent,
)
from src.environments import EnvironmentWrapper, LunarLanderEnvironment
from src.utils import (
    CheckpointManager,
    MetricsTracker,
    get_experiment_logger,
    get_metrics_tracker,
    set_seed,
    RLConfig,
    load_config,
)

logger = logging.getLogger(__name__)


class TrainingMode(Enum):
    """–†–µ–∂–∏–º—ã –æ–±—É—á–µ–Ω–∏—è."""
    
    TRAIN = "train"           # –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
    RESUME = "resume"         # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
    EVALUATE = "evaluate"     # –¢–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∞
    FINETUNE = "finetune"     # –î–æ–æ–±—É—á–µ–Ω–∏–µ


@dataclass
class TrainerConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è.
    
    –°–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è,
    –≤–∫–ª—é—á–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≥–µ–Ω—Ç–∞, —Å—Ä–µ–¥—ã, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    """
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    experiment_name: str = "default_experiment"
    algorithm: str = "PPO"
    environment_name: str = "LunarLander-v3"
    mode: TrainingMode = TrainingMode.TRAIN
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    total_timesteps: int = 100_000
    seed: int = 42
    n_envs: int = 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent_config: Optional[AgentConfig] = None
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ä–µ–¥—ã
    env_config: Optional[Dict[str, Any]] = None
    use_lunar_lander_wrapper: bool = True
    render_mode: Optional[str] = None
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ü–µ–Ω–∫–∞
    eval_freq: int = 10_000
    n_eval_episodes: int = 10
    eval_deterministic: bool = True
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
    save_freq: int = 50_000
    checkpoint_freq: int = 25_000
    max_checkpoints: int = 5
    
    # –ü—É—Ç–∏
    output_dir: str = "results"
    model_save_path: Optional[str] = None
    logs_dir: Optional[str] = None
    tensorboard_log: Optional[str] = None
    
    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    resume_from_checkpoint: Optional[str] = None
    resume_timestep: Optional[int] = None
    
    # –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    early_stopping: bool = False
    patience: int = 5
    min_improvement: float = 0.01
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    verbose: int = 1
    log_interval: int = 1000
    progress_bar: bool = True
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
    track_experiment: bool = True
    experiment_tags: List[str] = field(default_factory=list)
    
    def __post_init__(self) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        supported_algorithms = ["PPO", "A2C", "SAC", "TD3"]
        if self.algorithm.upper() not in supported_algorithms:
            raise ValueError(
                f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {self.algorithm}. "
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ: {supported_algorithms}"
            )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        self.algorithm = self.algorithm.upper()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if self.total_timesteps <= 0:
            raise ValueError(f"total_timesteps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.total_timesteps}")
        
        if self.eval_freq <= 0:
            raise ValueError(f"eval_freq –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.eval_freq}")
        
        if self.n_eval_episodes <= 0:
            raise ValueError(f"n_eval_episodes –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0: {self.n_eval_episodes}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Ç–µ–π
        self._setup_paths()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞
        if self.agent_config is None:
            self.agent_config = self._create_default_agent_config()
    
    def _setup_paths(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        output_path = Path(self.output_dir)
        experiment_path = output_path / self.experiment_name
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        experiment_path.mkdir(parents=True, exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
        if self.model_save_path is None:
            self.model_save_path = str(experiment_path / "models" / f"{self.algorithm.lower()}_model")
        
        if self.logs_dir is None:
            self.logs_dir = str(experiment_path / "logs")
        
        if self.tensorboard_log is None:
            self.tensorboard_log = str(experiment_path / "tensorboard")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        Path(self.model_save_path).parent.mkdir(parents=True, exist_ok=True)
        Path(self.logs_dir).mkdir(parents=True, exist_ok=True)
        Path(self.tensorboard_log).mkdir(parents=True, exist_ok=True)
    
    def _create_default_agent_config(self) -> AgentConfig:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        return AgentConfig(
            algorithm=self.algorithm,
            env_name=self.environment_name,
            total_timesteps=self.total_timesteps,
            seed=self.seed,
            model_save_path=self.model_save_path,
            tensorboard_log=self.tensorboard_log,
            verbose=self.verbose,
        )
    
    @classmethod
    def from_rl_config(cls, rl_config: RLConfig) -> "TrainerConfig":
        """–°–æ–∑–¥–∞—Ç—å TrainerConfig –∏–∑ RLConfig.
        
        Args:
            rl_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RL —Å–∏—Å—Ç–µ–º—ã
            
        Returns:
            –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞
        agent_config = AgentConfig(
            algorithm=rl_config.algorithm.name,
            env_name=rl_config.environment.name,
            total_timesteps=rl_config.training.total_timesteps,
            seed=rl_config.seed,
            learning_rate=rl_config.algorithm.learning_rate,
            batch_size=rl_config.algorithm.batch_size,
            n_steps=rl_config.algorithm.n_steps,
            n_epochs=rl_config.algorithm.n_epochs,
            gamma=rl_config.algorithm.gamma,
            gae_lambda=rl_config.algorithm.gae_lambda,
            clip_range=rl_config.algorithm.clip_range,
            ent_coef=rl_config.algorithm.ent_coef,
            vf_coef=rl_config.algorithm.vf_coef,
            max_grad_norm=rl_config.algorithm.max_grad_norm,
            use_sde=rl_config.algorithm.use_sde,
            sde_sample_freq=rl_config.algorithm.sde_sample_freq,
            target_kl=rl_config.algorithm.target_kl,
            device=rl_config.algorithm.device,
            verbose=rl_config.algorithm.verbose,
            policy_kwargs=rl_config.algorithm.policy_kwargs,
            tensorboard_log=rl_config.algorithm.tensorboard_log,
        )
        
        return cls(
            experiment_name=rl_config.experiment_name,
            algorithm=rl_config.algorithm.name,
            environment_name=rl_config.environment.name,
            total_timesteps=rl_config.training.total_timesteps,
            seed=rl_config.seed,
            agent_config=agent_config,
            eval_freq=rl_config.training.eval_freq,
            n_eval_episodes=rl_config.training.n_eval_episodes,
            save_freq=rl_config.training.save_freq,
            output_dir=rl_config.output_dir,
            early_stopping=rl_config.training.early_stopping,
            patience=rl_config.training.patience,
            min_improvement=rl_config.training.min_delta,
            log_interval=rl_config.training.log_interval,
        )


@dataclass
class TrainingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    success: bool
    total_timesteps: int
    training_time: float
    final_mean_reward: float
    final_std_reward: float
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–≥–µ–Ω—Ç–∞
    agent_result: Optional[AgentTrainingResult] = None
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    training_history: Dict[str, List[float]] = field(default_factory=dict)
    evaluation_history: Dict[str, List[float]] = field(default_factory=dict)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    model_path: Optional[str] = None
    checkpoint_paths: List[str] = field(default_factory=list)
    config_path: Optional[str] = None
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    experiment_name: str = ""
    algorithm: str = ""
    environment_name: str = ""
    seed: int = 42
    
    # –û—à–∏–±–∫–∏
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    best_mean_reward: float = float("-inf")
    convergence_timestep: Optional[int] = None
    early_stopped: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å."""
        result_dict = {
            "success": self.success,
            "total_timesteps": self.total_timesteps,
            "training_time": self.training_time,
            "final_mean_reward": self.final_mean_reward,
            "final_std_reward": self.final_std_reward,
            "best_mean_reward": self.best_mean_reward,
            "convergence_timestep": self.convergence_timestep,
            "early_stopped": self.early_stopped,
            "experiment_name": self.experiment_name,
            "algorithm": self.algorithm,
            "environment_name": self.environment_name,
            "seed": self.seed,
            "model_path": self.model_path,
            "checkpoint_paths": self.checkpoint_paths,
            "config_path": self.config_path,
            "error_message": self.error_message,
            "warnings": self.warnings,
            "training_history": self.training_history,
            "evaluation_history": self.evaluation_history,
        }
        
        if self.agent_result:
            result_dict["agent_result"] = {
                "total_timesteps": self.agent_result.total_timesteps,
                "training_time": self.agent_result.training_time,
                "final_mean_reward": self.agent_result.final_mean_reward,
                "final_std_reward": self.agent_result.final_std_reward,
                "episode_rewards": self.agent_result.episode_rewards,
                "episode_lengths": self.agent_result.episode_lengths,
                "best_mean_reward": self.agent_result.best_mean_reward,
                "success": self.agent_result.success,
            }
        
        return result_dict
    
    def save(self, path: Union[str, Path]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª.
        
        Args:
            path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w', encoding='utf-8') as f:
            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)


class Trainer:
    """–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤.
    
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è:
    - –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–∞–º–∏
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π
    - –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
    —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞.
    """
    
    # –ú–∞–ø–ø–∏–Ω–≥ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å—ã –∞–≥–µ–Ω—Ç–æ–≤
    AGENT_CLASSES: Dict[str, Type[Agent]] = {
        "PPO": PPOAgent,
        "A2C": A2CAgent,
        "SAC": SACAgent,
        "TD3": TD3Agent,
    }
    
    def __init__(self, config: TrainerConfig) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–Ω–µ—Ä.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
            
        Raises:
            ValueError: –ü—Ä–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            RuntimeError: –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        self.config = config
        self.experiment_name = config.experiment_name
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        set_seed(config.seed)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = get_experiment_logger(
            experiment_id=self.experiment_name,
            log_level=logging.INFO if config.verbose > 0 else logging.WARNING,
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–∫–µ—Ä–∞ –º–µ—Ç—Ä–∏–∫
        self.metrics_tracker = get_metrics_tracker(
            experiment_id=self.experiment_name
        ) if config.track_experiment else None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        checkpoint_dir = Path(config.output_dir) / config.experiment_name / "checkpoints"
        self.checkpoint_manager = CheckpointManager(
            checkpoint_dir=checkpoint_dir,
            max_checkpoints=config.max_checkpoints,
        )
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.agent: Optional[Agent] = None
        self.env: Optional[gym.Env] = None
        self.training_result: Optional[TrainingResult] = None
        self.current_timestep = 0
        self.best_mean_reward = float("-inf")
        self.patience_counter = 0
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.training_history: Dict[str, List[float]] = {
            "timesteps": [],
            "mean_rewards": [],
            "std_rewards": [],
            "episode_lengths": [],
        }
        
        self.evaluation_history: Dict[str, List[float]] = {
            "timesteps": [],
            "mean_rewards": [],
            "std_rewards": [],
        }
        
        self.logger.info(
            f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω Trainer –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ '{self.experiment_name}'",
            extra={
                "algorithm": config.algorithm,
                "environment": config.environment_name,
                "mode": config.mode.value,
                "total_timesteps": config.total_timesteps,
                "seed": config.seed,
            }
        )
    
    def setup(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        Raises:
            RuntimeError: –ü—Ä–∏ –æ—à–∏–±–∫–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
            self._setup_environment()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
            self._setup_agent()
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if self.config.mode == TrainingMode.RESUME:
                self._resume_training()
            
            self.logger.info("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}"
            self.logger.error(error_msg)
            raise RuntimeError(error_msg) from e
    
    def train(self) -> TrainingResult:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞.
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø—É—Ç—è–º–∏ –∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º
            
        Raises:
            RuntimeError: –ü—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—É—á–µ–Ω–∏—è
        """
        if self.config.mode == TrainingMode.EVALUATE:
            return self._evaluate_only()
        
        start_time = time.time()
        
        try:
            self.logger.info(
                f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ {self.config.mode.value}",
                extra={
                    "total_timesteps": self.config.total_timesteps,
                    "current_timestep": self.current_timestep,
                }
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ callbacks
            callbacks = self._create_callbacks()
            
            # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
            remaining_timesteps = self.config.total_timesteps - self.current_timestep
            
            if remaining_timesteps > 0:
                agent_result = self.agent.train(
                    total_timesteps=remaining_timesteps,
                    callback=callbacks,
                )
            else:
                # –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, —Å–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                agent_result = AgentTrainingResult(
                    total_timesteps=self.current_timestep,
                    training_time=0.0,
                    final_mean_reward=self.best_mean_reward,
                    final_std_reward=0.0,
                    success=True,
                )
            
            training_time = time.time() - start_time
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            final_eval = self._evaluate_agent()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            final_model_path = self._save_final_model()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self.training_result = TrainingResult(
                success=True,
                total_timesteps=self.config.total_timesteps,
                training_time=training_time,
                final_mean_reward=final_eval["mean_reward"],
                final_std_reward=final_eval["std_reward"],
                agent_result=agent_result,
                training_history=self.training_history,
                evaluation_history=self.evaluation_history,
                model_path=final_model_path,
                checkpoint_paths=self.checkpoint_manager.get_checkpoint_paths(),
                experiment_name=self.experiment_name,
                algorithm=self.config.algorithm,
                environment_name=self.config.environment_name,
                seed=self.config.seed,
                best_mean_reward=self.best_mean_reward,
                early_stopped=self.patience_counter >= self.config.patience,
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self._save_training_result()
            
            self.logger.info(
                "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ",
                extra={
                    "training_time": training_time,
                    "final_mean_reward": final_eval["mean_reward"],
                    "best_mean_reward": self.best_mean_reward,
                    "total_timesteps": self.config.total_timesteps,
                }
            )
            
            return self.training_result
            
        except Exception as e:
            training_time = time.time() - start_time
            error_msg = f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}"
            self.logger.error(error_msg)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –æ—à–∏–±–∫–æ–π
            self.training_result = TrainingResult(
                success=False,
                total_timesteps=self.current_timestep,
                training_time=training_time,
                final_mean_reward=self.best_mean_reward,
                final_std_reward=0.0,
                experiment_name=self.experiment_name,
                algorithm=self.config.algorithm,
                environment_name=self.config.environment_name,
                seed=self.config.seed,
                error_message=error_msg,
            )
            
            return self.training_result
    
    def evaluate(
        self,
        n_episodes: Optional[int] = None,
        deterministic: Optional[bool] = None,
        render: bool = False,
    ) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            deterministic: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            render: –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ä–µ–¥—É
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
            
        Raises:
            RuntimeError: –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –Ω–µ –æ–±—É—á–µ–Ω
        """
        if self.agent is None:
            raise RuntimeError("–ê–≥–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –í—ã–∑–æ–≤–∏—Ç–µ setup().")
        
        n_episodes = n_episodes or self.config.n_eval_episodes
        deterministic = deterministic if deterministic is not None else self.config.eval_deterministic
        
        self.logger.info(
            f"–ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–∞—Ö",
            extra={"deterministic": deterministic, "render": render}
        )
        
        return self.agent.evaluate(
            n_episodes=n_episodes,
            deterministic=deterministic,
            render=render,
        )
    
    def save_checkpoint(self, timestep: int) -> str:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç.
        
        Args:
            timestep: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —á–µ–∫–ø–æ–∏–Ω—Ç—É
        """
        checkpoint_data = {
            "timestep": timestep,
            "config": self.config,
            "training_history": self.training_history,
            "evaluation_history": self.evaluation_history,
            "best_mean_reward": self.best_mean_reward,
            "patience_counter": self.patience_counter,
        }
        
        checkpoint_path = self.checkpoint_manager.save_checkpoint(
            checkpoint_data,
            timestep=timestep,
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞
        if self.agent:
            model_path = Path(checkpoint_path).parent / f"model_{timestep}.zip"
            self.agent.save(str(model_path))
        
        self.logger.info(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_path}")
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç.
        
        Args:
            checkpoint_path: –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É
            
        Raises:
            FileNotFoundError: –ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
            RuntimeError: –ü—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏
        """
        try:
            checkpoint_data = self.checkpoint_manager.load_checkpoint(checkpoint_path)
            
            self.current_timestep = checkpoint_data["timestep"]
            self.training_history = checkpoint_data.get("training_history", {})
            self.evaluation_history = checkpoint_data.get("evaluation_history", {})
            self.best_mean_reward = checkpoint_data.get("best_mean_reward", float("-inf"))
            self.patience_counter = checkpoint_data.get("patience_counter", 0)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞
            model_path = Path(checkpoint_path).parent / f"model_{self.current_timestep}.zip"
            if model_path.exists() and self.agent:
                agent_class = self.AGENT_CLASSES[self.config.algorithm]
                self.agent = agent_class.load(str(model_path), env=self.env)
            
            self.logger.info(
                f"–ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {checkpoint_path}",
                extra={"timestep": self.current_timestep}
            )
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ {checkpoint_path}: {e}"
            self.logger.error(error_msg)
            raise RuntimeError(error_msg) from e
    
    def cleanup(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã."""
        if self.env:
            self.env.close()
        
        self.logger.info("–†–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã")
    
    def _setup_environment(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Å—Ä–µ–¥—É."""
        env_config = self.config.env_config or {}
        
        if self.config.use_lunar_lander_wrapper and "LunarLander" in self.config.environment_name:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ wrapper –¥–ª—è LunarLander
            self.env = LunarLanderEnvironment(
                render_mode=self.config.render_mode,
                **env_config
            )
        else:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ wrapper
            self.env = EnvironmentWrapper(
                env_id=self.config.environment_name,
                config=env_config,
                render_mode=self.config.render_mode,
            )
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è —Å—Ä–µ–¥—ã
        if hasattr(self.env, 'seed'):
            self.env.seed(self.config.seed)
        
        self.logger.info(
            f"–°—Ä–µ–¥–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: {self.config.environment_name}",
            extra={
                "action_space": str(self.env.action_space),
                "observation_space": str(self.env.observation_space),
            }
        )
    
    def _setup_agent(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        agent_class = self.AGENT_CLASSES.get(self.config.algorithm)
        if agent_class is None:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {self.config.algorithm}")
        
        self.agent = agent_class(
            config=self.config.agent_config,
            env=self.env,
            experiment_name=self.experiment_name,
        )
        
        self.logger.info(f"–ê–≥–µ–Ω—Ç {self.config.algorithm} –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
    def _resume_training(self) -> None:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
        if self.config.resume_from_checkpoint:
            self.load_checkpoint(self.config.resume_from_checkpoint)
        elif self.config.resume_timestep:
            # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            checkpoint_path = self.checkpoint_manager.find_checkpoint(self.config.resume_timestep)
            if checkpoint_path:
                self.load_checkpoint(checkpoint_path)
            else:
                self.logger.warning(
                    f"–ß–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è timestep {self.config.resume_timestep} –Ω–µ –Ω–∞–π–¥–µ–Ω"
                )
    
    def _evaluate_only(self) -> TrainingResult:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫—É –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è."""
        if self.agent is None or not self.agent.is_trained:
            raise RuntimeError("–ê–≥–µ–Ω—Ç –Ω–µ –æ–±—É—á–µ–Ω. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ—Ü–µ–Ω–∫—É.")
        
        start_time = time.time()
        
        eval_result = self._evaluate_agent()
        
        training_time = time.time() - start_time
        
        return TrainingResult(
            success=True,
            total_timesteps=0,
            training_time=training_time,
            final_mean_reward=eval_result["mean_reward"],
            final_std_reward=eval_result["std_reward"],
            experiment_name=self.experiment_name,
            algorithm=self.config.algorithm,
            environment_name=self.config.environment_name,
            seed=self.config.seed,
        )
    
    def _evaluate_agent(self) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        return self.agent.evaluate(
            n_episodes=self.config.n_eval_episodes,
            deterministic=self.config.eval_deterministic,
        )
    
    def _create_callbacks(self) -> CallbackList:
        """–°–æ–∑–¥–∞—Ç—å callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        callbacks = []
        
        # Callback –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        if self.config.eval_freq > 0:
            eval_callback = self._create_eval_callback()
            callbacks.append(eval_callback)
        
        # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if self.config.save_freq > 0:
            save_callback = self._create_save_callback()
            callbacks.append(save_callback)
        
        # Callback –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        if self.config.checkpoint_freq > 0:
            checkpoint_callback = self._create_checkpoint_callback()
            callbacks.append(checkpoint_callback)
        
        return CallbackList(callbacks)
    
    def _create_eval_callback(self) -> BaseCallback:
        """–°–æ–∑–¥–∞—Ç—å callback –¥–ª—è –æ—Ü–µ–Ω–∫–∏."""
        from stable_baselines3.common.callbacks import EvalCallback
        
        return EvalCallback(
            eval_env=self.env,
            n_eval_episodes=self.config.n_eval_episodes,
            eval_freq=self.config.eval_freq,
            deterministic=self.config.eval_deterministic,
            verbose=self.config.verbose,
        )
    
    def _create_save_callback(self) -> BaseCallback:
        """–°–æ–∑–¥–∞—Ç—å callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        from stable_baselines3.common.callbacks import CheckpointCallback
        
        return CheckpointCallback(
            save_freq=self.config.save_freq,
            save_path=str(Path(self.config.model_save_path).parent),
            name_prefix=f"{self.config.algorithm.lower()}_model",
            verbose=self.config.verbose,
        )
    
    def _create_checkpoint_callback(self) -> BaseCallback:
        """–°–æ–∑–¥–∞—Ç—å callback –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤."""
        class TrainerCheckpointCallback(BaseCallback):
            def __init__(self, trainer: "Trainer", freq: int):
                super().__init__()
                self.trainer = trainer
                self.freq = freq
            
            def _on_step(self) -> bool:
                if self.n_calls % self.freq == 0:
                    self.trainer.save_checkpoint(self.n_calls)
                return True
        
        return TrainerCheckpointCallback(self, self.config.checkpoint_freq)
    
    def _save_final_model(self) -> str:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å."""
        final_path = f"{self.config.model_save_path}_final"
        self.agent.save(final_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = f"{final_path}_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config.__dict__, f, default_flow_style=False, allow_unicode=True)
        
        return final_path
    
    def _save_training_result(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è."""
        if self.training_result:
            result_path = Path(self.config.output_dir) / self.experiment_name / "training_result.yaml"
            self.training_result.config_path = str(result_path.parent / "config.yaml")
            self.training_result.save(result_path)
    
    def __enter__(self) -> "Trainer":
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä."""
        self.setup()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ."""
        self.cleanup()


def create_trainer_from_config(
    config_path: Optional[str] = None,
    config_name: Optional[str] = None,
    overrides: Optional[List[str]] = None,
    **kwargs: Any,
) -> Trainer:
    """–°–æ–∑–¥–∞—Ç—å —Ç—Ä–µ–Ω–µ—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
    
    Args:
        config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_name: –ò–º—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ configs/
        overrides: –°–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è TrainerConfig
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
        
    Raises:
        FileNotFoundError: –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        ValueError: –ü—Ä–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ RLConfig
    rl_config = load_config(
        config_name=config_name,
        config_path=config_path,
        overrides=overrides,
    )
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ TrainerConfig
    trainer_config = TrainerConfig.from_rl_config(rl_config)
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    for key, value in kwargs.items():
        if hasattr(trainer_config, key):
            setattr(trainer_config, key, value)
    
    return Trainer(trainer_config)


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    import argparse
    
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ RL –∞–≥–µ–Ω—Ç–æ–≤")
    parser.add_argument("--config", type=str, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    parser.add_argument("--config-name", type=str, help="–ò–º—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    parser.add_argument("--algorithm", type=str, choices=["PPO", "A2C", "SAC", "TD3"], 
                       help="–ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--env", type=str, help="–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã")
    parser.add_argument("--timesteps", type=int, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--seed", type=int, default=42, help="Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏")
    parser.add_argument("--mode", type=str, choices=["train", "resume", "evaluate"], 
                       default="train", help="–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã")
    parser.add_argument("--resume-from", type=str, help="–ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
    parser.add_argument("--output-dir", type=str, default="results", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--experiment-name", type=str, help="–ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    
    args = parser.parse_args()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    overrides = []
    if args.algorithm:
        overrides.append(f"algorithm.name={args.algorithm}")
    if args.env:
        overrides.append(f"environment.name={args.env}")
    if args.timesteps:
        overrides.append(f"training.total_timesteps={args.timesteps}")
    if args.seed:
        overrides.append(f"seed={args.seed}")
    if args.experiment_name:
        overrides.append(f"experiment_name={args.experiment_name}")
    if args.output_dir:
        overrides.append(f"output_dir={args.output_dir}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = create_trainer_from_config(
        config_path=args.config,
        config_name=args.config_name,
        overrides=overrides,
        mode=TrainingMode(args.mode),
        resume_from_checkpoint=args.resume_from,
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    with trainer:
        result = trainer.train()
        
        if result.success:
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.1f} —Å–µ–∫")
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {result.model_path}")
        else:
            print(f"‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.error_message}")


if __name__ == "__main__":
    main()