# API Contracts: ML Project Architecture

**Feature**: 004-test-and-fix | **Date**: 2026-02-04
**Project Type**: Machine Learning (Reinforcement Learning) | **Phase**: 1 (Design & Contracts)

---

## üìã NOTE: ML Project Architecture

**–≠—Ç–æ ML –ø—Ä–æ–µ–∫—Ç (Reinforcement Learning), NOT traditional web application.**

**–ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è**:
- ‚ùå **–ù–µ—Ç REST API** - –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Python API (Stable-Baselines3)
- ‚ùå **–ù–µ—Ç GraphQL API** - –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤
- ‚ùå **–ù–µ—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö** - –î–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —Ñ–∞–π–ª–∞—Ö (CSV, JSON, ZIP)
- ‚úÖ **–ï—Å—Ç—å Python API** - Stable-Baselines3, Gymnasium, PyTorch
- ‚úÖ **–ï—Å—Ç—å CLI** - –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ **–ï—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏** - YAML/JSON —Ñ–∞–π–ª—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞**:
```
User (Developer/Researcher)
    ‚îÇ
    ‚îÇ CLI (Command Line Interface)
    ‚îÇ python -m src.experiments.completion.baseline_training ...
    ‚îÇ
    ‚ñº
Python API
    ‚îÇ
    ‚îú‚îÄ‚îÄ Stable-Baselines3 API (PPO, A2C, TD3)
    ‚îú‚îÄ‚îÄ Gymnasium API (Environments: LunarLander-v3)
    ‚îî‚îÄ‚îÄ PyTorch API (Deep Learning)
    ‚îÇ
    ‚ñº
File System
    ‚îÇ
    ‚îú‚îÄ‚îÄ Models (.zip files)
    ‚îú‚îÄ‚îÄ Metrics (CSV files)
    ‚îú‚îÄ‚îÄ Configurations (JSON files)
    ‚îú‚îÄ‚îÄ Checkpoints (ZIP files)
    ‚îî‚îÄ‚îÄ Visualizations (PNG, MP4 files)
```

**–ü–æ–ª–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å–º. –≤ –ø–∞–ø–∫–µ `/docs/`**:
- [PROJECT_CONTEXT.md](../../docs/PROJECT_CONTEXT.md) - –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞
- [QUICKSTART.md](../../docs/QUICKSTART.md) - –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- [TROUBLESHOOTING.md](../../docs/TROUBLESHOOTING.md) - –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

---

## Python API Contracts

### 1. Stable-Baselines3 API (PPO Agent)

**–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞**: `stable_baselines3.ppo.PPO`

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã**:

#### –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä
```python
def __init__(
    policy: Union[str, Type[ActorCriticPolicy]],
    env: Union[str, Env, VecEnv],
    learning_rate: float = 3e-4,
    n_steps: int = 2048,
    batch_size: int = 64,
    n_epochs: int = 10,
    gamma: float = 0.99,
    gae_lambda: float = 0.95,
    ent_coef: float = 0.0,
    verbose: int = 0,
    seed: Optional[int] = None,
    device: Union[str, th.device] = "auto",
    _init_setup_model: bool = True,
) -> None:
    """
    Proximal Policy Optimization (PPO)

    Args:
        policy: The policy model to use (MlpPolicy, CnnPolicy, etc.)
        env: The environment to learn from
        learning_rate: The learning rate, it can be a function
        n_steps: The number of steps to run for each environment per update
        batch_size: Minibatch size
        n_epochs: Number of epoch when optimizing the surrogate loss
        gamma: Discount factor
        gae_lambda: Factor for trade-off of bias vs variance for GAE
        ent_coef: Entropy coefficient for loss calculation
        verbose: Verbosity level: 0 for no output, 1 for info messages
        seed: Seed for the pseudo random generators
        device: Device (cpu, cuda, auto)
        _init_setup_model: Whether or not to build the network at the creation of the instance

    Returns:
        PPO agent instance
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
from stable_baselines3 import PPO

# –°–æ–∑–¥–∞–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞
model = PPO(
    policy="MlpPolicy",
    env="LunarLander-v3",
    learning_rate=3e-4,
    n_steps=1024,
    n_epochs=4,
    gamma=0.999,
    ent_coef=0.01,
    gae_lambda=0.98,
    verbose=1,
    seed=42,
    device="cpu"
)
```

---

#### –ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è
```python
def learn(
    self,
    total_timesteps: int,
    callback: Optional[Union[list, BaseCallback, MaybeCallback]] = None,
    log_interval: int = 100,
    tb_log_name: Optional[str] = "PPO",
    reset_num_timesteps: bool = True,
    progress_bar: bool = False,
) -> "PPO":
    """
    Return a trained model.

    Args:
        total_timesteps: The total number of samples (env steps) to train on
        callback: callback(s) called at every step with state of the algorithm
        log_interval: The number of timesteps before logging
        tb_log_name: The name of the run for TensorBoard
        reset_num_timesteps: Whether or not to reset the current timestep number
        progress_bar: Display a progress bar using tqdm and rich

    Returns:
        the trained model
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
model.learn(
    total_timesteps=500000,
    callback=[checkpoint_callback, eval_callback, metrics_callback]
)
```

---

#### –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
```python
def save(
    self,
    path: str,
    include: Optional[Sequence[str]] = None,
    exclude: Optional[Sequence[str]] = None,
) -> None:
    """
    Save the model to the given path.

    Args:
        path: the path to save the model to
        include: name of variables to include
        exclude: name of variables to exclude
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.save("results/experiments/ppo_seed42/ppo_seed42_model.zip")
```

---

#### –ú–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ (static)
```python
@staticmethod
def load(
    path: str,
    env: Optional[GymEnv] = None,
    device: Union[str, th.device] = "auto",
    custom_objects: Optional[Dict[str, Any]] = None,
    print_system_info: bool = False,
    force_reset: bool = True,
    **kwargs,
) -> "BaseAlgorithm":
    """
    Load the model from a zip-file.

    Args:
        path: The path to the file (or a file-like)
        env: the environment to use to evaluate the model if it was loaded with a different environment
        device: Device on which the code should run
        custom_objects: Dictionary of objects to replace upon loading
        print_system_info: Whether to print system info from the saved model
        force_reset: Force call to `reset_num_timesteps` (can be used to continue training)

    Returns:
        The loaded model
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = PPO.load("results/experiments/ppo_seed42/ppo_seed42_model.zip")
```

---

#### –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
```python
def predict(
    self,
    observation: Union[np.ndarray, Dict[str, np.ndarray]],
    state: Optional[Tuple[np.ndarray, ...]] = None,
    episode_start: Optional[np.ndarray] = None,
    deterministic: bool = False,
) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:
    """
    Get the model's action(s) from an observation.

    Args:
        observation: the input observation
        state: The last states (can be None, used in recurrent policies)
        episode_start: These last episode start(s) (can be None, used in recurrent policies)
        deterministic: Whether to use stochastic or deterministic actions

    Returns:
        The model's action and the next state (used in recurrent policies)
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
action, states = model.predict(observation, deterministic=True)
```

---

### 2. Gymnasium API (Environment)

**–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞**: `gymnasium`

#### –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
```python
def make(
    id: str,
    max_episode_steps: Optional[int] = None,
    autoreset: Optional[bool] = None,
    disable_env_checker: Optional[bool] = None,
    **kwargs,
) -> Env:
    """
    Create an environment from an ID.

    Args:
        id: The environment ID
        max_episode_steps: The maximum number of steps that an episode lasts
        autoreset: Whether to automatically reset the environment
        disable_env_checker: Whether to disable the environment checker
        **kwargs: Additional keyword arguments

    Returns:
        An instance of the environment
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
import gymnasium as gym

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã LunarLander-v3
env = gym.make("LunarLander-v3", render_mode="rgb_array")
```

---

#### –ú–µ—Ç–æ–¥ —Å–±—Ä–æ—Å–∞
```python
def reset(
    self,
    *,
    seed: Optional[int] = None,
    options: Optional[dict] = None,
) -> Tuple[ObsType, Dict[str, Any]]:
    """
    Reset the environment to an initial state.

    Args:
        seed: The seed for the PRNG
        options: Additional info to reset the environment with

    Returns:
        The initial observation and info dictionary
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –°–±—Ä–æ—Å —Å—Ä–µ–¥—ã
observation, info = env.reset(seed=42)
```

---

#### –ú–µ—Ç–æ–¥ —à–∞–≥–∞
```python
def step(
    self,
    action: ActType,
) -> Tuple[ObsType, SupportsFloat, bool, bool, Dict[str, Any]]:
    """
    Execute one step in the environment.

    Args:
        action: The action to take

    Returns:
        observation, reward, terminated, truncated, info
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –®–∞–≥ —Å—Ä–µ–¥—ã
observation, reward, terminated, truncated, info = env.step(action)
```

---

#### –ú–µ—Ç–æ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
```python
def render(self) -> Optional[Union[np.ndarray, str]]:
    """
    Render the environment to the screen

    Returns:
        None or a numpy array of RGB values
    """
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**:
```python
# –†–µ–Ω–¥–µ—Ä–∏–Ω–≥
frame = env.render()
```

---

### 3. CLI Contract (Command Line Interface)

**–°–∫—Ä–∏–ø—Ç**: `src/experiments/completion/baseline_training.py`

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã**:

#### –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
```bash
python -m src.experiments.completion.baseline_training \
    --algo ppo \
    --timesteps 200000 \
    --seed 42
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:
- `--algo`: –ê–ª–≥–æ—Ä–∏—Ç–º (ppo, a2c, td3)
- `--timesteps`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è (int)
- `--seed`: Random seed (int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 42)

---

#### –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (CPU)
```bash
python -m src.experiments.completion.baseline_training \
    --algo ppo \
    --timesteps 500000 \
    --seed 42 \
    --gamma 0.999 \
    --ent-coef 0.01 \
    --gae-lambda 0.98 \
    --n-steps 1024 \
    --n-epochs 4 \
    --device cpu
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:
- `--gamma`: Discount factor (float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.99)
- `--ent-coef`: Entropy coefficient (float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.0)
- `--gae-lambda`: GAE lambda (float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.95)
- `--n-steps`: Number of steps per update (int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048)
- `--n-epochs`: Number of epochs (int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
- `--device`: Device (auto/cpu/gpu/cuda/mps, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é auto)

---

#### –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (GPU)
```bash
CUDA_VISIBLE_DEVICES=0 HIP_VISIBLE_DEVICES=0 python -m src.experiments.completion.baseline_training \
    --algo ppo \
    --timesteps 500000 \
    --seed 42 \
    --gamma 0.999 \
    --ent-coef 0.01 \
    --gae-lambda 0.98 \
    --n-steps 1024 \
    --n-epochs 4 \
    --device auto
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è**:
- `CUDA_VISIBLE_DEVICES`: NVIDIA GPU ID (–¥–ª—è ROCm, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å "" –¥–ª—è CPU)
- `HIP_VISIBLE_DEVICES`: AMD GPU ID (–¥–ª—è ROCm, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å "" –¥–ª—è CPU)

---

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ (Integration Flows)

### 1. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è

```
[CLI Command]
    ‚îÇ
    ‚ñº
[Initialize Environment] (gym.make)
    ‚îÇ
    ‚ñº
[Initialize Agent] (PPO.__init__)
    ‚îÇ
    ‚ñº
[Train Agent] (model.learn)
    ‚îÇ
    ‚îú‚îÄ‚îÄ [Checkpoint Callback] - Save every 100K steps
    ‚îú‚îÄ‚îÄ [Eval Callback] - Evaluate every 10K steps
    ‚îî‚îÄ‚îÄ [Metrics Callback] - Log metrics every step
    ‚îÇ
    ‚ñº
[Save Final Model] (model.save)
    ‚îÇ
    ‚ñº
[Generate Plots] (matplotlib)
    ‚îÇ
    ‚ñº
[Generate Video] (gymnasiumÂΩïÂà∂)
```

---

### 2. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

```
[Load Model] (PPO.load)
    ‚îÇ
    ‚ñº
[Initialize Environment] (gym.make)
    ‚îÇ
    ‚ñº
[Reset Environment] (env.reset)
    ‚îÇ
    ‚ñº
[Loop Episodes]
    ‚îÇ
    ‚îú‚îÄ‚îÄ [Predict Action] (model.predict)
    ‚îú‚îÄ‚îÄ [Step Environment] (env.step)
    ‚îî‚îÄ‚îÄ [Check Done] (terminated or truncated)
    ‚îÇ
    ‚ñº
[Calculate Statistics] (mean, std, min, max)
    ‚îÇ
    ‚ñº
[Report Results]
```

---

## Error Handling

### –û—à–∏–±–∫–∏ Stable-Baselines3

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|----------|
| `ValueError: Unknown environment` | –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π ID —Å—Ä–µ–¥—ã | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å `gym.make("LunarLander-v3")` |
| `RuntimeError: CUDA out of memory` | –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ GPU | –£–º–µ–Ω—å—à–∏—Ç—å `batch_size` –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU |
| `UserWarning: You are trying to run PPO on the GPU` | GPU –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –Ω–∞ CPU | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `CUDA_VISIBLE_DEVICES=""` –∏ `HIP_VISIBLE_DEVICES=""` |

---

### –û—à–∏–±–∫–∏ Gymnasium

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|----------|
| `ImportError: No module named 'box2d'` | Box2D –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω | `pip install gymnasium[box2d]` |
| `gymnasium.error.DependencyNotInstalled: Box2D` | Box2D –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω | `pip install swig && pip install box2d-py` |

---

## –°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é

**–ü–æ–ª–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å–º. –≤ –ø–∞–ø–∫–µ `/docs/`**:

- üìÑ [PROJECT_CONTEXT.md](../../docs/PROJECT_CONTEXT.md) - –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞
- üìÑ [QUICKSTART.md](../../docs/QUICKSTART.md) - –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- üìÑ [TROUBLESHOOTING.md](../../docs/TROUBLESHOOTING.md) - –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

**–í–Ω–µ—à–Ω—è—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API**:

- üìñ [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/) - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è SB3
- üìñ [Gymnasium Documentation](https://gymnasium.farama.org/) - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Gymnasium
- üìñ [PyTorch Documentation](https://pytorch.org/docs/stable/) - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è PyTorch

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—ã–≤–∞–µ—Ç API –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –¥–ª—è ML –ø—Ä–æ–µ–∫—Ç–∞ (Reinforcement Learning), –≥–¥–µ:
- **Primary API**: Python API (Stable-Baselines3, Gymnasium, PyTorch)
- **CLI Interface**: –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- **No REST/GraphQL API**: –ù–µ—Ç HTTP endpoint-–æ–≤
- **No Database**: –î–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª–∞—Ö

–î–ª—è –ø–æ–ª–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–º. –ø–∞–ø–∫—É `/docs/`.
