"""–ü—Ä–∏–º–µ—Ä –±–∞–∑–æ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è.

–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Trainer:
- –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
- –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import logging
from pathlib import Path

from src.training import (
    Trainer,
    TrainerConfig,
    TrainingMode,
    create_trainer_from_config,
)
from src.agents.base import AgentConfig
from src.utils.logging import setup_logging


def basic_ppo_training():
    """–ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞."""
    print("üöÄ –ó–∞–ø—É—Å–∫ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è PPO...")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging(level=logging.INFO)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = TrainerConfig(
        experiment_name="basic_ppo_lunarlander",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        total_timesteps=100_000,
        seed=42,
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ü–µ–Ω–∫–∏
        eval_freq=10_000,
        n_eval_episodes=5,
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_freq=25_000,
        checkpoint_freq=20_000,
        
        # –ü—É—Ç–∏
        output_dir="results/examples",
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        verbose=1,
        progress_bar=True,
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–µ—Ä–∞
    with Trainer(config) as trainer:
        result = trainer.train()
        
        if result.success:
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}")
            print(f"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.best_mean_reward:.2f}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.1f} —Å–µ–∫")
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {result.model_path}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            print("\nüîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞...")
            eval_result = trainer.evaluate(n_episodes=10, render=False)
            print(f"üìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {eval_result['mean_reward']:.2f}")
            print(f"üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {eval_result['mean_length']:.1f}")
            
        else:
            print(f"‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.error_message}")
    
    return result


def compare_algorithms():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."""
    print("\nüî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤...")
    
    algorithms = ["PPO", "A2C"]
    results = {}
    
    for algorithm in algorithms:
        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ {algorithm}...")
        
        config = TrainerConfig(
            experiment_name=f"comparison_{algorithm.lower()}",
            algorithm=algorithm,
            environment_name="LunarLander-v3",
            total_timesteps=50_000,  # –ú–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            seed=42,
            eval_freq=10_000,
            output_dir="results/comparison",
            verbose=0,  # –ú–µ–Ω—å—à–µ –≤—ã–≤–æ–¥–∞
        )
        
        with Trainer(config) as trainer:
            result = trainer.train()
            results[algorithm] = result
            
            if result.success:
                print(f"‚úÖ {algorithm}: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}")
            else:
                print(f"‚ùå {algorithm}: –û—à–∏–±–∫–∞ - {result.error_message}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    print("-" * 50)
    for algorithm, result in results.items():
        if result.success:
            print(f"{algorithm:>8}: {result.final_mean_reward:>8.2f} ¬± {result.final_std_reward:>6.2f}")
        else:
            print(f"{algorithm:>8}: {'–û–®–ò–ë–ö–ê':>15}")
    
    return results


def advanced_training_with_config():
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
    print("\n‚öôÔ∏è  –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π...")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent_config = AgentConfig(
        algorithm="PPO",
        env_name="LunarLander-v3",
        total_timesteps=150_000,
        seed=42,
        
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã PPO
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,  # –ë–æ–ª—å—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        vf_coef=0.5,
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        policy_kwargs={
            "net_arch": [dict(pi=[64, 64], vf=[64, 64])],
            "activation_fn": "tanh",
        },
        
        verbose=1,
    )
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer_config = TrainerConfig(
        experiment_name="advanced_ppo_training",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        total_timesteps=150_000,
        seed=42,
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–∞
        agent_config=agent_config,
        
        # –ß–∞—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        eval_freq=5_000,
        n_eval_episodes=10,
        eval_deterministic=True,
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_freq=15_000,
        checkpoint_freq=10_000,
        max_checkpoints=10,
        
        # –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        early_stopping=True,
        patience=3,
        min_improvement=5.0,
        
        # –ü—É—Ç–∏
        output_dir="results/advanced",
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        verbose=1,
        log_interval=1000,
        track_experiment=True,
        experiment_tags=["advanced", "ppo", "lunarlander"],
    )
    
    with Trainer(trainer_config) as trainer:
        result = trainer.train()
        
        if result.success:
            print(f"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}")
            print(f"üõë –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {result.early_stopped}")
            print(f"üìà –ò—Å—Ç–æ—Ä–∏—è –æ—Ü–µ–Ω–∫–∏: {len(result.evaluation_history.get('mean_rewards', []))} —Ç–æ—á–µ–∫")
            
            # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
            if result.evaluation_history.get('mean_rewards'):
                rewards = result.evaluation_history['mean_rewards']
                print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {rewards[0]:.1f} ‚Üí {rewards[-1]:.1f}")
                print(f"üìä –ú–∞–∫—Å–∏–º—É–º: {max(rewards):.1f}")
        
        return result


def resume_training_example():
    """–ü—Ä–∏–º–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è."""
    print("\nüîÑ –ü—Ä–∏–º–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è...")
    
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å–∫–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    print("1Ô∏è‚É£ –ó–∞–ø—É—Å–∫ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
    
    config = TrainerConfig(
        experiment_name="resume_example",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        total_timesteps=50_000,
        seed=42,
        checkpoint_freq=10_000,
        output_dir="results/resume_example",
        verbose=0,
    )
    
    # –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    with Trainer(config) as trainer:
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ 20000 —à–∞–≥–æ–≤
        config.total_timesteps = 20_000
        result1 = trainer.train()
        
        if result1.success:
            print(f"‚úÖ –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å: {result1.final_mean_reward:.2f}")
            checkpoint_paths = result1.checkpoint_paths
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏")
            return
    
    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
    print("2Ô∏è‚É£ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ...")
    
    resume_config = TrainerConfig(
        experiment_name="resume_example_continued",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        total_timesteps=50_000,  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        seed=42,
        mode=TrainingMode.RESUME,
        resume_from_checkpoint=checkpoint_paths[-1] if checkpoint_paths else None,
        output_dir="results/resume_example",
        verbose=0,
    )
    
    with Trainer(resume_config) as trainer:
        result2 = trainer.train()
        
        if result2.success:
            print(f"‚úÖ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ: {result2.final_mean_reward:.2f}")
            print(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ: {result2.final_mean_reward - result1.final_mean_reward:.2f}")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏")
    
    return result1, result2


def config_file_training():
    """–û–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
    print("\nüìÑ –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
    config_content = """
experiment_name: "config_file_example"
output_dir: "results/config_example"
seed: 42

algorithm:
  name: "PPO"
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  gamma: 0.99

environment:
  name: "LunarLander-v3"

training:
  total_timesteps: 75000
  eval_freq: 15000
  n_eval_episodes: 5
  save_freq: 25000

logging:
  level: "INFO"
  log_to_file: true

reproducibility:
  seed: 42
  deterministic: true
"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_dir = Path("configs/examples")
    config_dir.mkdir(parents=True, exist_ok=True)
    config_file = config_dir / "example_config.yaml"
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"üìù –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_file}")
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        trainer = create_trainer_from_config(
            config_path=str(config_file),
            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
            overrides=[
                "training.total_timesteps=30000",  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                "algorithm.learning_rate=0.001",
            ]
        )
        
        with trainer:
            result = trainer.train()
            
            if result.success:
                print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
                print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.final_mean_reward:.2f}")
                print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_file}")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞: {result.error_message}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏."""
    print("üéÆ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    
    try:
        # 1. –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        result1 = basic_ppo_training()
        
        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        results2 = compare_algorithms()
        
        # 3. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        result3 = advanced_training_with_config()
        
        # 4. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        results4 = resume_training_example()
        
        # 5. –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        result5 = config_file_training()
        
        print("\nüéâ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
        print("\nüìã –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print("-" * 40)
        
        if result1 and result1.success:
            print(f"–ë–∞–∑–æ–≤–æ–µ PPO:     {result1.final_mean_reward:>8.2f}")
        
        if results2:
            for alg, res in results2.items():
                if res.success:
                    print(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ {alg}:   {res.final_mean_reward:>8.2f}")
        
        if result3 and result3.success:
            print(f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ:     {result3.final_mean_reward:>8.2f}")
        
        if result5 and result5.success:
            print(f"–ò–∑ —Ñ–∞–π–ª–∞:        {result5.final_mean_reward:>8.2f}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()