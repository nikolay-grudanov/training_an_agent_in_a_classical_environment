#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥—É–ª—è –æ—Ü–µ–Ω–∫–∏ RL –∞–≥–µ–Ω—Ç–æ–≤.

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Evaluator:
- –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
- –≠–∫—Å–ø–æ—Ä—Ç –≤ DataFrame
"""

import gymnasium as gym
import numpy as np
from pathlib import Path

from src.evaluation.evaluator import Evaluator
from src.agents.base import Agent
from typing import Any, Optional, Tuple


class ProgressCallback:
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ—Ü–µ–Ω–∫–∏."""
    
    def __init__(self) -> None:
        self.episodes_completed = 0
    
    def on_episode_start(self, episode: int) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞."""
        if episode % 10 == 0:
            print(f"–ù–∞—á–∞–ª–æ —ç–ø–∏–∑–æ–¥–∞ {episode}")
    
    def on_episode_end(
        self, 
        episode: int, 
        reward: float, 
        length: int, 
        success: bool
    ) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞."""
        self.episodes_completed += 1
        if episode % 10 == 0:
            print(f"–≠–ø–∏–∑–æ–¥ {episode}: –Ω–∞–≥—Ä–∞–¥–∞={reward:.2f}, –¥–ª–∏–Ω–∞={length}, —É—Å–ø–µ—Ö={success}")
    
    def on_evaluation_end(self, metrics) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ—Ü–µ–Ω–∫–∏."""
        print(f"–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.episodes_completed} —ç–ø–∏–∑–æ–¥–æ–≤")
        print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {metrics.mean_reward:.3f}")


class DummyAgent(Agent):
    """–ü—Ä–æ—Å—Ç–æ–π –∞–≥–µ–Ω—Ç-–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    
    def __init__(self, name: str, performance_level: float = 1.0) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞-–∑–∞–≥–ª—É—à–∫–∏.
        
        Args:
            name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            performance_level: –£—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–≤–ª–∏—è–µ—Ç –Ω–∞ –Ω–∞–≥—Ä–∞–¥—ã)
        """
        self.name = name
        self.performance_level = performance_level
        self.step_count = 0
    
    def predict(
        self, 
        observation: np.ndarray, 
        deterministic: bool = True, 
        **kwargs: Any
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è."""
        self.step_count += 1
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–µ–±–æ–ª—å—à–∏–º —Å–º–µ—â–µ–Ω–∏–µ–º
        if len(observation) >= 2:
            # –î–ª—è CartPole: –¥–µ–π—Å—Ç–≤–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É–≥–ª–∞
            action = 1 if observation[2] > 0 else 0
        else:
            action = np.random.randint(0, 2)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç performance_level
        if np.random.random() > self.performance_level:
            action = 1 - action  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        
        return np.array([action]), None
    
    def _create_model(self) -> Any:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."""
        return None
    
    def train(self, *args, **kwargs) -> Any:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        return None
    
    def save(self, path: str) -> None:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        pass
    
    @classmethod
    def load(cls, path: str, env: Optional[Any] = None, **kwargs: Any) -> "DummyAgent":
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏."""
        return cls("loaded_agent")


def create_dummy_agent(env: gym.Env, name: str, performance: float = 0.8) -> DummyAgent:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞-–∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    return DummyAgent(name=name, performance_level=performance)


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("ü§ñ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥—É–ª—è –æ—Ü–µ–Ω–∫–∏ RL –∞–≥–µ–Ω—Ç–æ–≤")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
    env = gym.make("CartPole-v1")
    env_name = getattr(env.spec, 'id', 'CartPole-v1') if env.spec else 'CartPole-v1'
    print(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Ä–µ–¥–∞: {env_name}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∞
    evaluator = Evaluator(
        env=env,
        success_threshold=200.0,  # –î–ª—è CartPole —É—Å–ø–µ—Ö = –Ω–∞–≥—Ä–∞–¥–∞ >= 200
        confidence_level=0.95,
        random_seed=42,
    )
    print("–°–æ–∑–¥–∞–Ω –æ—Ü–µ–Ω—â–∏–∫ –∞–≥–µ–Ω—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    print("\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤...")
    agent1 = create_dummy_agent(env, "PPO_Agent_1")
    agent2 = create_dummy_agent(env, "PPO_Agent_2")
    
    # 1. –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
    print("\nüîç –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞...")
    callback = ProgressCallback()
    
    metrics = evaluator.evaluate_agent(
        agent=agent1,
        num_episodes=50,
        agent_name="PPO_Agent_1",
        callback=callback,
    )
    
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ PPO_Agent_1:")
    print(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {metrics.mean_reward:.3f} ¬± {metrics.std_reward:.3f}")
    print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {metrics.mean_episode_length:.1f}")
    print(f"  –î–æ–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤: {metrics.success_rate:.1%}")
    print(f"  –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (95%): [{metrics.reward_ci_lower:.3f}, {metrics.reward_ci_upper:.3f}]")
    
    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤
    print("\n‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤...")
    comparison = evaluator.compare_agents(
        agent1=agent1,
        agent2=agent2,
        num_episodes=30,
        agent1_name="PPO_Agent_1",
        agent2_name="PPO_Agent_2",
    )
    
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
    print(f"  –õ—É—á—à–∏–π –∞–≥–µ–Ω—Ç: {comparison.better_agent}")
    print(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (–Ω–∞–≥—Ä–∞–¥—ã): {comparison.reward_significant}")
    print(f"  p-value (–Ω–∞–≥—Ä–∞–¥—ã): {comparison.reward_ttest_pvalue:.4f}")
    print(f"  –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (Cohen's d): {comparison.reward_effect_size:.3f}")
    
    # 3. –û—Ü–µ–Ω–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
    print("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤...")
    agents = {
        "PPO_Agent_1": agent1,
        "PPO_Agent_2": agent2,
    }
    
    results = evaluator.evaluate_multiple_agents(
        agents=agents,  # type: ignore
        num_episodes=20,
    )
    
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤:")
    for name, metrics in results.items():
        print(f"  {name}: {metrics.mean_reward:.3f} ¬± {metrics.std_reward:.3f}")
    
    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
    report = evaluator.generate_report(results)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_path = Path("evaluation_report.txt")
    evaluator.generate_report(results, save_path=report_path)
    print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_path}")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —á–∞—Å—Ç—å –æ—Ç—á–µ—Ç–∞
    print("\n–§—Ä–∞–≥–º–µ–Ω—Ç –æ—Ç—á–µ—Ç–∞:")
    print(report[:500] + "..." if len(report) > 500 else report)
    
    # 5. –≠–∫—Å–ø–æ—Ä—Ç –≤ DataFrame
    print("\nüìà –≠–∫—Å–ø–æ—Ä—Ç –≤ DataFrame...")
    df = evaluator.export_to_dataframe(results)
    print("\nDataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:")
    print(df.to_string(index=False))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    csv_path = Path("evaluation_results.csv")
    df.to_csv(csv_path, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV: {csv_path}")
    
    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"  - {report_path}")
    print(f"  - {csv_path}")
    
    # –û—á–∏—Å—Ç–∫–∞
    env.close()


if __name__ == "__main__":
    main()