#!/usr/bin/env python3
"""–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ Agent.

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–∞,
–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –±–∞–∑–æ–≤—ã–º –∫–ª–∞—Å—Å–æ–º.
"""

import logging
from pathlib import Path
from typing import Optional, Tuple, Any

import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback

from src.agents.base import Agent, AgentConfig, TrainingResult
from src.utils import set_seed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PPOAgent(Agent):
    """–ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ PPO –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞."""
    
    def _create_model(self):
        """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å PPO."""
        return PPO(
            policy=self.config.policy,
            env=self.env,
            learning_rate=self.config.learning_rate,
            n_steps=self.config.n_steps,
            batch_size=self.config.batch_size,
            n_epochs=self.config.n_epochs,
            gamma=self.config.gamma,
            gae_lambda=self.config.gae_lambda,
            clip_range=self.config.clip_range,
            ent_coef=self.config.ent_coef,
            vf_coef=self.config.vf_coef,
            max_grad_norm=self.config.max_grad_norm,
            use_sde=self.config.use_sde,
            sde_sample_freq=self.config.sde_sample_freq,
            target_kl=self.config.target_kl,
            policy_kwargs=self.config.policy_kwargs,
            device=self.config.device,
            verbose=self.config.verbose,
            seed=self.config.seed,
            tensorboard_log=self.config.tensorboard_log,
        )
    
    def train(
        self,
        total_timesteps: Optional[int] = None,
        callback: Optional[BaseCallback] = None,
        **kwargs: Any,
    ) -> TrainingResult:
        """–û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ PPO."""
        import time
        
        if self.model is None:
            self.model = self._create_model()
        
        timesteps = total_timesteps or self.config.total_timesteps
        start_time = time.time()
        
        try:
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.model.learn(
                total_timesteps=timesteps,
                callback=callback,
                **kwargs
            )
            
            training_time = time.time() - start_time
            self.is_trained = True
            
            # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            eval_metrics = self.evaluate(
                n_episodes=self.config.n_eval_episodes,
                deterministic=True,
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è
            self.training_result = TrainingResult(
                total_timesteps=timesteps,
                training_time=training_time,
                final_mean_reward=eval_metrics["mean_reward"],
                final_std_reward=eval_metrics["std_reward"],
                best_mean_reward=eval_metrics["mean_reward"],
                success=True,
            )
            
            self.logger.info(
                f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫",
                extra={
                    "timesteps": timesteps,
                    "mean_reward": eval_metrics["mean_reward"],
                    "std_reward": eval_metrics["std_reward"],
                }
            )
            
            return self.training_result
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}"
            self.logger.error(error_msg)
            
            self.training_result = TrainingResult(
                total_timesteps=timesteps,
                training_time=time.time() - start_time,
                final_mean_reward=float('-inf'),
                final_std_reward=0.0,
                success=False,
                error_message=error_msg,
            )
            
            raise RuntimeError(error_msg) from e
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = True,
        **kwargs: Any,
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ."""
        if not self.is_trained or self.model is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train().")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    @classmethod
    def load(
        cls,
        path: str,
        env: Optional[gym.Env] = None,
        **kwargs: Any,
    ) -> "PPOAgent":
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ PPO."""
        import yaml
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = Path(path).with_suffix('.yaml')
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            config = AgentConfig(**config_dict)
        else:
            raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        agent = cls(config=config, env=env)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        try:
            agent.model = PPO.load(path, env=agent.env)
            agent.is_trained = True
            
            agent.logger.info(f"–ê–≥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑: {path}")
            return agent
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ {path}: {e}"
            agent.logger.error(error_msg)
            raise RuntimeError(error_msg) from e


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ Agent")
    print("=" * 50)
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞...")
    config = AgentConfig(
        algorithm="PPO",
        env_name="CartPole-v1",
        total_timesteps=10_000,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        seed=42,
        verbose=1,
    )
    print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {config.algorithm} –¥–ª—è {config.env_name}")
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    print("\n2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞...")
    agent = PPOAgent(config=config, experiment_name="demo_cartpole")
    print(f"‚úÖ –ê–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {agent}")
    
    # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
    print("\n3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏...")
    info = agent.get_model_info()
    for key, value in info.items():
        print(f"   {key}: {value}")
    
    # 4. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    print("\n4. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
    try:
        result = agent.train()
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.2f} —Å–µ–∫")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f}")
        print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {result.final_std_reward:.2f}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        return
    
    # 5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\n5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    obs, _ = agent.env.reset()
    for i in range(5):
        action, _ = agent.predict(obs, deterministic=True)
        print(f"   –®–∞–≥ {i+1}: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ={obs[:2]}, –¥–µ–π—Å—Ç–≤–∏–µ={action}")
        obs, reward, terminated, truncated, _ = agent.env.step(action)
        if terminated or truncated:
            obs, _ = agent.env.reset()
    
    # 6. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\n6. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    eval_metrics = agent.evaluate(n_episodes=5, deterministic=True)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    for metric, value in eval_metrics.items():
        if isinstance(value, (int, float)):
            print(f"   {metric}: {value:.3f}")
        else:
            print(f"   {metric}: {value}")
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    save_path = "demo_cartpole_model.zip"
    try:
        agent.save(save_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
        model_file = Path(save_path)
        config_file = model_file.with_suffix('.yaml')
        print(f"   –§–∞–π–ª –º–æ–¥–µ–ª–∏: {model_file.exists()}")
        print(f"   –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_file.exists()}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    # 8. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\n8. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    try:
        loaded_agent = PPOAgent.load(save_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {loaded_agent}")
        
        # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        obs, _ = loaded_agent.env.reset()
        action, _ = loaded_agent.predict(obs)
        print(f"   –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ={obs[:2]}, –¥–µ–π—Å—Ç–≤–∏–µ={action}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    # 9. –°–±—Ä–æ—Å –º–æ–¥–µ–ª–∏
    print("\n9. –°–±—Ä–æ—Å –º–æ–¥–µ–ª–∏...")
    agent.reset_model()
    info_after_reset = agent.get_model_info()
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–±—Ä–æ—à–µ–Ω–∞, –æ–±—É—á–µ–Ω–∞: {info_after_reset['is_trained']}")
    
    print("\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    # –û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    try:
        Path(save_path).unlink(missing_ok=True)
        Path(save_path).with_suffix('.yaml').unlink(missing_ok=True)
        print("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")
    except Exception:
        pass


if __name__ == "__main__":
    main()