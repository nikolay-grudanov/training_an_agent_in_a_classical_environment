"""–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è PPO –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Å—Ä–µ–¥–µ LunarLander-v3.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã —Å PPO –∞–≥–µ–Ω—Ç–æ–º:
- –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –∫–æ–ª–±—ç–∫–∞–º–∏
- –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –º–µ—Ç—Ä–∏–∫
- –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python examples/ppo_agent_usage.py
"""

import logging
import time
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback

from src.agents.ppo_agent import PPOAgent, PPOConfig
from src.utils import configure_default_logging, set_seed


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
configure_default_logging(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProgressCallback(BaseCallback):
    """–ö–æ–ª–±—ç–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, log_freq: int = 10000, verbose: int = 1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.start_time = time.time()
        
    def _on_step(self) -> bool:
        if self.n_calls % self.log_freq == 0:
            elapsed_time = time.time() - self.start_time
            progress = self.n_calls / self.locals.get("total_timesteps", 1)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–∞–≥—Ä–∞–¥
            recent_rewards = []
            if len(self.model.ep_info_buffer) > 0:
                recent_rewards = [ep["r"] for ep in self.model.ep_info_buffer[-10:]]
            
            if recent_rewards:
                mean_reward = np.mean(recent_rewards)
                logger.info(
                    f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1%} | "
                    f"–®–∞–≥–∏: {self.n_calls:,} | "
                    f"–í—Ä–µ–º—è: {elapsed_time:.1f}—Å | "
                    f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (10 —ç–ø.): {mean_reward:.2f}"
                )
            else:
                logger.info(
                    f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1%} | "
                    f"–®–∞–≥–∏: {self.n_calls:,} | "
                    f"–í—Ä–µ–º—è: {elapsed_time:.1f}—Å"
                )
        
        return True


def create_optimized_config() -> PPOConfig:
    """–°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è LunarLander-v3.
    
    Returns:
        –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PPO —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    config = PPOConfig(
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        env_name="LunarLander-v3",
        total_timesteps=500_000,
        seed=42,
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LunarLander
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.999,
        gae_lambda=0.98,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        
        # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ learning rate
        use_lr_schedule=True,
        lr_schedule_type="linear",
        lr_final_ratio=0.1,
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏
        net_arch=[dict(pi=[64, 64], vf=[64, 64])],
        activation_fn="tanh",
        ortho_init=True,
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã
        normalize_env=True,
        norm_obs=True,
        norm_reward=True,
        clip_obs=10.0,
        clip_reward=10.0,
        
        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        early_stopping=True,
        target_reward=200.0,
        patience_episodes=100,
        min_improvement=5.0,
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        eval_freq=25_000,
        n_eval_episodes=10,
        save_freq=100_000,
        log_interval=1,
        verbose=1,
        
        # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        model_save_path="results/models/ppo_lunar_lander.zip",
        tensorboard_log="results/logs/ppo_tensorboard/",
        use_tensorboard=True,
    )
    
    return config


def train_ppo_agent() -> PPOAgent:
    """–û–±—É—á–∏—Ç—å PPO –∞–≥–µ–Ω—Ç–∞ –≤ —Å—Ä–µ–¥–µ LunarLander-v3.
    
    Returns:
        –û–±—É—á–µ–Ω–Ω—ã–π PPO –∞–≥–µ–Ω—Ç
    """
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è PPO –∞–≥–µ–Ω—Ç–∞ –≤ LunarLander-v3")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = create_optimized_config()
    logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {config.total_timesteps:,} —à–∞–≥–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    model_path = Path(config.model_save_path)
    model_path.parent.mkdir(parents=True, exist_ok=True)
    
    if config.tensorboard_log:
        tb_path = Path(config.tensorboard_log)
        tb_path.mkdir(parents=True, exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent = PPOAgent(
        config=config,
        experiment_name="ppo_lunar_lander_v3",
    )
    
    logger.info(f"–ê–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {agent}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∫–æ–ª–±—ç–∫–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    progress_callback = ProgressCallback(log_freq=25_000, verbose=1)
    
    # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    try:
        training_result = agent.train(callback=progress_callback)
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_result.training_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {training_result.final_mean_reward:.2f} ¬± {training_result.final_std_reward:.2f}")
        
        return agent
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        raise


def evaluate_agent(agent: PPOAgent, n_episodes: int = 20) -> Dict[str, float]:
    """–û—Ü–µ–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π PPO –∞–≥–µ–Ω—Ç
        n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
    """
    logger.info(f"üß™ –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–∞—Ö")
    
    metrics = agent.evaluate(
        n_episodes=n_episodes,
        deterministic=True,
        render=False,
    )
    
    logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    logger.info(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {metrics['mean_reward']:.2f} ¬± {metrics['std_reward']:.2f}")
    logger.info(f"  –î–∏–∞–ø–∞–∑–æ–Ω –Ω–∞–≥—Ä–∞–¥: [{metrics['min_reward']:.2f}, {metrics['max_reward']:.2f}]")
    logger.info(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {metrics['mean_length']:.1f} ¬± {metrics['std_length']:.1f}")
    
    # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    success_rate = sum(1 for _ in range(n_episodes) if metrics['mean_reward'] >= 200) / n_episodes * 100
    logger.info(f"  –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –ø–æ—Å–∞–¥–æ–∫: {success_rate:.1f}%")
    
    return metrics


def demonstrate_agent_usage(agent: PPOAgent, n_episodes: int = 3) -> List[float]:
    """–ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É –∞–≥–µ–Ω—Ç–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π PPO –∞–≥–µ–Ω—Ç
        n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–≥—Ä–∞–¥ –∑–∞ —ç–ø–∏–∑–æ–¥—ã
    """
    logger.info(f"üéÆ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ ({n_episodes} —ç–ø–∏–∑–æ–¥–æ–≤)")
    
    episode_rewards = []
    
    for episode in range(n_episodes):
        obs, _ = agent.env.reset()
        episode_reward = 0.0
        step_count = 0
        done = False
        
        logger.info(f"–≠–ø–∏–∑–æ–¥ {episode + 1}:")
        
        while not done:
            action, _ = agent.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = agent.env.step(action)
            
            episode_reward += reward
            step_count += 1
            done = terminated or truncated
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
            if step_count % 100 == 0:
                logger.debug(f"  –®–∞–≥ {step_count}: –Ω–∞–≥—Ä–∞–¥–∞ = {reward:.3f}, –æ–±—â–∞—è = {episode_reward:.2f}")
        
        episode_rewards.append(episode_reward)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —ç–ø–∏–∑–æ–¥–∞
        if episode_reward >= 200:
            result = "üéØ –£—Å–ø–µ—à–Ω–∞—è –ø–æ—Å–∞–¥–∫–∞!"
        elif episode_reward >= 0:
            result = "üõ¨ –ü–æ—Å–∞–¥–∫–∞ —Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏"
        else:
            result = "üí• –ö—Ä—É—à–µ–Ω–∏–µ"
        
        logger.info(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        logger.info(f"  –ù–∞–≥—Ä–∞–¥–∞: {episode_reward:.2f}, –®–∞–≥–æ–≤: {step_count}")
    
    mean_reward = np.mean(episode_rewards)
    logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é: {mean_reward:.2f}")
    
    return episode_rewards


def save_and_load_demo(agent: PPOAgent, save_path: str) -> PPOAgent:
    """–ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç
    """
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤ {save_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    agent.save(save_path)
    logger.info("‚úÖ –ê–≥–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∞–≥–µ–Ω—Ç–∞ –∏–∑ {save_path}")
    loaded_agent = PPOAgent.load(save_path)
    logger.info("‚úÖ –ê–≥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç
    test_metrics = loaded_agent.evaluate(n_episodes=3, deterministic=True)
    logger.info(f"–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞: —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ = {test_metrics['mean_reward']:.2f}")
    
    return loaded_agent


def plot_training_progress(agent: PPOAgent) -> None:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
    """
    if agent.training_result is None:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
        return
    
    logger.info("üìà –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç—Ä–µ–∫–µ—Ä–∞ –º–µ—Ç—Ä–∏–∫
    metrics_data = agent.metrics_tracker.get_summary()
    
    if not metrics_data.metrics:
        logger.warning("–ù–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle("–ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è PPO –∞–≥–µ–Ω—Ç–∞ –≤ LunarLander-v3", fontsize=14)
    
    # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥
    reward_metrics = [m for m in metrics_data.metrics if m.name == "mean_reward"]
    if reward_metrics:
        steps = [m.step for m in reward_metrics]
        rewards = [m.value for m in reward_metrics]
        
        axes[0, 0].plot(steps, rewards, 'b-', alpha=0.7)
        axes[0, 0].set_title("–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞")
        axes[0, 0].set_xlabel("–®–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
        axes[0, 0].set_ylabel("–ù–∞–≥—Ä–∞–¥–∞")
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axhline(y=200, color='r', linestyle='--', alpha=0.7, label='–¶–µ–ª—å (200)')
        axes[0, 0].legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —ç–ø–∏–∑–æ–¥–æ–≤
    length_metrics = [m for m in metrics_data.metrics if m.name == "mean_length"]
    if length_metrics:
        steps = [m.step for m in length_metrics]
        lengths = [m.value for m in length_metrics]
        
        axes[0, 1].plot(steps, lengths, 'g-', alpha=0.7)
        axes[0, 1].set_title("–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞")
        axes[0, 1].set_xlabel("–®–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
        axes[0, 1].set_ylabel("–®–∞–≥–∏")
        axes[0, 1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –ø–æ–ª–∏—Ç–∏–∫–∏
    policy_loss_metrics = [m for m in metrics_data.metrics if m.name == "policy_loss"]
    if policy_loss_metrics:
        steps = [m.step for m in policy_loss_metrics]
        losses = [m.value for m in policy_loss_metrics]
        
        axes[1, 0].plot(steps, losses, 'r-', alpha=0.7)
        axes[1, 0].set_title("–ü–æ—Ç–µ—Ä–∏ –ø–æ–ª–∏—Ç–∏–∫–∏")
        axes[1, 0].set_xlabel("–®–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
        axes[1, 0].set_ylabel("–ü–æ—Ç–µ—Ä–∏")
        axes[1, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
    value_loss_metrics = [m for m in metrics_data.metrics if m.name == "value_loss"]
    if value_loss_metrics:
        steps = [m.step for m in value_loss_metrics]
        losses = [m.value for m in value_loss_metrics]
        
        axes[1, 1].plot(steps, losses, 'orange', alpha=0.7)
        axes[1, 1].set_title("–ü–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏")
        axes[1, 1].set_xlabel("–®–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
        axes[1, 1].set_ylabel("–ü–æ—Ç–µ—Ä–∏")
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plots_dir = Path("results/plots")
    plots_dir.mkdir(parents=True, exist_ok=True)
    plot_path = plots_dir / "ppo_training_progress.png"
    
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")
    
    plt.show()


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ PPO –∞–≥–µ–Ω—Ç–∞."""
    logger.info("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è PPO –∞–≥–µ–Ω—Ç–∞ –¥–ª—è LunarLander-v3")
    logger.info("=" * 60)
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    set_seed(42)
    
    try:
        # 1. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        agent = train_ppo_agent()
        
        # 2. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        evaluation_metrics = evaluate_agent(agent, n_episodes=20)
        
        # 3. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
        demo_rewards = demonstrate_agent_usage(agent, n_episodes=3)
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
        save_path = "results/models/ppo_demo_model.zip"
        save_and_load_demo(agent, save_path)
        
        # 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        plot_training_progress(agent)
        
        # 6. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        logger.info("=" * 60)
        logger.info("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        logger.info("=" * 60)
        logger.info(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {agent.training_result.training_time:.2f} —Å–µ–∫")
        logger.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {evaluation_metrics['mean_reward']:.2f} ¬± {evaluation_metrics['std_reward']:.2f}")
        logger.info(f"–õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {evaluation_metrics['max_reward']:.2f}")
        logger.info(f"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã: {[f'{r:.1f}' for r in demo_rewards]}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        if evaluation_metrics['mean_reward'] >= 200:
            logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û! –ê–≥–µ–Ω—Ç –Ω–∞—É—á–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–∑–µ–º–ª—è—Ç—å—Å—è.")
        elif evaluation_metrics['mean_reward'] >= 0:
            logger.info("‚ö†Ô∏è  –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö. –ê–≥–µ–Ω—Ç –ø—Ä–∏–∑–µ–º–ª—è–µ—Ç—Å—è, –Ω–æ –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ.")
        else:
            logger.info("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.")
        
        logger.info("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        raise


if __name__ == "__main__":
    main()