"""–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞.

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è TrainingLoop –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
RL –∞–≥–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.
"""

import logging
import time
from pathlib import Path

import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
from src.training.train_loop import (
    TrainingLoop,
    TrainingStrategy,
    TrainingProgress,
    TrainingStatistics,
    LoggingHook,
    EarlyStoppingHook,
    create_training_loop,
)
from src.agents import PPOAgent, AgentConfig
from src.environments import LunarLanderEnvironment


def create_simple_agent_and_env():
    """–°–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏ —Å—Ä–µ–¥—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    config = AgentConfig(
        algorithm="PPO",
        env_name="LunarLander-v3",
        total_timesteps=10_000,
        learning_rate=3e-4,
        batch_size=64,
        n_steps=2048,
        verbose=1,
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
    env = LunarLanderEnvironment()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    agent = PPOAgent(config=config, env=env, experiment_name="train_loop_demo")
    
    return agent, env


def example_basic_training():
    """–ü—Ä–∏–º–µ—Ä –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º —Ü–∏–∫–ª–æ–º."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ ===")
    
    agent, env = create_simple_agent_and_env()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.TIMESTEP_BASED,
        total_timesteps=5_000,
        eval_freq=1_000,
        checkpoint_freq=2_000,
        save_freq=2_500,
        progress_update_interval=2.0,
        experiment_name="basic_training_demo",
    )
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –û–±—â–µ–µ –≤—Ä–µ–º—è: {statistics.total_training_time:.1f} —Å–µ–∫")
        print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {statistics.mean_episode_reward:.2f}")
        print(f"üìà –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {statistics.best_episode_reward:.2f}")
        print(f"üèÉ –°–∫–æ—Ä–æ—Å—Ç—å: {statistics.average_steps_per_second:.1f} —à–∞–≥–æ–≤/—Å–µ–∫")
        
    except KeyboardInterrupt:
        print("‚ùå –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    finally:
        env.close()


def example_episodic_training():
    """–ü—Ä–∏–º–µ—Ä —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 2: –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ ===")
    
    agent, env = create_simple_agent_and_env()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Å —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.EPISODIC,
        total_timesteps=8_000,
        max_episodes=50,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
        eval_freq=0,  # –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        checkpoint_freq=0,
        save_freq=0,
        progress_update_interval=3.0,
        experiment_name="episodic_training_demo",
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging_hook = LoggingHook(log_interval=500)
    training_loop.add_hook(logging_hook)
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –≠–ø–∏–∑–æ–¥–æ–≤: {statistics.total_episodes_completed}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {statistics.total_training_time:.1f} —Å–µ–∫")
        print(f"üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {statistics.mean_episode_length:.1f}")
        print(f"üéØ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {statistics.mean_episode_reward:.2f}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def example_training_with_early_stopping():
    """–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞–Ω–Ω–∏–º –æ—Å—Ç–∞–Ω–æ–≤–æ–º."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 3: –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–∏–º –æ—Å—Ç–∞–Ω–æ–≤–æ–º ===")
    
    agent, env = create_simple_agent_and_env()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.TIMESTEP_BASED,
        total_timesteps=15_000,
        eval_freq=1_500,
        convergence_threshold=200.0,  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        early_stopping_patience=5,
        progress_update_interval=2.0,
        experiment_name="early_stopping_demo",
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫ —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞
    early_stopping_hook = EarlyStoppingHook(
        patience=3,
        min_improvement=5.0,
        metric_name="mean_episode_reward"
    )
    training_loop.add_hook(early_stopping_hook)
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–∏–º –æ—Å—Ç–∞–Ω–æ–≤–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {statistics.mean_episode_reward:.2f}")
        
        if statistics.convergence_timestep:
            print(f"üéâ –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –Ω–∞ —à–∞–≥–µ {statistics.convergence_timestep}")
        else:
            print("‚ö†Ô∏è  –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def example_adaptive_training():
    """–ü—Ä–∏–º–µ—Ä –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 4: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ===")
    
    agent, env = create_simple_agent_and_env()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.ADAPTIVE,
        total_timesteps=10_000,
        eval_freq=2_000,
        memory_limit_mb=500.0,  # –õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏
        progress_update_interval=1.5,
        experiment_name="adaptive_training_demo",
    )
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ö—É–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    class AdaptiveMonitoringHook:
        def __init__(self):
            self.strategy_switches = 0
            self.last_strategy = None
        
        def on_training_start(self, progress):
            print("üöÄ –ù–∞—á–∞–ª–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        
        def on_episode_start(self, progress):
            pass
        
        def on_step(self, progress, step_info):
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if progress.current_timestep % 1000 == 0:
                print(f"üìä –®–∞–≥ {progress.current_timestep}: "
                      f"{progress.steps_per_second:.1f} —à–∞–≥–æ–≤/—Å–µ–∫, "
                      f"–ø–∞–º—è—Ç—å: {progress.memory_usage_mb:.1f}MB")
        
        def on_episode_end(self, progress, episode_info):
            pass
        
        def on_training_end(self, progress, statistics):
            print(f"üèÅ –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {self.strategy_switches}")
    
    adaptive_hook = AdaptiveMonitoringHook()
    training_loop.add_hook(adaptive_hook)
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üíæ –ü–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {statistics.peak_memory_usage_mb:.1f}MB")
        print(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {statistics.average_steps_per_second:.1f} —à–∞–≥–æ–≤/—Å–µ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def example_training_with_config():
    """–ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 5: –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===")
    
    agent, env = create_simple_agent_and_env()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    training_config = {
        "strategy": "mixed",
        "total_timesteps": 6_000,
        "max_episodes": 30,
        "eval_freq": 1_500,
        "checkpoint_freq": 3_000,
        "save_freq": 0,
        "progress_update_interval": 2.5,
        "memory_limit_mb": 300.0,
        "convergence_threshold": 150.0,
        "early_stopping_patience": 4,
        "tensorboard_log_dir": "results/tensorboard/config_demo",
        "enable_logging_hook": True,
        "enable_early_stopping": True,
        "log_interval": 750,
        "min_improvement": 3.0,
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    training_loop = create_training_loop(
        agent=agent,
        env=env,
        config=training_config,
        experiment_name="config_based_training",
    )
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  - –®–∞–≥–æ–≤: {statistics.total_timesteps_completed}")
        print(f"  - –≠–ø–∏–∑–æ–¥–æ–≤: {statistics.total_episodes_completed}")
        print(f"  - –í—Ä–µ–º—è: {statistics.total_training_time:.1f} —Å–µ–∫")
        print(f"  - –ù–∞–≥—Ä–∞–¥–∞: {statistics.mean_episode_reward:.2f} ¬± {statistics.std_episode_reward:.2f}")
        print(f"  - –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {statistics.best_episode_reward:.2f}")
        print(f"  - –ß–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {statistics.num_checkpoints_saved}")
        print(f"  - –û—Ü–µ–Ω–æ–∫: {statistics.num_evaluations_performed}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def example_pause_resume_training():
    """–ü—Ä–∏–º–µ—Ä –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 6: –ü—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ ===")
    
    agent, env = create_simple_agent_and_env()
    
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.TIMESTEP_BASED,
        total_timesteps=8_000,
        eval_freq=0,
        checkpoint_freq=0,
        save_freq=0,
        progress_update_interval=1.0,
        experiment_name="pause_resume_demo",
    )
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ö—É–∫ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–∞—É–∑—ã
    class PauseResumeHook:
        def __init__(self, training_loop):
            self.training_loop = training_loop
            self.paused = False
        
        def on_training_start(self, progress):
            print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–∞—É–∑—ã")
        
        def on_episode_start(self, progress):
            pass
        
        def on_step(self, progress, step_info):
            # –ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞ 2000 —à–∞–≥–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            if progress.current_timestep == 2000 and not self.paused:
                print("‚è∏Ô∏è  –ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 2 —Å–µ–∫—É–Ω–¥—ã...")
                self.training_loop.pause()
                self.paused = True
                
                # –ò–º–∏—Ç–∏—Ä—É–µ–º –ø–∞—É–∑—É
                import threading
                def resume_after_delay():
                    time.sleep(2)
                    print("‚ñ∂Ô∏è  –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ")
                    self.training_loop.resume()
                
                threading.Thread(target=resume_after_delay).start()
        
        def on_episode_end(self, progress, episode_info):
            pass
        
        def on_training_end(self, progress, statistics):
            print("üèÅ –û–±—É—á–µ–Ω–∏–µ —Å –ø–∞—É–∑–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    
    pause_hook = PauseResumeHook(training_loop)
    training_loop.add_hook(pause_hook)
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ —Å –ø–∞—É–∑–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {statistics.total_training_time:.1f} —Å–µ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def example_resource_monitoring():
    """–ü—Ä–∏–º–µ—Ä –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
    print("\n=== –ü—Ä–∏–º–µ—Ä 7: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ ===")
    
    agent, env = create_simple_agent_and_env()
    
    training_loop = TrainingLoop(
        agent=agent,
        env=env,
        strategy=TrainingStrategy.TIMESTEP_BASED,
        total_timesteps=5_000,
        eval_freq=0,
        checkpoint_freq=0,
        save_freq=0,
        memory_limit_mb=200.0,  # –ù–∏–∑–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        progress_update_interval=1.0,
        experiment_name="resource_monitoring_demo",
    )
    
    # –•—É–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
    class ResourceMonitoringHook:
        def __init__(self):
            self.max_memory = 0.0
            self.max_cpu = 0.0
        
        def on_training_start(self, progress):
            print("üìä –ù–∞—á–∞–ª–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤")
        
        def on_episode_start(self, progress):
            pass
        
        def on_step(self, progress, step_info):
            self.max_memory = max(self.max_memory, progress.memory_usage_mb)
            self.max_cpu = max(self.max_cpu, progress.cpu_usage_percent)
            
            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
            if progress.current_timestep % 1000 == 0:
                print(f"üíæ –ü–∞–º—è—Ç—å: {progress.memory_usage_mb:.1f}MB "
                      f"(–º–∞–∫—Å: {self.max_memory:.1f}MB)")
                print(f"üñ•Ô∏è  CPU: {progress.cpu_usage_percent:.1f}% "
                      f"(–º–∞–∫—Å: {self.max_cpu:.1f}%)")
                if progress.gpu_memory_mb > 0:
                    print(f"üéÆ GPU –ø–∞–º—è—Ç—å: {progress.gpu_memory_mb:.1f}MB")
        
        def on_episode_end(self, progress, episode_info):
            pass
        
        def on_training_end(self, progress, statistics):
            print(f"üìà –ü–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
            print(f"  - –ü–∞–º—è—Ç—å: {self.max_memory:.1f}MB")
            print(f"  - CPU: {self.max_cpu:.1f}%")
    
    resource_hook = ResourceMonitoringHook()
    training_loop.add_hook(resource_hook)
    
    try:
        statistics = training_loop.run()
        
        print(f"‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üíæ –ü–∏–∫–æ–≤–∞—è –ø–∞–º—è—Ç—å: {statistics.peak_memory_usage_mb:.1f}MB")
        print(f"üñ•Ô∏è  –°—Ä–µ–¥–Ω–∏–π CPU: {statistics.average_cpu_usage:.1f}%")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        env.close()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤."""
    print("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π TrainingLoop")
    print("=" * 50)
    
    examples = [
        ("–ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ", example_basic_training),
        ("–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", example_episodic_training),
        ("–†–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤", example_training_with_early_stopping),
        ("–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", example_adaptive_training),
        ("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", example_training_with_config),
        ("–ü–∞—É–∑–∞ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ", example_pause_resume_training),
        ("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤", example_resource_monitoring),
    ]
    
    for i, (name, example_func) in enumerate(examples, 1):
        try:
            print(f"\nüîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ {i}: {name}")
            example_func()
            print(f"‚úÖ –ü—Ä–∏–º–µ—Ä {i} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except KeyboardInterrupt:
            print(f"\n‚èπÔ∏è  –ü—Ä–∏–º–µ—Ä {i} –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ {i}: {e}")
            continue
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—Ä–∏–º–µ—Ä–∞–º–∏
        if i < len(examples):
            print("\n‚è≥ –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∏–º–µ—Ä–æ–º...")
            time.sleep(2)
    
    print("\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    main()