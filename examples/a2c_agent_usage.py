"""–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è A2C –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Å—Ä–µ–¥–µ LunarLander-v3.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
1. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫—É A2C –∞–≥–µ–Ω—Ç–∞
2. –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –º–µ—Ç—Ä–∏–∫
3. –û—Ü–µ–Ω–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏
5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å PPO –∞–≥–µ–Ω—Ç–æ–º
"""

import logging
import time
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from src.agents.a2c_agent import A2CAgent, A2CConfig
from src.agents.ppo_agent import PPOAgent, PPOConfig
from src.utils import set_seed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")


def create_a2c_config() -> A2CConfig:
    """–°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é A2C –¥–ª—è LunarLander-v3.
    
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A2C –∞–≥–µ–Ω—Ç–∞
    """
    return A2CConfig(
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        env_name="LunarLander-v3",
        total_timesteps=200_000,
        seed=42,
        
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã A2C
        learning_rate=7e-4,
        n_steps=5,
        gamma=0.99,
        gae_lambda=1.0,
        ent_coef=0.01,
        vf_coef=0.25,
        max_grad_norm=0.5,
        use_rms_prop=True,
        rms_prop_eps=1e-5,
        
        # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ learning rate
        use_lr_schedule=True,
        lr_schedule_type="linear",
        lr_final_ratio=0.1,
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏
        net_arch=[dict(pi=[64, 64], vf=[64, 64])],
        activation_fn="tanh",
        ortho_init=True,
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        normalize_env=True,
        norm_obs=True,
        norm_reward=True,
        clip_obs=10.0,
        clip_reward=10.0,
        
        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        early_stopping=True,
        target_reward=200.0,
        patience_episodes=100,
        min_improvement=2.0,
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        eval_freq=10_000,
        n_eval_episodes=10,
        save_freq=50_000,
        log_interval=1,
        use_tensorboard=True,
        
        # –ü—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        model_save_path="results/models/a2c_lunar_lander.zip",
        tensorboard_log="results/logs/a2c_tensorboard/",
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        verbose=1,
        device="cpu",
    )


def train_a2c_agent(config: A2CConfig) -> A2CAgent:
    """–û–±—É—á–∏—Ç—å A2C –∞–≥–µ–Ω—Ç–∞.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
        
    Returns:
        –û–±—É—á–µ–Ω–Ω—ã–π A2C –∞–≥–µ–Ω—Ç
    """
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ A2C –∞–≥–µ–Ω—Ç–∞...")
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    set_seed(config.seed)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    agent = A2CAgent(
        config=config,
        experiment_name="a2c_lunar_lander_experiment",
    )
    
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è A2C –∞–≥–µ–Ω—Ç–∞...")
    start_time = time.time()
    
    # –û–±—É—á–µ–Ω–∏–µ
    training_result = agent.train()
    
    training_time = time.time() - start_time
    
    logger.info(
        f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫",
        extra={
            "final_mean_reward": training_result.final_mean_reward,
            "final_std_reward": training_result.final_std_reward,
            "total_timesteps": training_result.total_timesteps,
            "success": training_result.success,
        },
    )
    
    return agent


def evaluate_agent(agent: A2CAgent, n_episodes: int = 20) -> Dict[str, float]:
    """–û—Ü–µ–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç
        n_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
    """
    logger.info(f"–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–∞—Ö...")
    
    metrics = agent.evaluate(
        n_episodes=n_episodes,
        deterministic=True,
        render=False,
    )
    
    logger.info(
        "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:",
        extra=metrics,
    )
    
    return metrics


def compare_with_ppo(a2c_config: A2CConfig) -> Dict[str, Dict[str, float]]:
    """–°—Ä–∞–≤–Ω–∏—Ç—å A2C —Å PPO –∞–≥–µ–Ω—Ç–æ–º.
    
    Args:
        a2c_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A2C –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π PPO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    logger.info("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ A2C —Å PPO...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ PPO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ A2C
    ppo_config = PPOConfig(
        env_name=a2c_config.env_name,
        total_timesteps=a2c_config.total_timesteps,
        seed=a2c_config.seed,
        learning_rate=a2c_config.learning_rate,
        gamma=a2c_config.gamma,
        ent_coef=a2c_config.ent_coef,
        vf_coef=a2c_config.vf_coef,
        max_grad_norm=a2c_config.max_grad_norm,
        normalize_env=a2c_config.normalize_env,
        early_stopping=a2c_config.early_stopping,
        target_reward=a2c_config.target_reward,
        model_save_path="results/models/ppo_lunar_lander_comparison.zip",
        tensorboard_log="results/logs/ppo_comparison_tensorboard/",
        verbose=0,  # –ú–µ–Ω—å—à–µ –ª–æ–≥–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    )
    
    # –û–±—É—á–µ–Ω–∏–µ A2C
    logger.info("–û–±—É—á–µ–Ω–∏–µ A2C –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
    a2c_agent = A2CAgent(config=a2c_config, experiment_name="a2c_comparison")
    a2c_result = a2c_agent.train()
    a2c_metrics = a2c_agent.evaluate(n_episodes=20, deterministic=True)
    
    # –û–±—É—á–µ–Ω–∏–µ PPO
    logger.info("–û–±—É—á–µ–Ω–∏–µ PPO –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
    ppo_agent = PPOAgent(config=ppo_config, experiment_name="ppo_comparison")
    ppo_result = ppo_agent.train()
    ppo_metrics = ppo_agent.evaluate(n_episodes=20, deterministic=True)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    comparison = {
        "A2C": {
            "mean_reward": a2c_metrics["mean_reward"],
            "std_reward": a2c_metrics["std_reward"],
            "training_time": a2c_result.training_time,
            "total_timesteps": a2c_result.total_timesteps,
        },
        "PPO": {
            "mean_reward": ppo_metrics["mean_reward"],
            "std_reward": ppo_metrics["std_reward"],
            "training_time": ppo_result.training_time,
            "total_timesteps": ppo_result.total_timesteps,
        },
    }
    
    logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
    for algorithm, metrics in comparison.items():
        logger.info(f"{algorithm}: {metrics}")
    
    return comparison


def visualize_results(
    agent: A2CAgent,
    comparison_results: Dict[str, Dict[str, float]],
    save_path: str = "results/plots/",
) -> None:
    """–°–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç
        comparison_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å PPO
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    """
    save_dir = Path(save_path)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    algorithms = list(comparison_results.keys())
    mean_rewards = [comparison_results[alg]["mean_reward"] for alg in algorithms]
    std_rewards = [comparison_results[alg]["std_reward"] for alg in algorithms]
    training_times = [comparison_results[alg]["training_time"] for alg in algorithms]
    
    # –°—Ä–µ–¥–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
    bars1 = ax1.bar(algorithms, mean_rewards, yerr=std_rewards, capsize=5)
    ax1.set_title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –Ω–∞–≥—Ä–∞–¥", fontsize=14, fontweight="bold")
    ax1.set_ylabel("–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞")
    ax1.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, reward in zip(bars1, mean_rewards):
        height = bar.get_height()
        ax1.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{reward:.1f}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )
    
    # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    bars2 = ax2.bar(algorithms, training_times)
    ax2.set_title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è", fontsize=14, fontweight="bold")
    ax2.set_ylabel("–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)")
    ax2.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, time_val in zip(bars2, training_times):
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{time_val:.1f}s",
            ha="center",
            va="bottom",
            fontweight="bold",
        )
    
    plt.tight_layout()
    plt.savefig(save_dir / "a2c_comparison.png", dpi=300, bbox_inches="tight")
    plt.show()
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ A2C
    model_info = agent.get_model_info()
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    info_text = f"""
    A2C Agent Information
    =====================
    
    Environment: {model_info.get('env_name', 'N/A')}
    Algorithm: {model_info.get('algorithm', 'N/A')}
    Total Timesteps: {model_info.get('total_timesteps', 'N/A'):,}
    
    Hyperparameters:
    ----------------
    Learning Rate: {agent.config.learning_rate}
    N Steps: {model_info.get('n_steps', 'N/A')}
    Entropy Coefficient: {model_info.get('ent_coef', 'N/A')}
    Value Function Coefficient: {model_info.get('vf_coef', 'N/A')}
    RMSProp Epsilon: {agent.config.rms_prop_eps}
    Use RMSProp: {model_info.get('use_rms_prop', 'N/A')}
    
    Training Results:
    ----------------
    Final Mean Reward: {model_info.get('final_mean_reward', 'N/A'):.2f}
    Training Time: {model_info.get('training_time', 'N/A'):.2f} sec
    Best Mean Reward: {model_info.get('best_mean_reward', 'N/A'):.2f}
    
    Environment Normalization: {model_info.get('normalize_env', 'N/A')}
    Early Stopping: {model_info.get('early_stopping', 'N/A')}
    Learning Rate Schedule: {model_info.get('use_lr_schedule', 'N/A')}
    """
    
    ax.text(0.05, 0.95, info_text, transform=ax.transAxes, fontsize=11,
            verticalalignment="top", fontfamily="monospace",
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis("off")
    ax.set_title("A2C Agent Model Information", fontsize=16, fontweight="bold", pad=20)
    
    plt.tight_layout()
    plt.savefig(save_dir / "a2c_model_info.png", dpi=300, bbox_inches="tight")
    plt.show()
    
    logger.info(f"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_dir}")


def demonstrate_save_load(agent: A2CAgent, config: A2CConfig) -> None:
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏.
    
    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    """
    logger.info("–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_path = "results/models/a2c_demo_save.zip"
    agent.save(save_path)
    logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    loaded_agent = A2CAgent.load(save_path, config=config)
    logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    original_metrics = agent.evaluate(n_episodes=5, deterministic=True)
    loaded_metrics = loaded_agent.evaluate(n_episodes=5, deterministic=True)
    
    logger.info("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    logger.info(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è: {original_metrics['mean_reward']:.2f} ¬± {original_metrics['std_reward']:.2f}")
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è: {loaded_metrics['mean_reward']:.2f} ¬± {loaded_metrics['std_reward']:.2f}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
    diff = abs(original_metrics["mean_reward"] - loaded_metrics["mean_reward"])
    if diff < 1.0:  # –ù–µ–±–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –¥–æ–ø—É—Å—Ç–∏–º–∞ –∏–∑-–∑–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥—ã
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        logger.warning(f"‚ö†Ô∏è –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {diff:.2f}")


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ A2C –∞–≥–µ–Ω—Ç–∞."""
    logger.info("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è A2C –∞–≥–µ–Ω—Ç–∞ –¥–ª—è LunarLander-v3")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Path("results/models").mkdir(parents=True, exist_ok=True)
    Path("results/logs").mkdir(parents=True, exist_ok=True)
    Path("results/plots").mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        logger.info("üìã –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ A2C...")
        config = create_a2c_config()
        
        # 2. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        logger.info("üéì –û–±—É—á–µ–Ω–∏–µ A2C –∞–≥–µ–Ω—Ç–∞...")
        agent = train_a2c_agent(config)
        
        # 3. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        logger.info("üìä –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        evaluation_metrics = evaluate_agent(agent, n_episodes=20)
        
        # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å PPO (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ä–µ–º—è)
        logger.info("‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å PPO –∞–≥–µ–Ω—Ç–æ–º...")
        comparison_config = A2CConfig(
            env_name="LunarLander-v3",
            total_timesteps=50_000,  # –ú–µ–Ω—å—à–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            seed=42,
            verbose=0,
            model_save_path="results/models/a2c_comparison.zip",
            tensorboard_log="results/logs/a2c_comparison_tensorboard/",
        )
        comparison_results = compare_with_ppo(comparison_config)
        
        # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
        visualize_results(agent, comparison_results)
        
        # 6. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏
        logger.info("üíæ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏...")
        demonstrate_save_load(agent, config)
        
        # 7. –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        logger.info("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'results/'")
        logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {evaluation_metrics['mean_reward']:.2f} ¬± {evaluation_metrics['std_reward']:.2f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if evaluation_metrics["mean_reward"] >= 200:
            logger.info("üèÜ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Ä–µ—à–∏–ª –∑–∞–¥–∞—á—É.")
        elif evaluation_metrics["mean_reward"] >= 100:
            logger.info("üëç –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ê–≥–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.")
        else:
            logger.info("üìö –ê–≥–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        raise


if __name__ == "__main__":
    main()