# Трек 1. Обучение агента в классической среде

## Цель
Продемонстрировать умение применить готовый RL-алгоритм (например, `stable-baselines3`) для решения стандартной задачи, провести контролируемые эксперименты и проанализировать их влияние на обучение.

---

## Что делать

### Выбор среды
Выберите одну из следующих сред (или другую из Gymnasium, но только если она обучается до сходимости за ≤30 минут на CPU):
- `LunarLander-v2`
- `MountainCarContinuous-v0`
- `Acrobot-v1`
- `Pendulum-v1`

---

### Выбор алгоритма
Обучите агента с помощью любого алгоритма (например, PPO, A2C, SAC, TD3 и т.д.).

---

### Проведение экспериментов
Проведите **два контролируемых эксперимента**, например:
- Сравнение двух алгоритмов на одной среде.
- Влияние гиперпараметра (например, исследовать, как частота обновления target-сети влияет на стабильность DQN в Acrobot).
- Изменение архитектуры нейросети.
- Reward shaping (например, добавление штрафа за использование топлива в LunarLander).
- Использование различных стратегий исследования.

→ **Важно**: эксперименты должны быть сравнимыми и иметь чёткую гипотезу (например: *«Я ожидаю, что при увеличении gamma агент будет дольше планировать»*).

---

## Обязательные элементы отчёта

### 1. График
График средней награды vs timestep/episode.

### 2. Анимация/видео
Включите анимацию или видео финального агента.

### 3. Количественная оценка
Проведите количественную оценку итогового агента — средняя награда по 10–20 эпизодам.

### 4. Воспроизводимость
- Используйте фиксированный seed.
- Включите вывод `!pip freeze`.
- Приложите полный код обучения.

## Настройка среды

**Среда:** Python 3.10.14, управляемая через conda

**Активация:** `conda activate rocm`

**Зависимости:** Все указаны в файле `environment.yml`. Добавление новых зависимостей через `pip`. НЕ ИСПОЛЬЗОВАТЬ `conda install`.

