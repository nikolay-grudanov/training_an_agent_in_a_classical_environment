# –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤

## 1. –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ API —Å–µ—Ä–≤–µ—Ä–∞

### –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞ –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:
```bash
# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫
python -m src.api.app

# –° —É–∫–∞–∑–∞–Ω–∏–µ–º —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞
python -m src.api.app --host 0.0.0.0 --port 8000

# –í —Ä–µ–∂–∏–º–µ –æ—Ç–ª–∞–¥–∫–∏ —Å –∞–≤—Ç–æ–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π
python -m src.api.app --host 0.0.0.0 --port 8000 --debug --reload

# –ß–µ—Ä–µ–∑ uvicorn –Ω–∞–ø—Ä—è–º—É—é
uvicorn src.api.app:create_app --host 0.0.0.0 --port 8000 --reload
```

### –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ:
```bash
# –° –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ workers
python -m src.api.app --host 0.0.0.0 --port 8000 --workers 4

# –ò–ª–∏ —á–µ—Ä–µ–∑ gunicorn
pip install gunicorn
gunicorn src.api.app:create_app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API:
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
curl http://localhost:8000/health

# –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ API
# Swagger UI: http://localhost:8000/docs
# ReDoc: http://localhost:8000/redoc

# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
curl -X POST "http://localhost:8000/experiments" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "ppo_lunarlander_test",
    "algorithm": "PPO",
    "environment": "LunarLander-v3",
    "hyperparameters": {
      "learning_rate": 0.0003,
      "n_steps": 2048,
      "batch_size": 64
    },
    "seed": 42,
    "description": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PPO –Ω–∞ LunarLander",
    "hypothesis": "PPO –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ LunarLander"
  }'
```

## 2. –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ (PPO –Ω–∞ LunarLander-v3)

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CLI:
```bash
# –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ PPO –Ω–∞ LunarLander-v3
python -m src.training.cli train --algorithm PPO --env LunarLander-v3 --timesteps 100000 --seed 42 --experiment ppo_lunarlander_exp

# –° –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
python -m src.training.cli train \
  --algorithm PPO \
  --env LunarLander-v3 \
  --timesteps 150000 \
  --seed 42 \
  --experiment ppo_lunarlander_advanced \
  --output results/ppo_lunarlander \
  --eval-freq 10000 \
  --save-freq 25000 \
  --early-stopping \
  --patience 5 \
  --verbose 2
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Python API:
```python
from src.training import Trainer, TrainerConfig

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = TrainerConfig(
    experiment_name="ppo_lunarlander_python",
    algorithm="PPO",
    environment_name="LunarLander-v3",
    total_timesteps=100000,
    seed=42,
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ü–µ–Ω–∫–∏
    eval_freq=10000,
    n_eval_episodes=5,
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    save_freq=25000,
    checkpoint_freq=20000,
    
    # –ü—É—Ç–∏
    output_dir="results/python_examples",
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    verbose=1,
    progress_bar=True,
)

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–µ—Ä–∞
with Trainer(config) as trainer:
    result = trainer.train()
    
    if result.success:
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}")
        print(f"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.best_mean_reward:.2f}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.1f} —Å–µ–∫")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {result.model_path}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        eval_result = trainer.evaluate(n_episodes=10, render=False)
        print(f"üìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {eval_result['mean_reward']:.2f}")
        print(f"üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {eval_result['mean_length']:.1f}")
    else:
        print(f"‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.error_message}")
```

## 3. –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ExperimentRunner:
```python
from src.experiments.config import Configuration
from src.experiments.experiment import Experiment
from src.experiments.runner import ExperimentRunner, ExecutionMode
from src.utils.config import RLConfig, AlgorithmConfig, EnvironmentConfig, TrainingConfig

# Baseline –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π PPO
baseline_algorithm = AlgorithmConfig(
    name="PPO",
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
)

baseline_environment = EnvironmentConfig(
    name="LunarLander-v3",
    render_mode=None,
)

baseline_training = TrainingConfig(
    total_timesteps=50000,
    eval_freq=10000,
    n_eval_episodes=5,
    save_freq=25000,
)

baseline_config = RLConfig(
    algorithm=baseline_algorithm,
    environment=baseline_environment,
    training=baseline_training,
    seed=42,
    experiment_name="baseline_ppo",
    output_dir="results/examples",
)

# Variant –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - PPO —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º learning rate
variant_algorithm = AlgorithmConfig(
    name="PPO",
    learning_rate=1e-3,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π learning rate
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
)

variant_training = TrainingConfig(
    total_timesteps=50000,
    eval_freq=10000,
    n_eval_episodes=5,
    save_freq=25000,
)

variant_config = RLConfig(
    algorithm=variant_algorithm,
    environment=baseline_environment,  # –¢–∞ –∂–µ —Å—Ä–µ–¥–∞
    training=variant_training,
    seed=42,  # –¢–æ—Ç –∂–µ seed –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    experiment_name="variant_ppo_high_lr",
    output_dir="results/examples",
)

# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
experiment = Experiment(
    baseline_config=baseline_config,
    variant_config=variant_config,
    hypothesis="–£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π learning rate (1e-3) –¥–æ–ª–∂–µ–Ω —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º (3e-4)",
    output_dir="results/examples",
)

# –°–æ–∑–¥–∞–Ω–∏–µ runner'–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
runner = ExperimentRunner(
    experiment=experiment,
    execution_mode=ExecutionMode.SEQUENTIAL,
    enable_monitoring=True,
    checkpoint_frequency=10000,
)

# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
success = runner.run()

if success:
    print("‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if runner.baseline_result and runner.variant_result:
        baseline_reward = runner.baseline_result.final_mean_reward
        variant_reward = runner.variant_result.final_mean_reward
        improvement = variant_reward - baseline_reward
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   Baseline –Ω–∞–≥—Ä–∞–¥–∞: {baseline_reward:.2f}")
        print(f"   Variant –Ω–∞–≥—Ä–∞–¥–∞: {variant_reward:.2f}")
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.2f}")
        
        if improvement > 0:
            print("üéâ –ì–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞: variant –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!")
        else:
            print("ü§î –ì–∏–ø–æ—Ç–µ–∑–∞ –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞: baseline –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
else:
    print("‚ùå –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
```

### CLI –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:
```bash
# –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
python -m src.training.cli compare --algorithm PPO --algorithm A2C --env LunarLander-v3 --timesteps 50000 --runs 3

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
python -m src.training.cli config create

# –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
python -m src.training.cli config validate

# –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
python -m src.training.cli config list
```

## 4. –ü—Ä–∏–º–µ—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:
```python
import numpy as np
from src.visualization.plots import plot_learning_curve, plot_multiple_runs, plot_convergence_analysis, PlotConfig
from src.visualization.generate_all import VisualizationGenerator
from pathlib import Path

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –∏–∑ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è)
np.random.seed(42)
timesteps = np.arange(0, 100000, 100)
base_reward = -200
max_improvement = 400
learning_rate = 0.00002

# –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å —à—É–º–æ–º
progress = 1 - np.exp(-learning_rate * timesteps)
rewards = base_reward + max_improvement * progress + 30 * np.random.randn(len(timesteps))

# –°–æ–∑–¥–∞–Ω–∏–µ learning curve
config = PlotConfig(
    figure_size=(10, 6),
    color_palette="publication",
    line_width=2.5,
)

fig = plot_learning_curve(
    timesteps=timesteps,
    rewards=rewards,
    title="PPO Training on LunarLander-v3",
    xlabel="Training Steps",
    ylabel="Episode Reward",
    smooth=True,
    confidence_interval=True,
    save_path=Path("results/plots/learning_curve"),
    config=config,
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
generator = VisualizationGenerator(
    output_dir="results/plots",
    formats=["png", "svg"],
)

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤
runs_data = {
    "ppo_seed_42": {
        "timesteps": timesteps,
        "reward": rewards,
    },
    "ppo_seed_123": {
        "timesteps": timesteps,
        "reward": rewards + 10 * np.random.randn(len(rewards)),
    },
    "ppo_seed_456": {
        "timesteps": timesteps,
        "reward": rewards + 15 * np.random.randn(len(rewards)),
    }
}

experiment_data = {"runs": runs_data}

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
plots = generator.generate_experiment_report(
    experiment_data=experiment_data,
    experiment_name="ppo_lunarlander_experiment",
)

print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(plots)} –≥—Ä–∞—Ñ–∏–∫–æ–≤:")
for plot_type, plot_path in plots.items():
    print(f"   - {plot_type}: {plot_path}")
```

### CLI –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:
```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
python -c "
from src.visualization.generate_all import VisualizationGenerator
import sys
import os
sys.path.append(os.getcwd())

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
generator = VisualizationGenerator(output_dir='results/plots')
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
"
```

## 5. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Jupyter notebook –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

### –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞:

–°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π —Ñ–∞–π–ª `analysis_notebook.ipynb` —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º:

```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤\\n\\n–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã –±—É–¥–µ–º –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö RL –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å\\nsys.path.append('.')\\n\\nfrom src.training import Trainer, TrainerConfig\\nfrom src.visualization.plots import plot_learning_curve, PlotConfig\\nfrom src.utils.metrics import MetricsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\\nplt.style.use('seaborn-v0_8-whitegrid')\\nsns.set_palette(\"husl\")\\n\\n# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤\\nplt.rcParams['figure.figsize'] = (12, 8)\\nplt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è PPO –Ω–∞ LunarLander-v3...\\n2024-01-15 12:00:00,000 - experiment_ppo_lunarlander - INFO - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω Trainer –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 'ppo_lunarlander_notebook'\\n2024-01-15 12:00:00,000 - experiment_ppo_lunarlander - INFO - –°—Ä–µ–¥–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: LunarLander-v3\\n2024-01-15 12:00:00,000 - experiment_ppo_lunarlander - INFO - –ê–≥–µ–Ω—Ç PPO –Ω–∞—Å—Ç—Ä–æ–µ–Ω\\n2024-01-15 12:00:00,000 - experiment_ppo_lunarlander - INFO - –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ train\\n2024-01-15 12:00:00,000 - sb3.PPO - INFO - Creating environment runner\\n2024-01-15 12:00:00,000 - sb3.PPO - INFO - Starting new experiment\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: 150.25 ¬± 45.32\\nüèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: 210.50\\n‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 125.5 —Å–µ–∫\\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: results/notebook_examples/models/ppo_model_final\\nüîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞...\\nüìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: 152.10\\nüìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: 850.2"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –ø—Ä—è–º–æ –≤ –Ω–æ—É—Ç–±—É–∫–µ\\nprint(\"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è PPO –Ω–∞ LunarLander-v3...\")\\n\\nconfig = TrainerConfig(\\n    experiment_name=\"ppo_lunarlander_notebook\",\\n    algorithm=\"PPO\",\\n    environment_name=\"LunarLander-v3\",\\n    total_timesteps=50000,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\\n    seed=42,\\n    \\n    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ü–µ–Ω–∫–∏\\n    eval_freq=10000,\\n    n_eval_episodes=5,\\n    \\n    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\\n    save_freq=25000,\\n    \\n    # –ü—É—Ç–∏\\n    output_dir=\"results/notebook_examples\",\\n    \\n    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥\\n    verbose=1,\\n)\\n\\nwith Trainer(config) as trainer:\\n    result = trainer.train()\\n    \\n    if result.success:\\n        print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\\n        print(f\"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f} ¬± {result.final_std_reward:.2f}\")\\n        print(f\"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.best_mean_reward:.2f}\")\\n        print(f\"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.1f} —Å–µ–∫\")\\n        print(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {result.model_path}\")\\n        \\n        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞\\n        print(\"üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞...\")\\n        eval_result = trainer.evaluate(n_episodes=10, render=False)\\n        print(f\"üìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {eval_result['mean_reward']:.2f}\")\\n        print(f\"üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {eval_result['mean_length']:.1f}\")\\n    else:\\n        print(f\"‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –∏–∑ –æ–±—É—á–µ–Ω–∏—è\\ndef load_training_metrics(log_dir):\\n    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫ –∏–∑ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è.\"\"\"\\n    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∫–æ–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤\\n    # –∏–ª–∏ –∏–∑ –æ–±—ä–µ–∫—Ç–∞ MetricsTracker\\n    \\n    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–¥–∏–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\\n    timesteps = np.arange(0, 50000, 1000)\\n    rewards = -200 + 400 * (1 - np.exp(-0.00005 * timesteps)) + 20 * np.random.randn(len(timesteps))\\n    \\n    return timesteps, rewards\\n\\ntimesteps, rewards = load_training_metrics(\"results/notebook_examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAgAElEQVR4nOzdeXxU9b3/8dcnM5kkk8k+QNgJ+yYgK4IgKu64V6utrbW2tbW2tta21lrbWmtttbW21lprq7W21lprbW1tq7W2VhFZBFlk3yEkgewzmcy9vz/uDQmQhGQmM5PJ5PP8eTwecp9zzvfMmXPmnHPv5yMiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiI