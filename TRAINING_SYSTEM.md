# üéÆ –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ MEPHI. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è RL —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏.

## üöÄ –ß—Ç–æ —Å–æ–∑–¥–∞–Ω–æ

### ‚ú® –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

#### 1. **Trainer** (`src/training/trainer.py`)
–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:

```python
from src.training import Trainer, TrainerConfig

config = TrainerConfig(
    experiment_name="my_experiment",
    algorithm="PPO",
    environment_name="LunarLander-v3",
    total_timesteps=100_000,
)

with Trainer(config) as trainer:
    result = trainer.train()
    print(f"–ù–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f}")
```

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤: PPO, A2C, SAC, TD3
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π —Å—Ä–µ–¥ (EnvironmentWrapper, LunarLanderEnvironment)
- ‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
- ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
- ‚úÖ –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º —Ç—Ä–µ–∫–∏–Ω–≥–æ–º

#### 2. **TrainerConfig** 
–ú–æ—â–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π:

```python
config = TrainerConfig(
    experiment_name="advanced_ppo",
    algorithm="PPO",
    environment_name="LunarLander-v3",
    total_timesteps=200_000,
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    eval_freq=10_000,
    n_eval_episodes=10,
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_freq=25_000,
    checkpoint_freq=20_000,
    
    # –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    early_stopping=True,
    patience=5,
    min_improvement=10.0,
)
```

#### 3. **CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** (`src/training/cli.py`)
–£–¥–æ–±–Ω—ã–π –∫–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å Rich UI:

```bash
# –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
python -m src.training.cli train --algorithm PPO --env LunarLander-v3 --timesteps 100000

# –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
python -m src.training.cli train --config configs/ppo_lunarlander.yaml

# –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
python -m src.training.cli resume checkpoints/checkpoint_50000.pkl

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
python -m src.training.cli compare --algorithm PPO A2C --timesteps 50000 --runs 3
```

#### 4. **–†–µ–∂–∏–º—ã –æ–±—É—á–µ–Ω–∏—è**
- **TRAIN** - –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
- **RESUME** - –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
- **EVALUATE** - —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
- **FINETUNE** - –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏

#### 5. **–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –æ–±—É—á–µ–Ω–∏—è:

```python
result = trainer.train()

if result.success:
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.final_mean_reward:.2f}")
    print(f"–õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {result.best_mean_reward:.2f}")
    print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result.training_time:.1f} —Å–µ–∫")
    print(f"–ú–æ–¥–µ–ª—å: {result.model_path}")
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    rewards = result.evaluation_history['mean_rewards']
    timesteps = result.evaluation_history['timesteps']
```

### üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
src/training/
‚îú‚îÄ‚îÄ __init__.py              # –≠–∫—Å–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª—è
‚îú‚îÄ‚îÄ trainer.py               # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å Trainer (1000+ —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ cli.py                   # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å Rich UI (500+ —Å—Ç—Ä–æ–∫)
‚îî‚îÄ‚îÄ README.md               # –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

tests/unit/training/
‚îú‚îÄ‚îÄ test_trainer.py         # –ü–æ–ª–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å –º–æ–∫–∞–º–∏ (400+ —Å—Ç—Ä–æ–∫)
‚îî‚îÄ‚îÄ test_trainer_config.py  # –¢–µ—Å—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (200+ —Å—Ç—Ä–æ–∫)

examples/training/
‚îî‚îÄ‚îÄ basic_training.py       # –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (400+ —Å—Ç—Ä–æ–∫)

configs/training/
‚îú‚îÄ‚îÄ ppo_lunarlander.yaml    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PPO
‚îú‚îÄ‚îÄ a2c_lunarlander.yaml    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A2C
‚îî‚îÄ‚îÄ sac_pendulum.yaml       # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è SAC

test_trainer_simple.py      # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
TRAINING_SYSTEM.md          # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üéØ –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

### üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏

#### –ê–≥–µ–Ω—Ç—ã
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤
trainer = Trainer(config)
# trainer.agent –±—É–¥–µ—Ç PPOAgent, A2CAgent, SACAgent –∏–ª–∏ TD3Agent
```

#### –°—Ä–µ–¥—ã
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä wrapper'–∞
config = TrainerConfig(
    environment_name="LunarLander-v3",
    use_lunar_lander_wrapper=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LunarLanderEnvironment
)
```

#### –£—Ç–∏–ª–∏—Ç—ã
```python
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
trainer.logger.info("–°–æ–æ–±—â–µ–Ω–∏–µ")

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
trainer.metrics_tracker.log_metric("reward", 250.0)

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
checkpoint_path = trainer.save_checkpoint(50000)
trainer.load_checkpoint(checkpoint_path)
```

### ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

#### –ò–∑ YAML —Ñ–∞–π–ª–æ–≤
```python
trainer = create_trainer_from_config(
    config_path="configs/ppo_lunarlander.yaml"
)
```

#### –ò–∑ RLConfig
```python
from src.utils.config import load_config

rl_config = load_config(config_name="ppo_lunarlander")
trainer_config = TrainerConfig.from_rl_config(rl_config)
trainer = Trainer(trainer_config)
```

#### –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
```python
trainer = create_trainer_from_config(
    config_name="ppo_lunarlander",
    overrides=[
        "training.total_timesteps=200000",
        "algorithm.learning_rate=0.001",
    ]
)
```

### üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å

#### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
```python
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å
trainer.logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è")

# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
trainer.logger.info(
    "–≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω",
    extra={
        "episode": 100,
        "reward": 250.0,
        "length": 500,
    }
)
```

#### –ú–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ
result = trainer.train()

# –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
training_rewards = result.training_history['mean_rewards']
eval_rewards = result.evaluation_history['mean_rewards']
```

#### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å TensorBoard
```python
config = TrainerConfig(
    tensorboard_log="results/tensorboard",
)
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
```

### üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã

#### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
```python
config = TrainerConfig(
    checkpoint_freq=25_000,  # –ö–∞–∂–¥—ã–µ 25k —à–∞–≥–æ–≤
    max_checkpoints=10,      # –ú–∞–∫—Å–∏–º—É–º 10 —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
)
```

#### –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π
```python
# –ò–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
config = TrainerConfig(
    mode=TrainingMode.RESUME,
    resume_from_checkpoint="checkpoints/checkpoint_50000.pkl",
)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
config = TrainerConfig(
    mode=TrainingMode.RESUME,
    resume_timestep=50_000,  # –ù–∞–π–¥–µ—Ç –±–ª–∏–∂–∞–π—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
)
```

### üõë –†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞

```python
config = TrainerConfig(
    early_stopping=True,
    patience=5,              # 5 –æ—Ü–µ–Ω–æ–∫ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    min_improvement=10.0,    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
    eval_freq=5_000,         # –ß–∞—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞
)
```

### üéÆ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

#### –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
```python
def basic_training():
    config = TrainerConfig(
        experiment_name="basic_ppo",
        algorithm="PPO",
        environment_name="LunarLander-v3",
        total_timesteps=100_000,
    )
    
    with Trainer(config) as trainer:
        result = trainer.train()
        return result
```

#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
```python
def compare_algorithms():
    algorithms = ["PPO", "A2C", "SAC"]
    results = {}
    
    for algorithm in algorithms:
        config = TrainerConfig(
            experiment_name=f"comparison_{algorithm}",
            algorithm=algorithm,
            total_timesteps=50_000,
        )
        
        with Trainer(config) as trainer:
            result = trainer.train()
            results[algorithm] = result
    
    return results
```

#### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
```python
def advanced_training():
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent_config = AgentConfig(
        algorithm="PPO",
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        gamma=0.99,
        policy_kwargs={
            "net_arch": [dict(pi=[64, 64], vf=[64, 64])],
        },
    )
    
    config = TrainerConfig(
        experiment_name="advanced_ppo",
        agent_config=agent_config,
        early_stopping=True,
        patience=3,
        eval_freq=5_000,
    )
    
    with Trainer(config) as trainer:
        result = trainer.train()
        return result
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
```bash
# –ü–æ–ª–Ω—ã–µ —Ç–µ—Å—Ç—ã (—Ç—Ä–µ–±—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
pytest tests/unit/training/ -v

# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
python test_trainer_simple.py
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```
üéÆ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ—Å—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è
==================================================
‚úÖ AgentConfig —Å–æ–∑–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
‚úÖ TrainerConfig: –≤–∞–ª–∏–¥–∞—Ü–∏—è, –ø—É—Ç–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚úÖ TrainingMode: enum –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ
‚úÖ TrainingResult: —Å–æ–∑–¥–∞–Ω–∏–µ, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è
‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤: PPO, A2C, SAC, TD3

üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!
```

## üìã –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–µ–∫—Ç–æ–º
- ‚úÖ **–ê–≥–µ–Ω—Ç—ã**: –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å `src.agents` (PPO, A2C, SAC, TD3)
- ‚úÖ **–°—Ä–µ–¥—ã**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å `src.environments` (EnvironmentWrapper, LunarLanderEnvironment)
- ‚úÖ **–£—Ç–∏–ª–∏—Ç—ã**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç `src.utils` (–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è, —á–µ–∫–ø–æ–∏–Ω—Ç—ã)
- ‚úÖ **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã**: –ì–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å `src.experiments`

### –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ **Type hints**: –ü–æ–ª–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
- ‚úÖ **Docstrings**: –†—É—Å—Å–∫–∏–µ docstrings –≤ Google —Å—Ç–∏–ª–µ
- ‚úÖ **Ruff**: –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –ø—Ä–æ–µ–∫—Ç–∞
- ‚úÖ **Pytest**: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

## üöÄ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å
1. ‚úÖ **–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π** - –≤–∞–ª–∏–¥–∞—Ü–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –ø—É—Ç–∏
2. ‚úÖ **–†–µ–∂–∏–º—ã –æ–±—É—á–µ–Ω–∏—è** - train, resume, evaluate, finetune
3. ‚úÖ **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è** - —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
4. ‚úÖ **CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - –∫–æ–º–∞–Ω–¥—ã train, resume, evaluate, compare
5. ‚úÖ **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã** - YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
6. ‚úÖ **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è** - –±–∞–∑–æ–≤—ã–µ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### –ß—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- üîß **–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - —Ç—Ä–µ–±—É–µ—Ç `gymnasium`, `stable-baselines3`
- üîß **CLI —Å Rich UI** - —Ç—Ä–µ–±—É–µ—Ç `typer`, `rich`
- üîß **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∞–≥–µ–Ω—Ç–∞–º–∏** - —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É –ø—Ä–æ–µ–∫—Ç–∞

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
1. **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**: `pip install gymnasium stable-baselines3 typer rich`
2. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏**: –ó–∞–ø—É—Å–∫ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏
3. **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏**: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
4. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**: –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–°–æ–∑–¥–∞–Ω–∞ **–∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤** —Å:

- üèóÔ∏è **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –ú–æ–¥—É–ª—å–Ω–∞—è, —Ä–∞—Å—à–∏—Ä—è–µ–º–∞—è, —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–∞—è
- üéÆ **–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å**: –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- üîß **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –ì–ª—É–±–æ–∫–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞
- üìä **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
- üõ†Ô∏è **–£–¥–æ–±—Å—Ç–≤–æ**: CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- üß™ **–ö–∞—á–µ—Å—Ç–≤–æ**: –ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –≤—Å–µ—Ö RL —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ!