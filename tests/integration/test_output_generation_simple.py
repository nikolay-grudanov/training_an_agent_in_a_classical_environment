"""–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–≠—Ç–æ—Ç —Ç–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã User Story 3 (Generate Required Outputs)
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–∫–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
"""

import json
import shutil
import tempfile
import time
from pathlib import Path
from typing import Dict
from unittest.mock import MagicMock, patch

import numpy as np
import pandas as pd
import pytest

from src.evaluation.quantitative_eval import QuantitativeEvaluator
from src.reporting.results_formatter import ResultsFormatter, ReportConfig
from src.utils.seeding import set_seed
from src.visualization.agent_demo import DemoConfig
from src.visualization.performance_plots import (
    PerformancePlotter,
    create_performance_report,
)


class TestOutputGenerationSimple:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    
    @pytest.fixture(scope="class")
    def test_output_dir(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
        temp_dir = Path(tempfile.mkdtemp(prefix="test_output_generation_simple_"))
        yield temp_dir
        # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
    
    @pytest.fixture(scope="class")
    def sample_training_data(self) -> Dict[str, pd.DataFrame]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è."""
        set_seed(42)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è PPO
        timesteps = np.arange(0, 5000, 100)
        ppo_rewards = 100 * (1 - np.exp(-timesteps / 2000)) + np.random.normal(0, 10, len(timesteps))
        
        ppo_data = pd.DataFrame({
            'timestep': timesteps,
            'episode': np.arange(len(timesteps)),
            'value': ppo_rewards,
            'timestamp': pd.date_range('2024-01-01', periods=len(timesteps), freq='1min')
        })
        
        return {'episode_reward': ppo_data}
    
    def test_performance_plots_creation(
        self,
        sample_training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 1: –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        print("\nüìä –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
        plots_dir = test_output_dir / "plots"
        plots_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–ª–æ—Ç—Ç–µ—Ä
        plotter = PerformancePlotter()
        
        # 1. –ì—Ä–∞—Ñ–∏–∫ –∫—Ä–∏–≤–æ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
        reward_plot_path = plotter.plot_reward_curve(
            data=sample_training_data,
            y_col='episode_reward',
            save_path=plots_dir / "reward_curve.png",
            title="–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è: –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ"
        )
        
        assert Path(reward_plot_path).exists()
        assert Path(reward_plot_path).stat().st_size > 0
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω: {reward_plot_path}")
        
        # 2. –î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫
        dashboard_path = plotter.create_dashboard(
            data=sample_training_data,
            save_path=plots_dir / "dashboard.png",
            title="–î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"
        )
        
        assert Path(dashboard_path).exists()
        assert Path(dashboard_path).stat().st_size > 0
        print(f"‚úÖ –î–∞—à–±–æ—Ä–¥ —Å–æ–∑–¥–∞–Ω: {dashboard_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        created_plots = list(plots_dir.glob("*.png"))
        assert len(created_plots) >= 2
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(created_plots)}")
        
        print("üéâ –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    def test_demo_videos_creation(
        self,
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 2: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ (–º–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)."""
        print("\nüé¨ –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ")
        
        videos_dir = test_output_dir / "videos"
        videos_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü–æ–ª–Ω–æ—Å—Ç—å—é –º–æ–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ
        with patch('src.visualization.agent_demo.create_best_episode_demo') as mock_create_demo:
            def mock_demo_creation(agent, env, output_path, config, **kwargs):
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –≤–∏–¥–µ–æ—Ñ–∞–π–ª
                Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                Path(output_path).write_text("mock video content")
                
                return {
                    "success": True,
                    "demo_type": "best_episode",
                    "agent_name": getattr(agent, 'name', 'TestAgent'),
                    "best_reward": 150.0,
                    "output_path": str(output_path)
                }
            
            mock_create_demo.side_effect = mock_demo_creation
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ
            from src.visualization.agent_demo import create_best_episode_demo
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–∫ –∞–≥–µ–Ω—Ç–∞
            mock_agent = MagicMock()
            mock_agent.name = "TestAgent"
            mock_agent.predict.return_value = (np.array([1]), None)
            
            # –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ
            demo_path = videos_dir / "test_demo.mp4"
            demo_info = create_best_episode_demo(
                agent=mock_agent,
                env="LunarLander-v2",
                output_path=demo_path,
                config=DemoConfig(auto_compress=False),
                num_candidates=3
            )
            
            assert demo_info["success"]
            assert Path(demo_info["output_path"]).exists()
            print(f"‚úÖ –î–µ–º–æ –≤–∏–¥–µ–æ —Å–æ–∑–¥–∞–Ω–æ: {demo_info['output_path']}")
        
        print("üéâ –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    @patch('src.evaluation.evaluator.Evaluator.evaluate_agent')
    def test_quantitative_evaluation(
        self,
        mock_evaluate_agent,
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 3: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ (–º–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)."""
        print("\nüìà –¢–µ—Å—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤")
        
        eval_dir = test_output_dir / "evaluation"
        eval_dir.mkdir(parents=True, exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–∞ –æ—Ü–µ–Ω–∫–∏
        def mock_evaluation(agent, num_episodes, **kwargs):
            # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
            rewards = np.random.normal(150, 25, num_episodes)
            lengths = np.random.randint(150, 250, num_episodes)
            successes = [r > 100 for r in rewards]
            
            from src.evaluation.evaluator import EvaluationMetrics
            return EvaluationMetrics(
                mean_reward=float(np.mean(rewards)),
                std_reward=float(np.std(rewards)),
                min_reward=float(np.min(rewards)),
                max_reward=float(np.max(rewards)),
                mean_episode_length=float(np.mean(lengths)),
                std_episode_length=float(np.std(lengths)),
                min_episode_length=int(np.min(lengths)),
                max_episode_length=int(np.max(lengths)),
                success_rate=float(np.mean(successes)),
                total_episodes=num_episodes,
                total_timesteps=int(np.sum(lengths)),
                evaluation_time=num_episodes * 0.1,
                episode_rewards=rewards.tolist(),
                episode_lengths=lengths.tolist(),
                episode_successes=successes,
                reward_ci_lower=float(np.mean(rewards) - 1.96 * np.std(rewards) / np.sqrt(num_episodes)),
                reward_ci_upper=float(np.mean(rewards) + 1.96 * np.std(rewards) / np.sqrt(num_episodes))
            )
        
        mock_evaluate_agent.side_effect = mock_evaluation
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–∫ —Å—Ä–µ–¥—ã –∏ –æ—Ü–µ–Ω—â–∏–∫–∞
        mock_env = MagicMock()
        mock_env.spec.id = "LunarLander-v2"
        
        evaluator = QuantitativeEvaluator(env=mock_env)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–∫ –∞–≥–µ–Ω—Ç–∞
        mock_agent = MagicMock()
        mock_agent.name = "TestAgent"
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É
        metrics = evaluator.evaluate_agent_quantitative(
            agent=mock_agent,
            num_episodes=15,
            agent_name="TestAgent"
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        assert metrics.base_metrics.total_episodes == 15
        assert metrics.reward_stability_score >= 0
        assert metrics.reward_stability_score <= 1
        
        print(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞: –Ω–∞–≥—Ä–∞–¥–∞ {metrics.base_metrics.mean_reward:.2f} ¬± "
              f"{metrics.base_metrics.std_reward:.2f}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ—Ü–µ–Ω–∫–∏
        text_report = evaluator.generate_comprehensive_report(
            metrics=metrics,
            save_path=eval_dir / "evaluation_report.txt",
            format_type="text"
        )
        
        assert (eval_dir / "evaluation_report.txt").exists()
        assert len(text_report) > 0
        print(f"‚úÖ –û—Ç—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω: {eval_dir / 'evaluation_report.txt'}")
        
        print("üéâ –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    def test_results_formatting(
        self,
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 4: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Ç—á–µ—Ç—ã."""
        print("\nüìù –¢–µ—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        reports_dir = test_output_dir / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤—â–∏–∫
        formatter = ResultsFormatter(
            output_dir=reports_dir,
            config=ReportConfig(language="ru", include_plots=True)
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        from src.evaluation.evaluator import EvaluationMetrics
        
        test_metrics = EvaluationMetrics(
            mean_reward=150.5,
            std_reward=25.3,
            min_reward=100.0,
            max_reward=200.0,
            mean_episode_length=195.2,
            std_episode_length=15.8,
            min_episode_length=150,
            max_episode_length=230,
            success_rate=0.85,
            total_episodes=20,
            total_timesteps=3904,
            evaluation_time=45.2,
            episode_rewards=[150.5] * 20,
            episode_lengths=[195] * 20,
            episode_successes=[True] * 17 + [False] * 3,
            reward_ci_lower=140.3,
            reward_ci_upper=160.7
        )
        
        # 1. –û—Ç—á–µ—Ç –ø–æ –æ–¥–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É
        single_agent_report = formatter.format_single_agent_report(
            agent_name="TestAgent",
            evaluation_results=test_metrics,
            output_format="html",
            filename="test_agent_report"
        )
        
        assert single_agent_report.exists()
        assert single_agent_report.suffix == ".html"
        print(f"‚úÖ –û—Ç—á–µ—Ç –ø–æ –∞–≥–µ–Ω—Ç—É —Å–æ–∑–¥–∞–Ω: {single_agent_report}")
        
        # 2. –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
        csv_export = formatter.export_to_csv(
            data=test_metrics,
            filename="test_results"
        )
        
        assert csv_export.exists()
        assert csv_export.suffix == ".csv"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ CSV
        df = pd.read_csv(csv_export)
        assert len(df) == 1
        assert "mean_reward" in df.columns
        print(f"‚úÖ CSV —ç–∫—Å–ø–æ—Ä—Ç —Å–æ–∑–¥–∞–Ω: {csv_export}")
        
        # 3. –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
        json_export = formatter.export_to_json(
            data={"test_metrics": test_metrics},
            filename="test_results_json"
        )
        
        assert json_export.exists()
        assert json_export.suffix == ".json"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ JSON
        with open(json_export, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        assert "test_metrics" in json_data
        print(f"‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç —Å–æ–∑–¥–∞–Ω: {json_export}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        created_reports = list(reports_dir.rglob("*"))
        created_files = [f for f in created_reports if f.is_file()]
        assert len(created_files) >= 3
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –æ—Ç—á–µ—Ç–æ–≤: {len(created_files)}")
        
        print("üéâ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    def test_performance_measurement(
        self,
        sample_training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 5: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        print("\n‚è±Ô∏è –¢–µ—Å—Ç –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
        performance_dir = test_output_dir / "performance"
        performance_dir.mkdir(parents=True, exist_ok=True)
        
        performance_metrics = {}
        
        # 1. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        start_time = time.time()
        
        plotter = PerformancePlotter()
        plots_created = 0
        
        for i in range(2):  # –°–æ–∑–¥–∞–µ–º 2 –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
            plot_path = plotter.plot_reward_curve(
                data=sample_training_data,
                save_path=performance_dir / f"perf_plot_{i}.png"
            )
            plots_created += 1
        
        plots_time = time.time() - start_time
        performance_metrics["plots"] = {
            "total_time": plots_time,
            "plots_created": plots_created,
            "time_per_plot": plots_time / plots_created,
        }
        
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏: {plots_created} –∑–∞ {plots_time:.2f}—Å "
              f"({performance_metrics['plots']['time_per_plot']:.2f}—Å/–≥—Ä–∞—Ñ–∏–∫)")
        
        # 2. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
        start_time = time.time()
        
        formatter = ResultsFormatter(output_dir=performance_dir)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        from src.evaluation.evaluator import EvaluationMetrics
        test_metrics = EvaluationMetrics(
            mean_reward=150.0, std_reward=25.0, min_reward=100.0, max_reward=200.0,
            mean_episode_length=200.0, std_episode_length=20.0, 
            min_episode_length=150, max_episode_length=250,
            success_rate=0.8, total_episodes=20, total_timesteps=4000, evaluation_time=30.0,
            episode_rewards=[150.0] * 20, episode_lengths=[200] * 20, 
            episode_successes=[True] * 16 + [False] * 4,
            reward_ci_lower=140.0, reward_ci_upper=160.0
        )
        
        reports_created = 0
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        for format_type in ["html", "json", "csv"]:
            try:
                if format_type == "csv":
                    formatter.export_to_csv(
                        data=test_metrics,
                        filename=f"perf_report_{format_type}"
                    )
                elif format_type == "json":
                    formatter.export_to_json(
                        data={"test_metrics": test_metrics},
                        filename=f"perf_report_{format_type}"
                    )
                else:
                    formatter.format_single_agent_report(
                        agent_name="PerfTestAgent",
                        evaluation_results=test_metrics,
                        output_format=format_type,
                        filename=f"perf_report_{format_type}"
                    )
                reports_created += 1
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞ {format_type}: {e}")
        
        reports_time = time.time() - start_time
        performance_metrics["reports"] = {
            "total_time": reports_time,
            "reports_created": reports_created,
            "time_per_report": reports_time / reports_created if reports_created > 0 else 0,
        }
        
        print(f"‚úÖ –û—Ç—á–µ—Ç—ã: {reports_created} –∑–∞ {reports_time:.2f}—Å "
              f"({performance_metrics['reports']['time_per_report']:.2f}—Å/–æ—Ç—á–µ—Ç)")
        
        # 3. –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        total_time = sum(m["total_time"] for m in performance_metrics.values())
        total_operations = (
            performance_metrics["plots"]["plots_created"] +
            performance_metrics["reports"]["reports_created"]
        )
        
        performance_metrics["overall"] = {
            "total_time": total_time,
            "total_operations": total_operations,
            "operations_per_second": total_operations / total_time if total_time > 0 else 0
        }
        
        print(f"‚úÖ –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_operations} –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞ {total_time:.2f}—Å "
              f"({performance_metrics['overall']['operations_per_second']:.2f} –æ–ø/—Å)")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        performance_report_path = performance_dir / "performance_metrics.json"
        with open(performance_report_path, 'w', encoding='utf-8') as f:
            json.dump(performance_metrics, f, indent=2)
        
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {performance_report_path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        assert performance_metrics["plots"]["time_per_plot"] < 10.0  # –ù–µ –±–æ–ª–µ–µ 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
        assert performance_metrics["reports"]["time_per_report"] < 5.0  # –ù–µ –±–æ–ª–µ–µ 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –æ—Ç—á–µ—Ç
        
        print("üéâ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
        
        return performance_metrics
    
    @pytest.mark.integration
    def test_complete_output_generation_workflow(
        self,
        sample_training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 6: –ü–æ–ª–Ω—ã–π workflow –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        print("\nüöÄ –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ workflow –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        workflow_dir = test_output_dir / "complete_workflow"
        workflow_dir.mkdir(parents=True, exist_ok=True)
        
        workflow_results = {
            "start_time": time.time(),
            "steps_completed": [],
            "files_created": [],
            "success": True
        }
        
        try:
            # –®–∞–≥ 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
            print("üîß –®–∞–≥ 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è")
            
            data_dir = workflow_dir / "training_data"
            data_dir.mkdir(exist_ok=True)
            
            for metric_name, data in sample_training_data.items():
                data_path = data_dir / f"{metric_name}.csv"
                data.to_csv(data_path, index=False)
                workflow_results["files_created"].append(str(data_path))
            
            workflow_results["steps_completed"].append("data_preparation")
            print("‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
            # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            print("üìä –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
            
            plots_dir = workflow_dir / "plots"
            performance_report_dir = create_performance_report(
                data=sample_training_data,
                output_dir=plots_dir,
                include_interactive=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                include_static=True
            )
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
            static_plots = list(Path(performance_report_dir).glob("static/*.png"))
            workflow_results["files_created"].extend([str(p) for p in static_plots])
            workflow_results["steps_completed"].append("performance_plots")
            
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(static_plots)}")
            
            # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
            print("üìù –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤")
            
            reports_dir = workflow_dir / "reports"
            reports_dir.mkdir(exist_ok=True)
            
            formatter = ResultsFormatter(output_dir=reports_dir)
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞
            from src.evaluation.evaluator import EvaluationMetrics
            test_metrics = EvaluationMetrics(
                mean_reward=150.0, std_reward=25.0, min_reward=100.0, max_reward=200.0,
                mean_episode_length=200.0, std_episode_length=20.0,
                min_episode_length=150, max_episode_length=250,
                success_rate=0.8, total_episodes=20, total_timesteps=4000, evaluation_time=30.0,
                episode_rewards=[150.0] * 20, episode_lengths=[200] * 20,
                episode_successes=[True] * 16 + [False] * 4,
                reward_ci_lower=140.0, reward_ci_upper=160.0
            )
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç—ã
            html_report = formatter.format_single_agent_report(
                agent_name="WorkflowTestAgent",
                evaluation_results=test_metrics,
                output_format="html",
                filename="workflow_report"
            )
            workflow_results["files_created"].append(str(html_report))
            
            csv_export = formatter.export_to_csv(
                data=test_metrics,
                filename="workflow_results"
            )
            workflow_results["files_created"].append(str(csv_export))
            
            workflow_results["steps_completed"].append("reports_creation")
            print("‚úÖ –û—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã")
            
            # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            print("üîç –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            
            files_verification = {
                "total_files": len(workflow_results["files_created"]),
                "existing_files": 0,
                "missing_files": []
            }
            
            for file_path in workflow_results["files_created"]:
                path_obj = Path(file_path)
                if path_obj.exists():
                    files_verification["existing_files"] += 1
                else:
                    files_verification["missing_files"].append(str(path_obj))
            
            workflow_results["files_verification"] = files_verification
            workflow_results["steps_completed"].append("files_verification")
            
            print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤: {files_verification['existing_files']}/{files_verification['total_files']} —Å—É—â–µ—Å—Ç–≤—É—é—Ç")
            
            if files_verification["missing_files"]:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã: {files_verification['missing_files']}")
            
        except Exception as e:
            workflow_results["success"] = False
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ workflow: {e}")
            raise
        
        finally:
            workflow_results["end_time"] = time.time()
            workflow_results["total_time"] = workflow_results["end_time"] - workflow_results["start_time"]
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ workflow
            workflow_summary_path = workflow_dir / "workflow_summary.json"
            with open(workflow_summary_path, 'w', encoding='utf-8') as f:
                json.dump(workflow_results, f, indent=2, default=str)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        assert workflow_results["success"], "Workflow –¥–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è —É—Å–ø–µ—à–Ω–æ"
        assert len(workflow_results["steps_completed"]) == 4, "–í—Å–µ —à–∞–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω—ã"
        assert workflow_results["files_verification"]["existing_files"] > 0, "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã"
        
        print(f"\nüéâ –ü–æ–ª–Ω—ã–π workflow –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ {workflow_results['total_time']:.2f}—Å")
        print(f"üìä –í—ã–ø–æ–ª–Ω–µ–Ω–æ —à–∞–≥–æ–≤: {len(workflow_results['steps_completed'])}")
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {workflow_results['files_verification']['total_files']}")
        print(f"‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {workflow_results['files_verification']['existing_files']}")
        
        return workflow_results


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    pytest.main([__file__, "-v", "-s", "--tb=short"])