"""–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Å–µ—Å—Å–∏–µ–π –æ–±—É—á–µ–Ω–∏—è.

–≠—Ç–æ—Ç —Ç–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ User Story 3 (Generate Required Outputs):
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∞–≥–µ–Ω—Ç–æ–≤
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ 10-20 —ç–ø–∏–∑–æ–¥–∞—Ö
- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Ç—á–µ—Ç—ã

–¢–µ—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –∏–ª–∏ –∏—Ö –º–æ–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

import json
import shutil
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, Optional, Tuple
from unittest.mock import MagicMock, patch

import numpy as np
import pandas as pd
import pytest

from src.evaluation.quantitative_eval import (
    QuantitativeEvaluator,
    QuantitativeMetrics,
    evaluate_agent_standard,
)
from src.reporting.results_formatter import ResultsFormatter, ReportConfig
from src.utils.seeding import set_seed
from src.visualization.agent_demo import (
    AgentDemoError,
    DemoConfig,
    create_best_episode_demo,
    create_batch_demos,
    quick_demo,
)
from src.visualization.performance_plots import (
    PerformancePlotter,
    create_performance_report,
    quick_reward_plot,
)


class MockAgent:
    """–ú–æ–∫ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""

    def __init__(
        self,
        name: str = "MockAgent",
        performance_level: str = "good",
        deterministic: bool = True,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–∫ –∞–≥–µ–Ω—Ç–∞.

        Args:
            name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            performance_level: –£—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ('poor', 'good', 'excellent')
            deterministic: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
        """
        self.name = name
        self.performance_level = performance_level
        self.deterministic = deterministic
        self._episode_count = 0

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if performance_level == "poor":
            self._base_reward = -200
            self._reward_variance = 50
            self._success_rate = 0.1
        elif performance_level == "good":
            self._base_reward = 100
            self._reward_variance = 30
            self._success_rate = 0.6
        else:  # excellent
            self._base_reward = 200
            self._reward_variance = 20
            self._success_rate = 0.9

    def predict(
        self, observation, deterministic: bool = True, **kwargs
    ) -> Tuple[np.ndarray, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞."""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è LunarLander
        action = np.random.randint(0, 4) if not deterministic else 1
        return np.array([action]), None

    def learn(self, total_timesteps: int, **kwargs) -> "MockAgent":
        """–ò–º–∏—Ç–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞."""
        return self

    def save(self, path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞."""
        save_path = Path(path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "name": self.name,
            "performance_level": self.performance_level,
            "deterministic": self.deterministic,
            "episode_count": self._episode_count,
        }

        with open(save_path, "w") as f:
            json.dump(metadata, f)

    @classmethod
    def load(cls, path: str, **kwargs) -> "MockAgent":
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞."""
        with open(path, "r") as f:
            metadata = json.load(f)

        agent = cls(
            name=metadata["name"],
            performance_level=metadata["performance_level"],
            deterministic=metadata["deterministic"],
        )
        agent._episode_count = metadata["episode_count"]
        return agent

    def simulate_episode_reward(self) -> float:
        """–°–∏–º—É–ª—è—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —ç–ø–∏–∑–æ–¥."""
        self._episode_count += 1

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ç—Ä–µ–Ω–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
        trend_bonus = min(self._episode_count * 0.5, 20)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–≥—Ä–∞–¥—É —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å—é
        if self.deterministic:
            np.random.seed(42 + self._episode_count)

        reward = np.random.normal(
            self._base_reward + trend_bonus, self._reward_variance
        )

        return float(reward)


class MockEnvironment:
    """–ú–æ–∫ —Å—Ä–µ–¥—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""

    def __init__(self, env_name: str = "LunarLander-v2"):
        self.env_name = env_name
        self.spec = MagicMock()
        self.spec.id = env_name
        self._episode_length = 0
        self._max_episode_length = 200

    def reset(self, seed: Optional[int] = None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã."""
        if seed is not None:
            np.random.seed(seed)
        self._episode_length = 0
        observation = np.random.random(
            8
        )  # LunarLander –∏–º–µ–µ—Ç 8-–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        info = {}
        return observation, info

    def step(self, action):
        """–®–∞–≥ –≤ —Å—Ä–µ–¥–µ."""
        self._episode_length += 1

        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è
        observation = np.random.random(8)
        reward = np.random.normal(0, 1)  # –°–ª—É—á–∞–π–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
        done = self._episode_length >= self._max_episode_length
        truncated = False
        info = {}

        return observation, reward, done, truncated, info

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å—Ä–µ–¥—ã."""
        pass


class TestOutputGeneration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""

    @pytest.fixture(scope="class")
    def test_output_dir(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
        temp_dir = Path(tempfile.mkdtemp(prefix="test_output_generation_"))
        yield temp_dir
        # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤
        if temp_dir.exists():
            shutil.rmtree(temp_dir)

    @pytest.fixture(scope="class")
    def mock_env(self) -> MockEnvironment:
        """–ú–æ–∫ —Å—Ä–µ–¥—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return MockEnvironment("LunarLander-v2")

    @pytest.fixture(scope="class")
    def trained_agents(self) -> Dict[str, MockAgent]:
        """–ù–∞–±–æ—Ä –æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è."""
        return {
            "PPO_Excellent": MockAgent("PPO_Excellent", "excellent"),
            "A2C_Good": MockAgent("A2C_Good", "good"),
            "SAC_Poor": MockAgent("SAC_Poor", "poor"),
        }

    @pytest.fixture(scope="class")
    def training_data(self) -> Dict[str, pd.DataFrame]:
        """–°–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤."""
        set_seed(42)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è PPO (—Ö–æ—Ä–æ—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å)
        timesteps = np.arange(0, 10000, 100)
        ppo_rewards = 100 * (1 - np.exp(-timesteps / 3000)) + np.random.normal(
            0, 10, len(timesteps)
        )
        ppo_lengths = (
            200
            - 50 * (1 - np.exp(-timesteps / 2000))
            + np.random.normal(0, 5, len(timesteps))
        )

        ppo_data = pd.DataFrame(
            {
                "timestep": timesteps,
                "episode": np.arange(len(timesteps)),
                "value": ppo_rewards,
                "episode_length": ppo_lengths.astype(int),
                "timestamp": pd.date_range(
                    "2024-01-01", periods=len(timesteps), freq="1min"
                ),
            }
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è A2C (–±–æ–ª–µ–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å)
        a2c_rewards = 80 * (1 - np.exp(-timesteps / 4000)) + np.random.normal(
            0, 15, len(timesteps)
        )
        a2c_lengths = (
            220
            - 40 * (1 - np.exp(-timesteps / 3000))
            + np.random.normal(0, 8, len(timesteps))
        )

        a2c_data = pd.DataFrame(
            {
                "timestep": timesteps,
                "episode": np.arange(len(timesteps)),
                "value": a2c_rewards,
                "episode_length": a2c_lengths.astype(int),
                "timestamp": pd.date_range(
                    "2024-01-01", periods=len(timesteps), freq="1min"
                ),
            }
        )

        return {
            "episode_reward": ppo_data,
            "episode_reward_a2c": a2c_data,
            "episode_length": ppo_data[
                ["timestep", "episode", "episode_length", "timestamp"]
            ].rename(columns={"episode_length": "value"}),
        }

    def test_data_preparation_and_validation(
        self,
        trained_agents: Dict[str, MockAgent],
        training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è."""
        print("\nüîß –¢–µ—Å—Ç 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–≥–µ–Ω—Ç–æ–≤
        assert len(trained_agents) == 3
        for name, agent in trained_agents.items():
            assert isinstance(agent, MockAgent)
            assert agent.name == name
            print(f"‚úÖ –ê–≥–µ–Ω—Ç {name} –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω (—É—Ä–æ–≤–µ–Ω—å: {agent.performance_level})")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        assert len(training_data) >= 2
        for metric_name, data in training_data.items():
            assert isinstance(data, pd.DataFrame)
            assert len(data) > 0
            assert "timestep" in data.columns
            assert "value" in data.columns
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ {metric_name}: {len(data)} –∑–∞–ø–∏—Å–µ–π")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        data_dir = test_output_dir / "training_data"
        data_dir.mkdir(parents=True, exist_ok=True)

        for metric_name, data in training_data.items():
            data_path = data_dir / f"{metric_name}.csv"
            data.to_csv(data_path, index=False)
            assert data_path.exists()
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ {metric_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {data_path}")

        print("üéâ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    def test_performance_plots_generation(
        self, training_data: Dict[str, pd.DataFrame], test_output_dir: Path
    ):
        """–¢–µ—Å—Ç 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        print("\nüìä –¢–µ—Å—Ç 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

        plots_dir = test_output_dir / "plots"
        plots_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–ª–æ—Ç—Ç–µ—Ä
        plotter = PerformancePlotter()

        # 1. –ì—Ä–∞—Ñ–∏–∫ –∫—Ä–∏–≤–æ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
        reward_plot_path = plotter.plot_reward_curve(
            data=training_data,
            y_col="episode_reward",
            save_path=plots_dir / "reward_curve.png",
            title="–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è: –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ",
        )

        assert Path(reward_plot_path).exists()
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω: {reward_plot_path}")

        # 2. –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —ç–ø–∏–∑–æ–¥–æ–≤
        length_plot_path = plotter.plot_episode_lengths(
            data=training_data,
            y_col="episode_length",
            save_path=plots_dir / "episode_lengths.png",
            title="–î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–æ–≤",
        )

        assert Path(length_plot_path).exists()
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —ç–ø–∏–∑–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω: {length_plot_path}")

        # 3. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –∞–≥–µ–Ω—Ç–æ–≤
        agents_data = {
            "PPO": training_data["episode_reward"],
            "A2C": training_data["episode_reward_a2c"],
        }

        comparison_plot_path = plotter.plot_multiple_agents(
            agents_data=agents_data,
            metric="episode_reward",
            save_path=plots_dir / "agents_comparison.png",
            title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤: PPO vs A2C",
        )

        assert Path(comparison_plot_path).exists()
        print(f"‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å–æ–∑–¥–∞–Ω: {comparison_plot_path}")

        # 4. –î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫
        dashboard_path = plotter.create_dashboard(
            data=training_data,
            save_path=plots_dir / "training_dashboard.png",
            title="–î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è",
        )

        assert Path(dashboard_path).exists()
        print(f"‚úÖ –î–∞—à–±–æ—Ä–¥ —Å–æ–∑–¥–∞–Ω: {dashboard_path}")

        # 5. –ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞—Ñ–∏–∫ —á–µ—Ä–µ–∑ —É–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        quick_plot_path = quick_reward_plot(
            data_source=training_data["episode_reward"],
            save_path=plots_dir / "quick_reward_plot.png",
        )

        assert Path(quick_plot_path).exists()
        print(f"‚úÖ –ë—ã—Å—Ç—Ä—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å–æ–∑–¥–∞–Ω: {quick_plot_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã
        created_plots = list(plots_dir.glob("*.png"))
        assert len(created_plots) >= 5
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(created_plots)}")

        print("üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    @patch("src.visualization.video_generator.setup_recording_environment")
    @patch("src.visualization.video_generator.record_agent_episode")
    def test_agent_demo_videos_generation(
        self,
        mock_record_episode,
        mock_setup_env,
        trained_agents: Dict[str, MockAgent],
        mock_env: MockEnvironment,
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 3: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∞–≥–µ–Ω—Ç–æ–≤."""
        print("\nüé¨ –¢–µ—Å—Ç 3: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ")

        videos_dir = test_output_dir / "videos"
        videos_dir.mkdir(parents=True, exist_ok=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_setup_env.return_value = mock_env
        mock_record_episode.return_value = {
            "success": True,
            "total_reward": 150.0,
            "episode_length": 200,
            "output_path": str(videos_dir / "test_video.mp4"),
        }

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –≤–∏–¥–µ–æ—Ñ–∞–π–ª –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        def create_mock_video(path: Path):
            path.parent.mkdir(parents=True, exist_ok=True)
            path.write_text("mock video content")

        # 1. –î–µ–º–æ –ª—É—á—à–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        best_agent = trained_agents["PPO_Excellent"]
        demo_config = DemoConfig(
            auto_compress=False,  # –û—Ç–∫–ª—é—á–∞–µ–º —Å–∂–∞—Ç–∏–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
            auto_naming=True,
        )

        best_demo_path = videos_dir / "best_episode_demo.mp4"
        create_mock_video(best_demo_path)

        # –ú–æ–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ
        with patch("src.visualization.agent_demo.record_agent_episode") as mock_record:
            mock_record.return_value = {
                "success": True,
                "total_reward": 180.5,
                "episode_length": 195,
                "output_path": str(best_demo_path),
            }

            demo_info = create_best_episode_demo(
                agent=best_agent,
                env="LunarLander-v2",
                output_path=best_demo_path,
                config=demo_config,
                num_candidates=5,
            )

        assert demo_info["success"]
        assert demo_info["demo_type"] == "best_episode"
        assert demo_info["agent_name"] == "PPO_Excellent"
        print(f"‚úÖ –î–µ–º–æ –ª—É—á—à–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞ —Å–æ–∑–¥–∞–Ω–æ: {demo_info['output_path']}")

        # 2. –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        batch_demos_dir = videos_dir / "batch_demos"

        # –ú–æ–∫–∞–µ–º –ø–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ
        with patch(
            "src.visualization.agent_demo.create_best_episode_demo"
        ) as mock_batch_demo:

            def mock_demo_creation(agent, env, output_path, config):
                create_mock_video(Path(output_path))
                return {
                    "success": True,
                    "demo_type": "best_episode",
                    "agent_name": agent.name,
                    "best_reward": agent.simulate_episode_reward(),
                    "output_path": str(output_path),
                }

            mock_batch_demo.side_effect = mock_demo_creation

            agents_list = [(name, agent) for name, agent in trained_agents.items()]
            batch_result = create_batch_demos(
                agents=agents_list,
                env="LunarLander-v2",
                output_dir=batch_demos_dir,
                demo_types=["best_episode"],
                config=demo_config,
            )

        assert batch_result["success"]
        assert batch_result["demos_created"] == len(trained_agents)
        assert batch_result["demos_failed"] == 0
        print(f"‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–µ –¥–µ–º–æ —Å–æ–∑–¥–∞–Ω—ã: {batch_result['demos_created']} –≤–∏–¥–µ–æ")

        # 3. –ë—ã—Å—Ç—Ä–æ–µ –¥–µ–º–æ —á–µ—Ä–µ–∑ —É–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        quick_demo_path = videos_dir / "quick_demo.mp4"
        create_mock_video(quick_demo_path)

        with patch(
            "src.visualization.agent_demo.create_best_episode_demo"
        ) as mock_quick:
            mock_quick.return_value = {
                "success": True,
                "output_path": str(quick_demo_path),
                "compressed_path": None,
            }

            quick_result_path = quick_demo(
                agent=best_agent, env="LunarLander-v2", output_path=quick_demo_path
            )

        assert Path(quick_result_path).exists()
        print(f"‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –¥–µ–º–æ —Å–æ–∑–¥–∞–Ω–æ: {quick_result_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—ã
        created_videos = list(videos_dir.rglob("*.mp4"))
        assert len(created_videos) >= 3
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –≤–∏–¥–µ–æ: {len(created_videos)}")

        print("üéâ –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

    def test_quantitative_evaluation(
        self,
        trained_agents: Dict[str, MockAgent],
        mock_env: MockEnvironment,
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 10-20 —ç–ø–∏–∑–æ–¥–∞—Ö."""
        print("\nüìà –¢–µ—Å—Ç 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤")

        eval_dir = test_output_dir / "evaluation"
        eval_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫
        evaluator = QuantitativeEvaluator(
            env=mock_env, baseline_threshold=100.0, min_effect_size=0.5, random_seed=42
        )

        # –ú–æ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
        with patch.object(evaluator.evaluator, "evaluate_agent") as mock_eval:

            def mock_evaluation(agent, num_episodes, **kwargs):
                # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω—è –∞–≥–µ–Ω—Ç–∞
                rewards = [agent.simulate_episode_reward() for _ in range(num_episodes)]
                lengths = [np.random.randint(150, 250) for _ in range(num_episodes)]
                successes = [r > 0 for r in rewards]

                from src.evaluation.evaluator import EvaluationMetrics

                return EvaluationMetrics(
                    mean_reward=float(np.mean(rewards)),
                    std_reward=float(np.std(rewards)),
                    min_reward=float(np.min(rewards)),
                    max_reward=float(np.max(rewards)),
                    mean_episode_length=float(np.mean(lengths)),
                    std_episode_length=float(np.std(lengths)),
                    min_episode_length=int(np.min(lengths)),
                    max_episode_length=int(np.max(lengths)),
                    success_rate=float(np.mean(successes)),
                    total_episodes=num_episodes,
                    total_timesteps=sum(lengths),
                    evaluation_time=num_episodes * 0.1,
                    episode_rewards=rewards,
                    episode_lengths=lengths,
                    episode_successes=successes,
                    reward_ci_lower=float(
                        np.mean(rewards)
                        - 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                    ),
                    reward_ci_upper=float(
                        np.mean(rewards)
                        + 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                    ),
                )

            mock_eval.side_effect = mock_evaluation

            # 1. –û—Ü–µ–Ω–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
            agents_metrics = {}
            for name, agent in trained_agents.items():
                print(f"üìä –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ {name}...")

                metrics = evaluator.evaluate_agent_quantitative(
                    agent=agent,
                    num_episodes=15,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
                    agent_name=name,
                )

                agents_metrics[name] = metrics

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–µ—Ç—Ä–∏–∫
                assert isinstance(metrics, QuantitativeMetrics)
                assert metrics.base_metrics.total_episodes == 15
                assert metrics.reward_stability_score >= 0
                assert metrics.reward_stability_score <= 1

                print(
                    f"‚úÖ {name}: –Ω–∞–≥—Ä–∞–¥–∞ {metrics.base_metrics.mean_reward:.2f} ¬± "
                    f"{metrics.base_metrics.std_reward:.2f}, "
                    f"—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å {metrics.reward_stability_score:.3f}"
                )

            # 2. –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
            print("üìä –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤...")

            batch_result = evaluator.evaluate_multiple_agents_batch(
                agents=trained_agents, num_episodes=12, include_pairwise_comparison=True
            )

            assert len(batch_result.agents_metrics) == len(trained_agents)
            assert batch_result.best_agent in trained_agents.keys()
            assert len(batch_result.ranking) == len(trained_agents)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª—É—á—à–∏–π –∞–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–∏–π
            best_reward = batch_result.ranking[0][1]
            for _, reward in batch_result.ranking[1:]:
                assert best_reward >= reward

            print(
                f"‚úÖ –õ—É—á—à–∏–π –∞–≥–µ–Ω—Ç: {batch_result.best_agent} "
                f"(–Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f})"
            )

            # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–µ–π
            print("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–µ–π...")

            best_agent = trained_agents[batch_result.best_agent]
            baseline_agent = trained_agents[
                "SAC_Poor"
            ]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–∞–±–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∫–∞–∫ –±–∞–∑–æ–≤—É—é –ª–∏–Ω–∏—é

            comparison = evaluator.compare_with_baseline(
                agent=best_agent,
                baseline_agent=baseline_agent,
                num_episodes=10,
                agent_name=batch_result.best_agent,
                baseline_name="SAC_Poor",
            )

            assert comparison.agent_name == batch_result.best_agent
            assert comparison.baseline_name == "SAC_Poor"
            assert comparison.is_better  # –õ—É—á—à–∏–π –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª—É—á—à–µ —Å–ª–∞–±–æ–≥–æ

            print(
                f"‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: —É–ª—É—á—à–µ–Ω–∏–µ {comparison.reward_improvement:.1f}%, "
                f"—Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ {comparison.effect_size:.3f}"
            )

        # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏...")

        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        text_report = evaluator.generate_comprehensive_report(
            metrics=batch_result,
            save_path=eval_dir / "evaluation_report.txt",
            format_type="text",
        )

        assert (eval_dir / "evaluation_report.txt").exists()
        assert len(text_report) > 0
        print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {eval_dir / 'evaluation_report.txt'}")

        # JSON –æ—Ç—á–µ—Ç
        json_report = evaluator.generate_comprehensive_report(
            metrics=batch_result,
            save_path=eval_dir / "evaluation_report.json",
            format_type="json",
        )

        assert (eval_dir / "evaluation_report.json").exists()
        json_data = json.loads(json_report)
        assert "agents_metrics" in json_data
        print(f"‚úÖ JSON –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {eval_dir / 'evaluation_report.json'}")

        # CSV –æ—Ç—á–µ—Ç
        csv_report = evaluator.generate_comprehensive_report(
            metrics=batch_result,
            save_path=eval_dir / "evaluation_report.csv",
            format_type="csv",
        )

        assert (eval_dir / "evaluation_report.csv").exists()
        print(f"‚úÖ CSV –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {eval_dir / 'evaluation_report.csv'}")

        print("üéâ –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return {
            "agents_metrics": agents_metrics,
            "batch_result": batch_result,
            "comparison": comparison,
        }

    def test_results_formatting_and_reporting(self, test_output_dir: Path):
        """–¢–µ—Å—Ç 5: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Ç—á–µ—Ç—ã."""
        print("\nüìù –¢–µ—Å—Ç 5: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Ç—á–µ—Ç—ã")

        reports_dir = test_output_dir / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤—â–∏–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        formatter = ResultsFormatter(
            output_dir=reports_dir,
            config=ReportConfig(
                language="ru", include_plots=True, include_statistics=True
            ),
        )

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        from src.evaluation.evaluator import EvaluationMetrics

        test_metrics = {
            "PPO_Excellent": EvaluationMetrics(
                mean_reward=180.5,
                std_reward=25.3,
                min_reward=120.0,
                max_reward=220.0,
                mean_episode_length=195.2,
                std_episode_length=15.8,
                min_episode_length=150,
                max_episode_length=230,
                success_rate=0.85,
                total_episodes=20,
                total_timesteps=3904,
                evaluation_time=45.2,
                episode_rewards=[180.5] * 20,
                episode_lengths=[195] * 20,
                episode_successes=[True] * 17 + [False] * 3,
                reward_ci_lower=169.3,
                reward_ci_upper=191.7,
            ),
            "A2C_Good": EvaluationMetrics(
                mean_reward=145.8,
                std_reward=32.1,
                min_reward=80.0,
                max_reward=200.0,
                mean_episode_length=210.5,
                std_episode_length=22.4,
                min_episode_length=160,
                max_episode_length=250,
                success_rate=0.65,
                total_episodes=20,
                total_timesteps=4210,
                evaluation_time=48.7,
                episode_rewards=[145.8] * 20,
                episode_lengths=[210] * 20,
                episode_successes=[True] * 13 + [False] * 7,
                reward_ci_lower=131.5,
                reward_ci_upper=160.1,
            ),
        }

        # 1. –û—Ç—á–µ—Ç –ø–æ –æ–¥–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É
        single_agent_report = formatter.format_single_agent_report(
            agent_name="PPO_Excellent",
            evaluation_results=test_metrics["PPO_Excellent"],
            output_format="html",
            filename="ppo_agent_report",
        )

        assert single_agent_report.exists()
        assert single_agent_report.suffix == ".html"
        print(f"‚úÖ –û—Ç—á–µ—Ç –ø–æ –∞–≥–µ–Ω—Ç—É —Å–æ–∑–¥–∞–Ω: {single_agent_report}")

        # 2. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        comparison_report = formatter.format_comparison_report(
            agents_results=test_metrics,
            output_format="html",
            filename="agents_comparison_report",
        )

        assert comparison_report.exists()
        assert comparison_report.suffix == ".html"
        print(f"‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {comparison_report}")

        # 3. –û—Ç—á–µ—Ç –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É
        experiment_data = {
            "experiment_name": "PPO vs A2C Comparison",
            "hypothesis": "PPO –ø–æ–∫–∞–∂–µ—Ç –ª—É—á—à—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —á–µ–º A2C",
            "agents_results": test_metrics,
            "conclusion": "–ì–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞",
            "statistical_significance": True,
        }

        experiment_report = formatter.format_experiment_report(
            experiment_name="PPO_vs_A2C_Experiment",
            experiment_data=experiment_data,
            output_format="markdown",
            filename="experiment_report",
        )

        assert experiment_report.exists()
        assert experiment_report.suffix == ".markdown"
        print(f"‚úÖ –û—Ç—á–µ—Ç –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É —Å–æ–∑–¥–∞–Ω: {experiment_report}")

        # 4. –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        experiments_data = {
            "PPO_vs_A2C": experiment_data,
            "Baseline_Comparison": {
                "experiment_name": "Baseline Comparison",
                "agents_results": {"PPO": test_metrics["PPO_Excellent"]},
                "conclusion": "–ü—Ä–µ–≤—ã—à–∞–µ—Ç –±–∞–∑–æ–≤—É—é –ª–∏–Ω–∏—é",
            },
        }

        summary_report = formatter.format_summary_report(
            experiments_data=experiments_data,
            output_format="html",
            filename="experiments_summary",
        )

        assert summary_report.exists()
        assert summary_report.suffix == ".html"
        print(f"‚úÖ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {summary_report}")

        # 5. –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
        csv_export = formatter.export_to_csv(
            data=test_metrics, filename="agents_results"
        )

        assert csv_export.exists()
        assert csv_export.suffix == ".csv"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ CSV
        df = pd.read_csv(csv_export)
        assert len(df) == len(test_metrics)
        assert "agent" in df.columns
        assert "mean_reward" in df.columns
        print(f"‚úÖ CSV —ç–∫—Å–ø–æ—Ä—Ç —Å–æ–∑–¥–∞–Ω: {csv_export}")

        # 6. –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
        json_export = formatter.export_to_json(
            data={"agents_results": test_metrics, "experiment_data": experiment_data},
            filename="complete_results",
        )

        assert json_export.exists()
        assert json_export.suffix == ".json"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ JSON
        with open(json_export, "r", encoding="utf-8") as f:
            json_data = json.load(f)
        assert "agents_results" in json_data
        assert "experiment_data" in json_data
        print(f"‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç —Å–æ–∑–¥–∞–Ω: {json_export}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        created_reports = list(reports_dir.rglob("*"))
        created_files = [f for f in created_reports if f.is_file()]
        assert len(created_files) >= 6
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –æ—Ç—á–µ—Ç–æ–≤: {len(created_files)}")

        print("üéâ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

    def test_integration_between_modules(
        self,
        trained_agents: Dict[str, MockAgent],
        training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏."""
        print("\nüîó –¢–µ—Å—Ç 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏")

        integration_dir = test_output_dir / "integration"
        integration_dir.mkdir(parents=True, exist_ok=True)

        # 1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        print("üîó –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: –¥–∞–Ω–Ω—ã–µ ‚Üí –≥—Ä–∞—Ñ–∏–∫–∏")

        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        performance_report_dir = create_performance_report(
            data=training_data,
            output_dir=integration_dir / "performance_report",
            include_interactive=True,
            include_static=True,
        )

        assert Path(performance_report_dir).exists()
        static_plots = list(Path(performance_report_dir).glob("static/*.png"))
        interactive_plots = list(
            Path(performance_report_dir).glob("interactive/*.html")
        )

        assert len(static_plots) >= 2
        assert len(interactive_plots) >= 1
        print(
            f"‚úÖ –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {len(static_plots)} —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö, "
            f"{len(interactive_plots)} –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤"
        )

        # 2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
        print("üîó –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: –æ—Ü–µ–Ω–∫–∞ ‚Üí –æ—Ç—á–µ—Ç—ã")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ—Ü–µ–Ω–∫–∏
        mock_env = MockEnvironment()

        with patch("src.evaluation.evaluator.Evaluator.evaluate_agent") as mock_eval:

            def mock_evaluation(agent, num_episodes, **kwargs):
                rewards = [agent.simulate_episode_reward() for _ in range(num_episodes)]
                lengths = [np.random.randint(150, 250) for _ in range(num_episodes)]
                successes = [r > 0 for r in rewards]

                from src.evaluation.evaluator import EvaluationMetrics

                return EvaluationMetrics(
                    mean_reward=float(np.mean(rewards)),
                    std_reward=float(np.std(rewards)),
                    min_reward=float(np.min(rewards)),
                    max_reward=float(np.max(rewards)),
                    mean_episode_length=float(np.mean(lengths)),
                    std_episode_length=float(np.std(lengths)),
                    min_episode_length=int(np.min(lengths)),
                    max_episode_length=int(np.max(lengths)),
                    success_rate=float(np.mean(successes)),
                    total_episodes=num_episodes,
                    total_timesteps=sum(lengths),
                    evaluation_time=num_episodes * 0.1,
                    episode_rewards=rewards,
                    episode_lengths=lengths,
                    episode_successes=successes,
                    reward_ci_lower=float(
                        np.mean(rewards)
                        - 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                    ),
                    reward_ci_upper=float(
                        np.mean(rewards)
                        + 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                    ),
                )

            mock_eval.side_effect = mock_evaluation

            # –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
            best_agent = trained_agents["PPO_Excellent"]
            evaluation_metrics = evaluate_agent_standard(
                agent=best_agent,
                env=mock_env,
                num_episodes=15,
                agent_name="PPO_Excellent",
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏
            formatter = ResultsFormatter(
                output_dir=integration_dir / "integrated_reports"
            )

            integrated_report = formatter.format_single_agent_report(
                agent_name="PPO_Excellent",
                evaluation_results=evaluation_metrics.base_metrics,
                quantitative_results=evaluation_metrics,
                output_format="html",
            )

            assert integrated_report.exists()
            print(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {integrated_report}")

        # 3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        print("üîó –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: –¥–∞–Ω–Ω—ã–µ ‚Üí –æ—Ü–µ–Ω–∫–∞ ‚Üí –≥—Ä–∞—Ñ–∏–∫–∏ ‚Üí –æ—Ç—á–µ—Ç—ã")

        pipeline_dir = integration_dir / "full_pipeline"
        pipeline_dir.mkdir(parents=True, exist_ok=True)

        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        pipeline_results = {
            "training_data": training_data,
            "agents_evaluated": len(trained_agents),
            "plots_created": len(static_plots) + len(interactive_plots),
            "reports_generated": 1,
            "integration_successful": True,
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline_summary_path = pipeline_dir / "pipeline_summary.json"
        with open(pipeline_summary_path, "w", encoding="utf-8") as f:
            json.dump(pipeline_results, f, indent=2, default=str)

        assert pipeline_summary_path.exists()
        print(f"‚úÖ –°–≤–æ–¥–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {pipeline_summary_path}")

        print("üéâ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

    def test_error_handling_and_edge_cases(self, test_output_dir: Path):
        """–¢–µ—Å—Ç 7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤."""
        print("\n‚ö†Ô∏è –¢–µ—Å—Ç 7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤")

        error_test_dir = test_output_dir / "error_handling"
        error_test_dir.mkdir(parents=True, exist_ok=True)

        # 1. –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –≥—Ä–∞—Ñ–∏–∫–∞—Ö
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –≥—Ä–∞—Ñ–∏–∫–∞—Ö")

        plotter = PerformancePlotter()

        # –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        with pytest.raises(ValueError, match="–ø—É—Å—Ç—ã–µ"):
            empty_data = pd.DataFrame()
            plotter.plot_reward_curve(data=empty_data)

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        with pytest.raises(ValueError, match="–Ω–µ –Ω–∞–π–¥–µ–Ω–∞"):
            invalid_data = {
                "nonexistent_metric": pd.DataFrame({"x": [1, 2], "y": [1, 2]})
            }
            plotter.plot_reward_curve(data=invalid_data, y_col="missing_column")

        print("‚úÖ –û—à–∏–±–∫–∏ –≤ –≥—Ä–∞—Ñ–∏–∫–∞—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # 2. –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –¥–µ–º–æ –≤–∏–¥–µ–æ
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –¥–µ–º–æ –≤–∏–¥–µ–æ")

        # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∞–≥–µ–Ω—Ç
        with pytest.raises(AgentDemoError):
            with patch(
                "src.visualization.agent_demo.record_agent_episode"
            ) as mock_record:
                mock_record.side_effect = Exception("–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ")

                create_best_episode_demo(
                    agent=MockAgent("ErrorAgent"),
                    env="InvalidEnv",
                    output_path=error_test_dir / "error_demo.mp4",
                    config=DemoConfig(continue_on_error=False),
                )

        print("‚úÖ –û—à–∏–±–∫–∏ –≤ –¥–µ–º–æ –≤–∏–¥–µ–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # 3. –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –æ—Ü–µ–Ω–∫–µ
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –æ—Ü–µ–Ω–∫–µ")

        mock_env = MockEnvironment()
        evaluator = QuantitativeEvaluator(env=mock_env)

        # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —ç–ø–∏–∑–æ–¥–æ–≤
        with pytest.raises(ValueError, match="–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å >= 5"):
            evaluator.evaluate_agent_quantitative(
                agent=MockAgent("TestAgent"),
                num_episodes=3,  # –ú–µ–Ω—å—à–µ –º–∏–Ω–∏–º—É–º–∞
            )

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        with pytest.raises(ValueError, match="–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å"):
            evaluator.compare_with_baseline(
                agent=MockAgent("TestAgent"), baseline_agent=None, baseline_metrics=None
            )

        print("‚úÖ –û—à–∏–±–∫–∏ –≤ –æ—Ü–µ–Ω–∫–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # 4. –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –æ—Ç—á–µ—Ç–∞—Ö
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –æ—Ç—á–µ—Ç–∞—Ö")

        formatter = ResultsFormatter(output_dir=error_test_dir)

        # –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        with pytest.raises(ValueError, match="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"):
            formatter.generate_comprehensive_report(
                metrics={}, format_type="unsupported_format"
            )

        print("‚úÖ –û—à–∏–±–∫–∏ –≤ –æ—Ç—á–µ—Ç–∞—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # 5. –¢–µ—Å—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        print("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤")

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        minimal_data = pd.DataFrame(
            {
                "timestep": [0, 1],
                "episode": [0, 1],
                "value": [0.0, 1.0],
                "timestamp": pd.date_range("2024-01-01", periods=2, freq="1min"),
            }
        )

        minimal_plot_path = plotter.plot_reward_curve(
            data={"episode_reward": minimal_data},
            save_path=error_test_dir / "minimal_plot.png",
            smooth_window=1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ
        )

        assert Path(minimal_plot_path).exists()
        print("‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        constant_data = pd.DataFrame(
            {
                "timestep": range(10),
                "episode": range(10),
                "value": [100.0] * 10,
                "timestamp": pd.date_range("2024-01-01", periods=10, freq="1min"),
            }
        )

        constant_plot_path = plotter.plot_reward_curve(
            data={"episode_reward": constant_data},
            save_path=error_test_dir / "constant_plot.png",
        )

        assert Path(constant_plot_path).exists()
        print("‚úÖ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        print("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

    def test_performance_and_timing(
        self,
        trained_agents: Dict[str, MockAgent],
        training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 8: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."""
        print("\n‚è±Ô∏è –¢–µ—Å—Ç 8: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")

        performance_dir = test_output_dir / "performance"
        performance_dir.mkdir(parents=True, exist_ok=True)

        performance_metrics = {}

        # 1. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        print("‚è±Ô∏è –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤")

        start_time = time.time()

        plotter = PerformancePlotter()
        plots_created = 0

        # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        for i in range(3):
            plot_path = plotter.plot_reward_curve(
                data=training_data, save_path=performance_dir / f"perf_plot_{i}.png"
            )
            plots_created += 1

        plots_time = time.time() - start_time
        performance_metrics["plots"] = {
            "total_time": plots_time,
            "plots_created": plots_created,
            "time_per_plot": plots_time / plots_created,
            "plots_per_second": plots_created / plots_time,
        }

        print(
            f"‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏: {plots_created} –∑–∞ {plots_time:.2f}—Å "
            f"({performance_metrics['plots']['time_per_plot']:.2f}—Å/–≥—Ä–∞—Ñ–∏–∫)"
        )

        # 2. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤
        print("‚è±Ô∏è –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏")

        mock_env = MockEnvironment()
        evaluator = QuantitativeEvaluator(env=mock_env)

        start_time = time.time()
        agents_evaluated = 0

        with patch.object(evaluator.evaluator, "evaluate_agent") as mock_eval:

            def quick_evaluation(agent, num_episodes, **kwargs):
                # –ë—ã—Å—Ç—Ä–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                rewards = [agent.simulate_episode_reward() for _ in range(num_episodes)]
                lengths = [200] * num_episodes
                successes = [True] * num_episodes

                from src.evaluation.evaluator import EvaluationMetrics

                return EvaluationMetrics(
                    mean_reward=float(np.mean(rewards)),
                    std_reward=float(np.std(rewards)),
                    min_reward=float(np.min(rewards)),
                    max_reward=float(np.max(rewards)),
                    mean_episode_length=200.0,
                    std_episode_length=0.0,
                    min_episode_length=200,
                    max_episode_length=200,
                    success_rate=1.0,
                    total_episodes=num_episodes,
                    total_timesteps=num_episodes * 200,
                    evaluation_time=0.1,
                    episode_rewards=rewards,
                    episode_lengths=lengths,
                    episode_successes=successes,
                    reward_ci_lower=float(np.mean(rewards) - 10),
                    reward_ci_upper=float(np.mean(rewards) + 10),
                )

            mock_eval.side_effect = quick_evaluation

            for name, agent in trained_agents.items():
                evaluator.evaluate_agent_quantitative(
                    agent=agent, num_episodes=10, agent_name=name
                )
                agents_evaluated += 1

        evaluation_time = time.time() - start_time
        performance_metrics["evaluation"] = {
            "total_time": evaluation_time,
            "agents_evaluated": agents_evaluated,
            "time_per_agent": evaluation_time / agents_evaluated,
            "agents_per_second": agents_evaluated / evaluation_time,
        }

        print(
            f"‚úÖ –û—Ü–µ–Ω–∫–∞: {agents_evaluated} –∞–≥–µ–Ω—Ç–æ–≤ –∑–∞ {evaluation_time:.2f}—Å "
            f"({performance_metrics['evaluation']['time_per_agent']:.2f}—Å/–∞–≥–µ–Ω—Ç)"
        )

        # 3. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
        print("‚è±Ô∏è –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç—á–µ—Ç–æ–≤")

        formatter = ResultsFormatter(output_dir=performance_dir)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        from src.evaluation.evaluator import EvaluationMetrics

        test_metrics = EvaluationMetrics(
            mean_reward=150.0,
            std_reward=25.0,
            min_reward=100.0,
            max_reward=200.0,
            mean_episode_length=200.0,
            std_episode_length=20.0,
            min_episode_length=150,
            max_episode_length=250,
            success_rate=0.8,
            total_episodes=20,
            total_timesteps=4000,
            evaluation_time=30.0,
            episode_rewards=[150.0] * 20,
            episode_lengths=[200] * 20,
            episode_successes=[True] * 16 + [False] * 4,
            reward_ci_lower=140.0,
            reward_ci_upper=160.0,
        )

        start_time = time.time()
        reports_created = 0

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        for format_type in ["html", "json", "csv"]:
            try:
                if format_type == "csv":
                    formatter.export_to_csv(
                        data=test_metrics, filename=f"perf_report_{format_type}"
                    )
                elif format_type == "json":
                    formatter.export_to_json(
                        data={"test_metrics": test_metrics},
                        filename=f"perf_report_{format_type}",
                    )
                else:
                    formatter.format_single_agent_report(
                        agent_name="TestAgent",
                        evaluation_results=test_metrics,
                        output_format=format_type,
                        filename=f"perf_report_{format_type}",
                    )
                reports_created += 1
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞ {format_type}: {e}")

        reports_time = time.time() - start_time
        performance_metrics["reports"] = {
            "total_time": reports_time,
            "reports_created": reports_created,
            "time_per_report": reports_time / reports_created
            if reports_created > 0
            else 0,
            "reports_per_second": reports_created / reports_time
            if reports_time > 0
            else 0,
        }

        print(
            f"‚úÖ –û—Ç—á–µ—Ç—ã: {reports_created} –∑–∞ {reports_time:.2f}—Å "
            f"({performance_metrics['reports']['time_per_report']:.2f}—Å/–æ—Ç—á–µ—Ç)"
        )

        # 4. –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        total_time = sum(m["total_time"] for m in performance_metrics.values())
        total_operations = (
            performance_metrics["plots"]["plots_created"]
            + performance_metrics["evaluation"]["agents_evaluated"]
            + performance_metrics["reports"]["reports_created"]
        )

        performance_metrics["overall"] = {
            "total_time": total_time,
            "total_operations": total_operations,
            "operations_per_second": total_operations / total_time
            if total_time > 0
            else 0,
        }

        print(
            f"‚úÖ –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_operations} –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞ {total_time:.2f}—Å "
            f"({performance_metrics['overall']['operations_per_second']:.2f} –æ–ø/—Å)"
        )

        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        performance_report_path = performance_dir / "performance_metrics.json"
        with open(performance_report_path, "w", encoding="utf-8") as f:
            json.dump(performance_metrics, f, indent=2)

        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {performance_report_path}")

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–±–∞–∑–æ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
        assert (
            performance_metrics["plots"]["time_per_plot"] < 5.0
        )  # –ù–µ –±–æ–ª–µ–µ 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
        assert (
            performance_metrics["evaluation"]["time_per_agent"] < 2.0
        )  # –ù–µ –±–æ–ª–µ–µ 2 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∞–≥–µ–Ω—Ç–∞
        assert (
            performance_metrics["reports"]["time_per_report"] < 3.0
        )  # –ù–µ –±–æ–ª–µ–µ 3 —Å–µ–∫—É–Ω–¥ –Ω–∞ –æ—Ç—á–µ—Ç

        print("üéâ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")

        return performance_metrics

    @pytest.mark.integration
    def test_full_output_generation_workflow(
        self,
        trained_agents: Dict[str, MockAgent],
        training_data: Dict[str, pd.DataFrame],
        test_output_dir: Path,
    ):
        """–¢–µ—Å—Ç 9: –ü–æ–ª–Ω—ã–π workflow –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        print("\nüöÄ –¢–µ—Å—Ç 9: –ü–æ–ª–Ω—ã–π workflow –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

        workflow_dir = test_output_dir / "full_workflow"
        workflow_dir.mkdir(parents=True, exist_ok=True)

        workflow_results = {
            "start_time": time.time(),
            "steps_completed": [],
            "files_created": [],
            "errors_encountered": [],
            "success": True,
        }

        try:
            # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
            print("üîß –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è")

            data_dir = workflow_dir / "training_data"
            data_dir.mkdir(exist_ok=True)

            for metric_name, data in training_data.items():
                data_path = data_dir / f"{metric_name}.csv"
                data.to_csv(data_path, index=False)
                workflow_results["files_created"].append(str(data_path))

            workflow_results["steps_completed"].append("data_preparation")
            print("‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã")

            # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            print("üìä –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

            plots_dir = workflow_dir / "plots"
            performance_report_dir = create_performance_report(
                data=training_data,
                output_dir=plots_dir,
                include_interactive=True,
                include_static=True,
            )

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
            static_plots = list(Path(performance_report_dir).glob("static/*.png"))
            interactive_plots = list(
                Path(performance_report_dir).glob("interactive/*.html")
            )

            workflow_results["files_created"].extend([str(p) for p in static_plots])
            workflow_results["files_created"].extend(
                [str(p) for p in interactive_plots]
            )
            workflow_results["steps_completed"].append("performance_plots")

            print(
                f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(static_plots)} —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö, {len(interactive_plots)} –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö"
            )

            # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ
            print("üé¨ –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ")

            videos_dir = workflow_dir / "videos"
            videos_dir.mkdir(exist_ok=True)

            # –ú–æ–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
            with patch(
                "src.visualization.agent_demo.create_best_episode_demo"
            ) as mock_demo:

                def mock_demo_creation(agent, env, output_path, config, **kwargs):
                    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –≤–∏–¥–µ–æ—Ñ–∞–π–ª
                    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                    Path(output_path).write_text("mock video content")

                    return {
                        "success": True,
                        "demo_type": "best_episode",
                        "agent_name": agent.name,
                        "best_reward": agent.simulate_episode_reward(),
                        "output_path": str(output_path),
                    }

                mock_demo.side_effect = mock_demo_creation

                agents_list = [(name, agent) for name, agent in trained_agents.items()]
                batch_result = create_batch_demos(
                    agents=agents_list,
                    env="LunarLander-v2",
                    output_dir=videos_dir,
                    demo_types=["best_episode"],
                    config=DemoConfig(auto_compress=False),
                )

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ
            created_videos = list(videos_dir.rglob("*.mp4"))
            workflow_results["files_created"].extend([str(v) for v in created_videos])
            workflow_results["steps_completed"].append("demo_videos")

            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –≤–∏–¥–µ–æ: {len(created_videos)}")

            # –®–∞–≥ 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤
            print("üìà –®–∞–≥ 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤")

            eval_dir = workflow_dir / "evaluation"
            eval_dir.mkdir(exist_ok=True)

            mock_env = MockEnvironment()
            evaluator = QuantitativeEvaluator(env=mock_env)

            # –ú–æ–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            with patch.object(evaluator.evaluator, "evaluate_agent") as mock_eval:

                def mock_evaluation(agent, num_episodes, **kwargs):
                    rewards = [
                        agent.simulate_episode_reward() for _ in range(num_episodes)
                    ]
                    lengths = [np.random.randint(150, 250) for _ in range(num_episodes)]
                    successes = [r > 0 for r in rewards]

                    from src.evaluation.evaluator import EvaluationMetrics

                    return EvaluationMetrics(
                        mean_reward=float(np.mean(rewards)),
                        std_reward=float(np.std(rewards)),
                        min_reward=float(np.min(rewards)),
                        max_reward=float(np.max(rewards)),
                        mean_episode_length=float(np.mean(lengths)),
                        std_episode_length=float(np.std(lengths)),
                        min_episode_length=int(np.min(lengths)),
                        max_episode_length=int(np.max(lengths)),
                        success_rate=float(np.mean(successes)),
                        total_episodes=num_episodes,
                        total_timesteps=sum(lengths),
                        evaluation_time=num_episodes * 0.1,
                        episode_rewards=rewards,
                        episode_lengths=lengths,
                        episode_successes=successes,
                        reward_ci_lower=float(
                            np.mean(rewards)
                            - 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                        ),
                        reward_ci_upper=float(
                            np.mean(rewards)
                            + 1.96 * np.std(rewards) / np.sqrt(num_episodes)
                        ),
                    )

                mock_eval.side_effect = mock_evaluation

                # –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤
                batch_evaluation = evaluator.evaluate_multiple_agents_batch(
                    agents=trained_agents,
                    num_episodes=15,
                    include_pairwise_comparison=True,
                )

                # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏
                for format_type in ["text", "json", "csv"]:
                    report_path = eval_dir / f"evaluation_report.{format_type}"
                    evaluator.generate_comprehensive_report(
                        metrics=batch_evaluation,
                        save_path=report_path,
                        format_type=format_type,
                    )
                    workflow_results["files_created"].append(str(report_path))

            workflow_results["steps_completed"].append("quantitative_evaluation")
            print(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ª—É—á—à–∏–π –∞–≥–µ–Ω—Ç: {batch_evaluation.best_agent}")

            # –®–∞–≥ 5: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            print("üìù –®–∞–≥ 5: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞")

            reports_dir = workflow_dir / "reports"
            reports_dir.mkdir(exist_ok=True)

            formatter = ResultsFormatter(output_dir=reports_dir)

            # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç
            experiment_data = {
                "experiment_name": "Full Output Generation Workflow Test",
                "hypothesis": "–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ",
                "training_data_metrics": {
                    "datasets": len(training_data),
                    "total_records": sum(len(data) for data in training_data.values()),
                },
                "evaluation_results": {
                    "agents_evaluated": len(trained_agents),
                    "best_agent": batch_evaluation.best_agent,
                    "evaluation_time": batch_evaluation.statistical_summary[
                        "evaluation_time"
                    ],
                },
                "output_files": {
                    "plots_created": len(static_plots) + len(interactive_plots),
                    "videos_created": len(created_videos),
                    "reports_created": 3,  # text, json, csv
                },
                "conclusion": "–ü–∞–π–ø–ª–∞–π–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ",
            }

            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            for format_type in ["html", "markdown"]:
                report_path = formatter.format_experiment_report(
                    experiment_name="Full_Workflow_Test",
                    experiment_data=experiment_data,
                    output_format=format_type,
                    filename="full_workflow_report",
                )
                workflow_results["files_created"].append(str(report_path))

            # –≠–∫—Å–ø–æ—Ä—Ç —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            summary_data = {
                "workflow_results": workflow_results,
                "experiment_data": experiment_data,
                "batch_evaluation": {
                    "best_agent": batch_evaluation.best_agent,
                    "ranking": batch_evaluation.ranking,
                    "statistical_summary": batch_evaluation.statistical_summary,
                },
            }

            json_export = formatter.export_to_json(
                data=summary_data, filename="workflow_summary"
            )
            workflow_results["files_created"].append(str(json_export))

            workflow_results["steps_completed"].append("results_formatting")
            print("‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã")

            # –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            print("üîç –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")

            files_verification = {
                "total_files": len(workflow_results["files_created"]),
                "existing_files": 0,
                "missing_files": [],
                "file_sizes": {},
            }

            for file_path in workflow_results["files_created"]:
                path_obj = Path(file_path)
                if path_obj.exists():
                    files_verification["existing_files"] += 1
                    files_verification["file_sizes"][str(path_obj)] = (
                        path_obj.stat().st_size
                    )
                else:
                    files_verification["missing_files"].append(str(path_obj))

            workflow_results["files_verification"] = files_verification
            workflow_results["steps_completed"].append("files_verification")

            print(
                f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤: {files_verification['existing_files']}/{files_verification['total_files']} —Å—É—â–µ—Å—Ç–≤—É—é—Ç"
            )

            if files_verification["missing_files"]:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã: {files_verification['missing_files']}")

        except Exception as e:
            workflow_results["success"] = False
            workflow_results["errors_encountered"].append(str(e))
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ workflow: {e}")
            raise

        finally:
            workflow_results["end_time"] = time.time()
            workflow_results["total_time"] = (
                workflow_results["end_time"] - workflow_results["start_time"]
            )

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ workflow
            workflow_summary_path = workflow_dir / "workflow_summary.json"
            with open(workflow_summary_path, "w", encoding="utf-8") as f:
                json.dump(workflow_results, f, indent=2, default=str)

        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        assert workflow_results["success"], "Workflow –¥–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è —É—Å–ø–µ—à–Ω–æ"
        assert len(workflow_results["steps_completed"]) == 6, (
            "–í—Å–µ —à–∞–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω—ã"
        )
        assert len(workflow_results["errors_encountered"]) == 0, "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—à–∏–±–æ–∫"
        assert workflow_results["files_verification"]["existing_files"] > 0, (
            "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã"
        )

        print(
            f"\nüéâ –ü–æ–ª–Ω—ã–π workflow –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ {workflow_results['total_time']:.2f}—Å"
        )
        print(f"üìä –í—ã–ø–æ–ª–Ω–µ–Ω–æ —à–∞–≥–æ–≤: {len(workflow_results['steps_completed'])}")
        print(
            f"üìÅ –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {workflow_results['files_verification']['total_files']}"
        )
        print(
            f"‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {workflow_results['files_verification']['existing_files']}"
        )

        return workflow_results


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    pytest.main([__file__, "-v", "-s", "--tb=short", "-m", "not integration"])
