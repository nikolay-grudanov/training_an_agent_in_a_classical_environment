{
  "experiment_id": "e8e074ec-440b-43bb-bc76-67a514318492",
  "baseline_config": {
    "experiment_name": "baseline_ppo_lunarlander",
    "output_dir": "results",
    "seed": 42,
    "algorithm": {
      "name": "PPO",
      "learning_rate": 0.0003,
      "n_steps": 2048,
      "batch_size": 64,
      "n_epochs": 10,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.0,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "use_sde": false,
      "sde_sample_freq": -1,
      "target_kl": null,
      "device": "auto",
      "verbose": 1,
      "seed": null,
      "policy_kwargs": null,
      "tensorboard_log": null
    },
    "environment": {
      "name": "LunarLander-v3",
      "render_mode": null,
      "max_episode_steps": 1000,
      "reward_threshold": null,
      "wrappers": [],
      "wrapper_kwargs": {},
      "record_video": false,
      "video_folder": null,
      "video_length": 0
    },
    "training": {
      "total_timesteps": 200000,
      "eval_freq": 10000,
      "n_eval_episodes": 10,
      "eval_log_path": null,
      "save_freq": 20000,
      "save_path": null,
      "save_replay_buffer": false,
      "monitor_training": true,
      "log_interval": 1000,
      "early_stopping": false,
      "patience": 5,
      "min_delta": 0.01
    },
    "experiment": null,
    "logging": {
      "level": "INFO",
      "log_to_file": true,
      "log_dir": null,
      "json_format": false,
      "console_output": true,
      "max_bytes": 10485760,
      "backup_count": 5
    },
    "reproducibility": {
      "seed": 42,
      "deterministic": true,
      "benchmark": false,
      "use_cuda": false
    }
  },
  "variant_config": {
    "experiment_name": "variant_ppo_lunarlander",
    "output_dir": "results",
    "seed": 42,
    "algorithm": {
      "name": "PPO",
      "learning_rate": 0.001,
      "n_steps": 2048,
      "batch_size": 64,
      "n_epochs": 10,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.0,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "use_sde": false,
      "sde_sample_freq": -1,
      "target_kl": null,
      "device": "auto",
      "verbose": 1,
      "seed": null,
      "policy_kwargs": null,
      "tensorboard_log": null
    },
    "environment": {
      "name": "LunarLander-v3",
      "render_mode": null,
      "max_episode_steps": 1000,
      "reward_threshold": null,
      "wrappers": [],
      "wrapper_kwargs": {},
      "record_video": false,
      "video_folder": null,
      "video_length": 0
    },
    "training": {
      "total_timesteps": 200000,
      "eval_freq": 10000,
      "n_eval_episodes": 10,
      "eval_log_path": null,
      "save_freq": 20000,
      "save_path": null,
      "save_replay_buffer": false,
      "monitor_training": true,
      "log_interval": 1000,
      "early_stopping": false,
      "patience": 5,
      "min_delta": 0.01
    },
    "experiment": null,
    "logging": {
      "level": "INFO",
      "log_to_file": true,
      "log_dir": null,
      "json_format": false,
      "console_output": true,
      "max_bytes": 10485760,
      "backup_count": 5
    },
    "reproducibility": {
      "seed": 42,
      "deterministic": true,
      "benchmark": false,
      "use_cuda": false
    }
  },
  "hypothesis": "Увеличение learning rate с 3e-4 до 1e-3 улучшит производительность агента PPO в среде LunarLander-v3, что приведет к более высокой средней награде и более быстрой сходимости.",
  "results": {
    "baseline": {
      "mean_reward": 150.5,
      "final_reward": 180.2,
      "episode_length": 245,
      "convergence_timesteps": 120000,
      "training_time": 3600,
      "best_reward": 220.1,
      "worst_reward": 85.3,
      "reward_std": 45.2,
      "success_rate": 0.75,
      "timestamp": "2026-01-14T17:51:53.987523",
      "config_type": "baseline",
      "metrics_history": [
        {
          "timestep": 0,
          "mean_reward": -11.296778326759176,
          "episode_length": 250,
          "loss_policy": 0.09424478511994264,
          "loss_value": 0.0998436430193497
        },
        {
          "timestep": 10000,
          "mean_reward": 9.053288663481299,
          "episode_length": 249,
          "loss_policy": 0.06179765741148601,
          "loss_value": 0.13912454016420128
        },
        {
          "timestep": 20000,
          "mean_reward": -2.3266233820235094,
          "episode_length": 248,
          "loss_policy": 0.08035632935418995,
          "loss_value": 0.12316716339974726
        },
        {
          "timestep": 30000,
          "mean_reward": 2.882948975101236,
          "episode_length": 247,
          "loss_policy": 0.07337491933866094,
          "loss_value": 0.19248870788623645
        },
        {
          "timestep": 40000,
          "mean_reward": 23.69483780865108,
          "episode_length": 246,
          "loss_policy": 0.07264462184251189,
          "loss_value": 0.1674713117013959
        },
        {
          "timestep": 50000,
          "mean_reward": 32.421516250358806,
          "episode_length": 245,
          "loss_policy": 0.020591852238615697,
          "loss_value": 0.09063766342128723
        },
        {
          "timestep": 60000,
          "mean_reward": 44.40775936676015,
          "episode_length": 244,
          "loss_policy": 0.08209911629018343,
          "loss_value": 0.11763425614297231
        },
        {
          "timestep": 70000,
          "mean_reward": 64.70723330241775,
          "episode_length": 243,
          "loss_policy": 0.010096200387519917,
          "loss_value": 0.1679445493238428
        },
        {
          "timestep": 80000,
          "mean_reward": 56.20971293567342,
          "episode_length": 242,
          "loss_policy": 0.09274712933449068,
          "loss_value": 0.19722827453730085
        },
        {
          "timestep": 90000,
          "mean_reward": 54.266654055437996,
          "episode_length": 241,
          "loss_policy": 0.018308149142508827,
          "loss_value": 0.1899351824109594
        },
        {
          "timestep": 100000,
          "mean_reward": 91.88404051316203,
          "episode_length": 240,
          "loss_policy": 0.05494511339966178,
          "loss_value": 0.10673746380159395
        },
        {
          "timestep": 110000,
          "mean_reward": 67.1376921140248,
          "episode_length": 239,
          "loss_policy": 0.07193452553630227,
          "loss_value": 0.12620835484617754
        },
        {
          "timestep": 120000,
          "mean_reward": 81.96860346677568,
          "episode_length": 238,
          "loss_policy": 0.0742894576364955,
          "loss_value": 0.1455429772303984
        },
        {
          "timestep": 130000,
          "mean_reward": 110.44612056767653,
          "episode_length": 237,
          "loss_policy": 0.03771659846768762,
          "loss_value": 0.0951786184200567
        },
        {
          "timestep": 140000,
          "mean_reward": 94.57129986725454,
          "episode_length": 236,
          "loss_policy": 0.05256610127783403,
          "loss_value": 0.07867814355665326
        },
        {
          "timestep": 150000,
          "mean_reward": 119.28486092000236,
          "episode_length": 235,
          "loss_policy": 0.01320520567236871,
          "loss_value": 0.05274041450734965
        },
        {
          "timestep": 160000,
          "mean_reward": 109.87450582846758,
          "episode_length": 234,
          "loss_policy": 0.09958570704015829,
          "loss_value": 0.09369800016580358
        },
        {
          "timestep": 170000,
          "mean_reward": 120.10930429890564,
          "episode_length": 233,
          "loss_policy": 0.06780923939664156,
          "loss_value": 0.0842900254768209
        },
        {
          "timestep": 180000,
          "mean_reward": 116.56725042249256,
          "episode_length": 232,
          "loss_policy": 0.06690296751291568,
          "loss_value": 0.10154335188052424
        },
        {
          "timestep": 190000,
          "mean_reward": 160.306542624123,
          "episode_length": 231,
          "loss_policy": 0.04494355480002896,
          "loss_value": 0.1837776638804524
        }
      ]
    },
    "variant": {
      "mean_reward": 175.8,
      "final_reward": 195.4,
      "episode_length": 230,
      "convergence_timesteps": 100000,
      "training_time": 3400,
      "best_reward": 240.5,
      "worst_reward": 95.7,
      "reward_std": 38.9,
      "success_rate": 0.82,
      "timestamp": "2026-01-14T17:51:53.987563",
      "config_type": "variant",
      "metrics_history": [
        {
          "timestep": 0,
          "mean_reward": 8.678772196542962,
          "episode_length": 250,
          "loss_policy": 0.046940293504346746,
          "loss_value": 0.0955822293828613
        },
        {
          "timestep": 10000,
          "mean_reward": 12.758751330890414,
          "episode_length": 249,
          "loss_policy": 0.02384011258588185,
          "loss_value": 0.17700454131199134
        },
        {
          "timestep": 20000,
          "mean_reward": -0.2190591460898439,
          "episode_length": 248,
          "loss_policy": 0.034621843475671106,
          "loss_value": 0.07943884211697205
        },
        {
          "timestep": 30000,
          "mean_reward": 22.511548296885056,
          "episode_length": 247,
          "loss_policy": 0.02624350181948345,
          "loss_value": 0.1247727928839632
        },
        {
          "timestep": 40000,
          "mean_reward": 38.24364146874897,
          "episode_length": 246,
          "loss_policy": 0.05246171594484847,
          "loss_value": 0.1983326037484343
        },
        {
          "timestep": 50000,
          "mean_reward": 62.42753879922509,
          "episode_length": 245,
          "loss_policy": 0.09457519314205123,
          "loss_value": 0.17411021680905492
        },
        {
          "timestep": 60000,
          "mean_reward": 65.7289628861788,
          "episode_length": 244,
          "loss_policy": 0.09142728747403431,
          "loss_value": 0.1205365728529298
        },
        {
          "timestep": 70000,
          "mean_reward": 61.66262965800415,
          "episode_length": 243,
          "loss_policy": 0.012393120214866311,
          "loss_value": 0.06468588941999111
        },
        {
          "timestep": 80000,
          "mean_reward": 63.1253171552986,
          "episode_length": 242,
          "loss_policy": 0.03667742311147522,
          "loss_value": 0.19698372588322288
        },
        {
          "timestep": 90000,
          "mean_reward": 73.27525852610526,
          "episode_length": 241,
          "loss_policy": 0.025184802899073044,
          "loss_value": 0.1849533727914469
        },
        {
          "timestep": 100000,
          "mean_reward": 104.56450016834742,
          "episode_length": 240,
          "loss_policy": 0.08364270446758879,
          "loss_value": 0.05896073144910409
        },
        {
          "timestep": 110000,
          "mean_reward": 98.51403958420977,
          "episode_length": 239,
          "loss_policy": 0.023557093534293458,
          "loss_value": 0.1206200197103565
        },
        {
          "timestep": 120000,
          "mean_reward": 115.04335753937455,
          "episode_length": 238,
          "loss_policy": 0.08582008341191065,
          "loss_value": 0.12305649569384693
        },
        {
          "timestep": 130000,
          "mean_reward": 98.99269661052755,
          "episode_length": 237,
          "loss_policy": 0.057748564860884415,
          "loss_value": 0.15206970938467085
        },
        {
          "timestep": 140000,
          "mean_reward": 134.2537838730116,
          "episode_length": 236,
          "loss_policy": 0.033997899241277496,
          "loss_value": 0.06362064951753697
        },
        {
          "timestep": 150000,
          "mean_reward": 136.63122615417262,
          "episode_length": 235,
          "loss_policy": 0.0815528928168543,
          "loss_value": 0.13782594393945818
        },
        {
          "timestep": 160000,
          "mean_reward": 157.08716786039935,
          "episode_length": 234,
          "loss_policy": 0.05769071814208651,
          "loss_value": 0.053817583337767455
        },
        {
          "timestep": 170000,
          "mean_reward": 139.6467963736686,
          "episode_length": 233,
          "loss_policy": 0.08299869830276885,
          "loss_value": 0.16114891760590822
        },
        {
          "timestep": 180000,
          "mean_reward": 155.29296192666908,
          "episode_length": 232,
          "loss_policy": 0.06231627278070518,
          "loss_value": 0.07247362206760075
        },
        {
          "timestep": 190000,
          "mean_reward": 166.2609892716354,
          "episode_length": 231,
          "loss_policy": 0.01179007111794473,
          "loss_value": 0.11280668235043938
        }
      ]
    },
    "comparison": {
      "timestamp": "2026-01-14T17:51:53.987576",
      "hypothesis_confirmed": null,
      "performance_metrics": {
        "mean_reward": {
          "baseline": 150.5,
          "variant": 175.8,
          "improvement": 25.30000000000001,
          "improvement_percent": 16.810631229235888,
          "better": "variant"
        },
        "final_reward": {
          "baseline": 180.2,
          "variant": 195.4,
          "improvement": 15.200000000000017,
          "improvement_percent": 8.435072142064383,
          "better": "variant"
        },
        "episode_length": {
          "baseline": 245,
          "variant": 230,
          "improvement": -15,
          "improvement_percent": -6.122448979591836,
          "better": "baseline"
        },
        "convergence_timesteps": {
          "baseline": 120000,
          "variant": 100000,
          "improvement": -20000,
          "improvement_percent": -16.666666666666664,
          "better": "baseline"
        },
        "training_time": {
          "baseline": 3600,
          "variant": 3400,
          "improvement": -200,
          "improvement_percent": -5.555555555555555,
          "better": "baseline"
        }
      },
      "statistical_significance": {},
      "summary": {
        "overall_better": "variant",
        "reward_improvement": 25.30000000000001,
        "significant_improvement": true
      }
    },
    "metadata": {
      "experiment_id": "e8e074ec-440b-43bb-bc76-67a514318492",
      "hypothesis": "Увеличение learning rate с 3e-4 до 1e-3 улучшит производительность агента PPO в среде LunarLander-v3, что приведет к более высокой средней награде и более быстрой сходимости.",
      "duration_seconds": 0.00019,
      "baseline_algorithm": "PPO",
      "variant_algorithm": "PPO",
      "environment": "LunarLander-v3",
      "completed_at": "2026-01-14T17:51:53.987659"
    },
    "configurations": {
      "baseline": {
        "algorithm_name": "PPO",
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "batch_size": 64,
        "gamma": 0.99,
        "total_timesteps": 200000,
        "environment": "LunarLander-v3"
      },
      "variant": {
        "algorithm_name": "PPO",
        "learning_rate": 0.001,
        "n_steps": 2048,
        "batch_size": 64,
        "gamma": 0.99,
        "total_timesteps": 200000,
        "environment": "LunarLander-v3"
      }
    }
  },
  "status": "completed",
  "created_at": "2026-01-14T17:51:53.987390",
  "started_at": "2026-01-14T17:51:53.987469",
  "completed_at": "2026-01-14T17:51:53.987659",
  "paused_at": null,
  "output_dir": "results/experiments/demo",
  "_baseline_completed": true,
  "_variant_completed": true
}