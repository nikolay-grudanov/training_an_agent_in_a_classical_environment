{
  "experiment_id": "1040542b-905b-48f2-9432-405cd2f6f76e",
  "baseline_config": {
    "experiment_name": "baseline_ppo_lunarlander",
    "output_dir": "results",
    "seed": 42,
    "algorithm": {
      "name": "PPO",
      "learning_rate": 0.0003,
      "n_steps": 2048,
      "batch_size": 64,
      "n_epochs": 10,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.0,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "use_sde": false,
      "sde_sample_freq": -1,
      "target_kl": null,
      "device": "auto",
      "verbose": 1,
      "seed": 42,
      "policy_kwargs": null,
      "tensorboard_log": null
    },
    "environment": {
      "name": "LunarLander-v3",
      "render_mode": null,
      "max_episode_steps": 1000,
      "reward_threshold": null,
      "wrappers": [],
      "wrapper_kwargs": {},
      "record_video": false,
      "video_folder": null,
      "video_length": 0
    },
    "training": {
      "total_timesteps": 200000,
      "eval_freq": 10000,
      "n_eval_episodes": 10,
      "eval_log_path": null,
      "save_freq": 20000,
      "save_path": null,
      "save_replay_buffer": false,
      "monitor_training": true,
      "log_interval": 1000,
      "early_stopping": false,
      "patience": 5,
      "min_delta": 0.01
    },
    "experiment": null,
    "logging": {
      "level": "INFO",
      "log_to_file": true,
      "log_dir": null,
      "json_format": false,
      "console_output": true,
      "max_bytes": 10485760,
      "backup_count": 5
    },
    "reproducibility": {
      "seed": 42,
      "deterministic": true,
      "benchmark": false,
      "use_cuda": false,
      "enforce_seed_consistency": true,
      "validate_determinism": true,
      "warn_on_seed_conflicts": true,
      "auto_propagate_seeds": true
    }
  },
  "variant_config": {
    "experiment_name": "variant_ppo_lunarlander",
    "output_dir": "results",
    "seed": 42,
    "algorithm": {
      "name": "PPO",
      "learning_rate": 0.001,
      "n_steps": 2048,
      "batch_size": 64,
      "n_epochs": 10,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.0,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "use_sde": false,
      "sde_sample_freq": -1,
      "target_kl": null,
      "device": "auto",
      "verbose": 1,
      "seed": 42,
      "policy_kwargs": null,
      "tensorboard_log": null
    },
    "environment": {
      "name": "LunarLander-v3",
      "render_mode": null,
      "max_episode_steps": 1000,
      "reward_threshold": null,
      "wrappers": [],
      "wrapper_kwargs": {},
      "record_video": false,
      "video_folder": null,
      "video_length": 0
    },
    "training": {
      "total_timesteps": 200000,
      "eval_freq": 10000,
      "n_eval_episodes": 10,
      "eval_log_path": null,
      "save_freq": 20000,
      "save_path": null,
      "save_replay_buffer": false,
      "monitor_training": true,
      "log_interval": 1000,
      "early_stopping": false,
      "patience": 5,
      "min_delta": 0.01
    },
    "experiment": null,
    "logging": {
      "level": "INFO",
      "log_to_file": true,
      "log_dir": null,
      "json_format": false,
      "console_output": true,
      "max_bytes": 10485760,
      "backup_count": 5
    },
    "reproducibility": {
      "seed": 42,
      "deterministic": true,
      "benchmark": false,
      "use_cuda": false,
      "enforce_seed_consistency": true,
      "validate_determinism": true,
      "warn_on_seed_conflicts": true,
      "auto_propagate_seeds": true
    }
  },
  "hypothesis": "Увеличение learning rate с 3e-4 до 1e-3 улучшит производительность агента PPO в среде LunarLander-v3, что приведет к более высокой средней награде и более быстрой сходимости.",
  "results": {
    "baseline": {
      "mean_reward": 150.5,
      "final_reward": 180.2,
      "episode_length": 245,
      "convergence_timesteps": 120000,
      "training_time": 3600,
      "best_reward": 220.1,
      "worst_reward": 85.3,
      "reward_std": 45.2,
      "success_rate": 0.75,
      "timestamp": "2026-01-15T00:47:34.169945",
      "config_type": "baseline",
      "metrics_history": [
        {
          "timestep": 0,
          "mean_reward": 8.327936846150912,
          "episode_length": 250,
          "loss_policy": 0.07520423883796351,
          "loss_value": 0.11533799470508582
        },
        {
          "timestep": 10000,
          "mean_reward": 17.191574428609698,
          "episode_length": 249,
          "loss_policy": 0.07502622394272769,
          "loss_value": 0.18185456792481292
        },
        {
          "timestep": 20000,
          "mean_reward": 20.10176921969689,
          "episode_length": 248,
          "loss_policy": 0.028379253353643955,
          "loss_value": 0.06317399206334537
        },
        {
          "timestep": 30000,
          "mean_reward": 23.879922176650986,
          "episode_length": 247,
          "loss_policy": 0.06268566288618342,
          "loss_value": 0.16898808051187647
        },
        {
          "timestep": 40000,
          "mean_reward": 20.99447138943124,
          "episode_length": 246,
          "loss_policy": 0.02669620578547595,
          "loss_value": 0.09374992954899479
        },
        {
          "timestep": 50000,
          "mean_reward": 48.41088053577666,
          "episode_length": 245,
          "loss_policy": 0.0701416121960114,
          "loss_value": 0.12272025457105828
        },
        {
          "timestep": 60000,
          "mean_reward": 47.18182964030059,
          "episode_length": 244,
          "loss_policy": 0.05047084108216777,
          "loss_value": 0.16412400065909982
        },
        {
          "timestep": 70000,
          "mean_reward": 47.95788425035363,
          "episode_length": 243,
          "loss_policy": 0.04085355110514007,
          "loss_value": 0.13288566836296903
        },
        {
          "timestep": 80000,
          "mean_reward": 49.70243477975647,
          "episode_length": 242,
          "loss_policy": 0.014714063531611189,
          "loss_value": 0.15837982078266183
        },
        {
          "timestep": 90000,
          "mean_reward": 53.0645307605265,
          "episode_length": 241,
          "loss_policy": 0.05462149643647777,
          "loss_value": 0.07600419494442867
        },
        {
          "timestep": 100000,
          "mean_reward": 69.97882539718042,
          "episode_length": 240,
          "loss_policy": 0.08111706438244108,
          "loss_value": 0.13933955911192225
        },
        {
          "timestep": 110000,
          "mean_reward": 78.61773143256755,
          "episode_length": 239,
          "loss_policy": 0.027092694951062674,
          "loss_value": 0.19352322213323048
        },
        {
          "timestep": 120000,
          "mean_reward": 89.1951579288091,
          "episode_length": 238,
          "loss_policy": 0.06287048732561612,
          "loss_value": 0.17245934349507164
        },
        {
          "timestep": 130000,
          "mean_reward": 78.58318135030572,
          "episode_length": 237,
          "loss_policy": 0.07527333727067705,
          "loss_value": 0.12193715345180672
        },
        {
          "timestep": 140000,
          "mean_reward": 107.16415033663465,
          "episode_length": 236,
          "loss_policy": 0.024942764965860145,
          "loss_value": 0.19067829216959814
        },
        {
          "timestep": 150000,
          "mean_reward": 110.28981423415053,
          "episode_length": 235,
          "loss_policy": 0.010258197834498555,
          "loss_value": 0.17383274090913503
        },
        {
          "timestep": 160000,
          "mean_reward": 102.37815041007889,
          "episode_length": 234,
          "loss_policy": 0.01841674183290929,
          "loss_value": 0.09612997766497569
        },
        {
          "timestep": 170000,
          "mean_reward": 142.53709169040195,
          "episode_length": 233,
          "loss_policy": 0.04801646686682002,
          "loss_value": 0.1736228747546653
        },
        {
          "timestep": 180000,
          "mean_reward": 127.47879864073435,
          "episode_length": 232,
          "loss_policy": 0.014302101193745729,
          "loss_value": 0.13344160301964098
        },
        {
          "timestep": 190000,
          "mean_reward": 150.96202923079227,
          "episode_length": 231,
          "loss_policy": 0.0824226826801895,
          "loss_value": 0.13293533484119574
        }
      ]
    },
    "variant": {
      "mean_reward": 175.8,
      "final_reward": 195.4,
      "episode_length": 230,
      "convergence_timesteps": 100000,
      "training_time": 3400,
      "best_reward": 240.5,
      "worst_reward": 95.7,
      "reward_std": 38.9,
      "success_rate": 0.82,
      "timestamp": "2026-01-15T00:47:34.169987",
      "config_type": "variant",
      "metrics_history": [
        {
          "timestep": 0,
          "mean_reward": 11.044217055299114,
          "episode_length": 250,
          "loss_policy": 0.09255634278087395,
          "loss_value": 0.12012161592419904
        },
        {
          "timestep": 10000,
          "mean_reward": 3.267955260558969,
          "episode_length": 249,
          "loss_policy": 0.09151437217595619,
          "loss_value": 0.19356146830863274
        },
        {
          "timestep": 20000,
          "mean_reward": -2.4150570718138376,
          "episode_length": 248,
          "loss_policy": 0.0796937924157687,
          "loss_value": 0.09603082970477324
        },
        {
          "timestep": 30000,
          "mean_reward": 9.350527719722251,
          "episode_length": 247,
          "loss_policy": 0.049633739489072466,
          "loss_value": 0.06990810558730441
        },
        {
          "timestep": 40000,
          "mean_reward": 22.115315376455808,
          "episode_length": 246,
          "loss_policy": 0.0706410954540042,
          "loss_value": 0.17559223523834322
        },
        {
          "timestep": 50000,
          "mean_reward": 44.786129462065304,
          "episode_length": 245,
          "loss_policy": 0.08541570196906394,
          "loss_value": 0.12714798914789655
        },
        {
          "timestep": 60000,
          "mean_reward": 64.74836955184027,
          "episode_length": 244,
          "loss_policy": 0.08708218235250938,
          "loss_value": 0.06865730165969738
        },
        {
          "timestep": 70000,
          "mean_reward": 54.61149787472985,
          "episode_length": 243,
          "loss_policy": 0.04611645036281974,
          "loss_value": 0.07725654799767188
        },
        {
          "timestep": 80000,
          "mean_reward": 79.82722743884916,
          "episode_length": 242,
          "loss_policy": 0.06597166071024561,
          "loss_value": 0.17459541922964883
        },
        {
          "timestep": 90000,
          "mean_reward": 83.8232125371058,
          "episode_length": 241,
          "loss_policy": 0.018146337789836063,
          "loss_value": 0.1524674452333929
        },
        {
          "timestep": 100000,
          "mean_reward": 77.96795739571623,
          "episode_length": 240,
          "loss_policy": 0.029913975773093834,
          "loss_value": 0.17035803501654959
        },
        {
          "timestep": 110000,
          "mean_reward": 99.78384639923046,
          "episode_length": 239,
          "loss_policy": 0.04031971150834535,
          "loss_value": 0.18716158598185006
        },
        {
          "timestep": 120000,
          "mean_reward": 91.29531873783084,
          "episode_length": 238,
          "loss_policy": 0.037311132189660146,
          "loss_value": 0.10340090817070553
        },
        {
          "timestep": 130000,
          "mean_reward": 94.69129835190446,
          "episode_length": 237,
          "loss_policy": 0.04017620792531245,
          "loss_value": 0.14216845890860577
        },
        {
          "timestep": 140000,
          "mean_reward": 111.19832126607147,
          "episode_length": 236,
          "loss_policy": 0.013305580524303438,
          "loss_value": 0.06719968687963389
        },
        {
          "timestep": 150000,
          "mean_reward": 149.1261505205953,
          "episode_length": 235,
          "loss_policy": 0.03445729516890483,
          "loss_value": 0.0615187808443114
        },
        {
          "timestep": 160000,
          "mean_reward": 127.44634306989855,
          "episode_length": 234,
          "loss_policy": 0.08521409071833365,
          "loss_value": 0.1923676893328034
        },
        {
          "timestep": 170000,
          "mean_reward": 131.57052649556223,
          "episode_length": 233,
          "loss_policy": 0.047555069323577386,
          "loss_value": 0.16120455512397835
        },
        {
          "timestep": 180000,
          "mean_reward": 150.09417801020822,
          "episode_length": 232,
          "loss_policy": 0.02838555585567406,
          "loss_value": 0.1955245612999636
        },
        {
          "timestep": 190000,
          "mean_reward": 165.17291622004163,
          "episode_length": 231,
          "loss_policy": 0.034802203574407775,
          "loss_value": 0.08066591960482311
        }
      ]
    },
    "comparison": {
      "timestamp": "2026-01-15T00:47:34.170001",
      "hypothesis_confirmed": null,
      "performance_metrics": {
        "mean_reward": {
          "baseline": 150.5,
          "variant": 175.8,
          "improvement": 25.30000000000001,
          "improvement_percent": 16.810631229235888,
          "better": "variant"
        },
        "final_reward": {
          "baseline": 180.2,
          "variant": 195.4,
          "improvement": 15.200000000000017,
          "improvement_percent": 8.435072142064383,
          "better": "variant"
        },
        "episode_length": {
          "baseline": 245,
          "variant": 230,
          "improvement": -15,
          "improvement_percent": -6.122448979591836,
          "better": "baseline"
        },
        "convergence_timesteps": {
          "baseline": 120000,
          "variant": 100000,
          "improvement": -20000,
          "improvement_percent": -16.666666666666664,
          "better": "baseline"
        },
        "training_time": {
          "baseline": 3600,
          "variant": 3400,
          "improvement": -200,
          "improvement_percent": -5.555555555555555,
          "better": "baseline"
        }
      },
      "statistical_significance": {},
      "summary": {
        "overall_better": "variant",
        "reward_improvement": 25.30000000000001,
        "significant_improvement": true
      }
    },
    "metadata": {
      "experiment_id": "1040542b-905b-48f2-9432-405cd2f6f76e",
      "hypothesis": "Увеличение learning rate с 3e-4 до 1e-3 улучшит производительность агента PPO в среде LunarLander-v3, что приведет к более высокой средней награде и более быстрой сходимости.",
      "duration_seconds": 0.000203,
      "baseline_algorithm": "PPO",
      "variant_algorithm": "PPO",
      "environment": "LunarLander-v3",
      "completed_at": "2026-01-15T00:47:34.170092"
    },
    "configurations": {
      "baseline": {
        "algorithm_name": "PPO",
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "batch_size": 64,
        "gamma": 0.99,
        "total_timesteps": 200000,
        "environment": "LunarLander-v3"
      },
      "variant": {
        "algorithm_name": "PPO",
        "learning_rate": 0.001,
        "n_steps": 2048,
        "batch_size": 64,
        "gamma": 0.99,
        "total_timesteps": 200000,
        "environment": "LunarLander-v3"
      }
    }
  },
  "status": "completed",
  "created_at": "2026-01-15T00:47:34.169781",
  "started_at": "2026-01-15T00:47:34.169889",
  "completed_at": "2026-01-15T00:47:34.170092",
  "paused_at": null,
  "output_dir": "results/experiments/demo",
  "_baseline_completed": true,
  "_variant_completed": true
}